{
  "type": "pull",
  "pull": {
    "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717",
    "id": 1008940822,
    "node_id": "PR_kwDOABII5848IzcW",
    "html_url": "https://github.com/bitcoin/bitcoin/pull/25717",
    "diff_url": "https://github.com/bitcoin/bitcoin/pull/25717.diff",
    "patch_url": "https://github.com/bitcoin/bitcoin/pull/25717.patch",
    "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717",
    "commits_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717/commits",
    "review_comments_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717/comments",
    "review_comment_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments%7B/number%7D",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717/comments",
    "statuses_url": "https://api.github.com/repos/bitcoin/bitcoin/statuses/3add23454624c4c79c9eebc060b6fbed4e3131a7",
    "number": 25717,
    "state": "closed",
    "locked": false,
    "maintainer_can_modify": false,
    "title": "p2p: Implement anti-DoS headers sync",
    "user": {
      "login": "sdaftuar",
      "id": 7463573,
      "node_id": "MDQ6VXNlcjc0NjM1NzM=",
      "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sdaftuar",
      "html_url": "https://github.com/sdaftuar",
      "followers_url": "https://api.github.com/users/sdaftuar/followers",
      "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
      "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
      "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
      "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
      "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
      "repos_url": "https://api.github.com/users/sdaftuar/repos",
      "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
      "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
      "type": "User",
      "site_admin": false
    },
    "body": "New nodes starting up for the first time lack protection against DoS from low-difficulty headers. While checkpoints serve as our protection against headers that fork from the main chain below the known checkpointed values, this protection only applies to nodes that have been able to download the honest chain to the checkpointed heights.\r\n\r\nWe can protect all nodes from DoS from low-difficulty headers by adopting a different strategy: before we commit to storing a header in permanent storage, first verify that the header is part of a chain that has sufficiently high work (either `nMinimumChainWork`, or something comparable to our tip). This means that we will download headers from a given peer twice: once to verify the work on the chain, and a second time when permanently storing the headers.\r\n\r\nThe p2p protocol doesn't provide an easy way for us to ensure that we receive the same headers during the second download of peer's headers chain. To ensure that a peer doesn't (say) give us the main chain in phase 1 to trick us into permanently storing an alternate, low-work chain in phase 2, we store commitments to the headers during our first download, which we validate in the second download.\r\n\r\nSome parameters must be chosen for commitment size/frequency in phase 1, and validation of commitments in phase 2. In this PR, those parameters are chosen to both (a) minimize the per-peer memory usage that an attacker could utilize, and (b) bound the expected amount of permanent memory that an attacker could get us to use to be well-below the memory growth that we'd get from the honest chain (where we expect 1 new block header every 10 minutes).\r\n\r\nAfter this PR, we should be able to remove checkpoints from our code, which is a nice philosophical change for us to make as well, as there has been confusion over the years about the role checkpoints play in Bitcoin's consensus algorithm.\r\n\r\nThanks to Pieter Wuille for collaborating on this design.",
    "labels": [
      {
        "id": 98298007,
        "node_id": "MDU6TGFiZWw5ODI5ODAwNw==",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/labels/P2P",
        "name": "P2P",
        "color": "006b75",
        "default": false
      }
    ],
    "milestone": {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/milestones/54",
      "html_url": "https://github.com/bitcoin/bitcoin/milestone/54",
      "labels_url": "https://api.github.com/repos/bitcoin/bitcoin/milestones/54/labels",
      "id": 7150037,
      "node_id": "MI_kwDOABII584AbRnV",
      "number": 54,
      "state": "closed",
      "title": "24.0",
      "description": "",
      "creator": {
        "login": "laanwj",
        "id": 126646,
        "node_id": "MDQ6VXNlcjEyNjY0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/laanwj",
        "html_url": "https://github.com/laanwj",
        "followers_url": "https://api.github.com/users/laanwj/followers",
        "following_url": "https://api.github.com/users/laanwj/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/laanwj/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/laanwj/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
        "organizations_url": "https://api.github.com/users/laanwj/orgs",
        "repos_url": "https://api.github.com/users/laanwj/repos",
        "events_url": "https://api.github.com/users/laanwj/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/laanwj/received_events",
        "type": "User",
        "site_admin": false
      },
      "open_issues": 0,
      "closed_issues": 159,
      "created_at": "2021-09-14T08:27:02Z",
      "updated_at": "2022-11-29T15:44:44Z",
      "closed_at": "2022-11-29T15:44:44Z"
    },
    "created_at": "2022-07-26T20:35:09Z",
    "updated_at": "2022-10-13T21:25:22Z",
    "closed_at": "2022-08-30T14:38:32Z",
    "mergeable_state": "unknown",
    "merged_at": "2022-08-30T14:38:31Z",
    "merge_commit_sha": "e9035f867a36a430998e3811385958229ac79cf5",
    "assignees": [],
    "requested_reviewers": [],
    "requested_teams": [],
    "head": {
      "label": "sdaftuar:2022-02-headers-dos-prevention",
      "ref": "2022-02-headers-dos-prevention",
      "sha": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "repo": {
        "id": 28761781,
        "node_id": "MDEwOlJlcG9zaXRvcnkyODc2MTc4MQ==",
        "name": "bitcoin",
        "full_name": "sdaftuar/bitcoin",
        "owner": {
          "login": "sdaftuar",
          "id": 7463573,
          "node_id": "MDQ6VXNlcjc0NjM1NzM=",
          "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/sdaftuar",
          "html_url": "https://github.com/sdaftuar",
          "followers_url": "https://api.github.com/users/sdaftuar/followers",
          "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
          "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
          "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
          "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
          "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
          "repos_url": "https://api.github.com/users/sdaftuar/repos",
          "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
          "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
          "type": "User",
          "site_admin": false
        },
        "private": false,
        "html_url": "https://github.com/sdaftuar/bitcoin",
        "description": "Bitcoin Core integration/staging tree",
        "fork": true,
        "url": "https://api.github.com/repos/sdaftuar/bitcoin",
        "archive_url": "https://api.github.com/repos/sdaftuar/bitcoin/%7Barchive_format%7D%7B/ref%7D",
        "assignees_url": "https://api.github.com/repos/sdaftuar/bitcoin/assignees%7B/user%7D",
        "blobs_url": "https://api.github.com/repos/sdaftuar/bitcoin/git/blobs%7B/sha%7D",
        "branches_url": "https://api.github.com/repos/sdaftuar/bitcoin/branches%7B/branch%7D",
        "collaborators_url": "https://api.github.com/repos/sdaftuar/bitcoin/collaborators%7B/collaborator%7D",
        "comments_url": "https://api.github.com/repos/sdaftuar/bitcoin/comments%7B/number%7D",
        "commits_url": "https://api.github.com/repos/sdaftuar/bitcoin/commits%7B/sha%7D",
        "compare_url": "https://api.github.com/repos/sdaftuar/bitcoin/compare/%7Bbase%7D...%7Bhead%7D",
        "contents_url": "https://api.github.com/repos/sdaftuar/bitcoin/contents/%7B+path%7D",
        "contributors_url": "https://api.github.com/repos/sdaftuar/bitcoin/contributors",
        "deployments_url": "https://api.github.com/repos/sdaftuar/bitcoin/deployments",
        "downloads_url": "https://api.github.com/repos/sdaftuar/bitcoin/downloads",
        "events_url": "https://api.github.com/repos/sdaftuar/bitcoin/events",
        "forks_url": "https://api.github.com/repos/sdaftuar/bitcoin/forks",
        "git_commits_url": "https://api.github.com/repos/sdaftuar/bitcoin/git/commits%7B/sha%7D",
        "git_refs_url": "https://api.github.com/repos/sdaftuar/bitcoin/git/refs%7B/sha%7D",
        "git_tags_url": "https://api.github.com/repos/sdaftuar/bitcoin/git/tags%7B/sha%7D",
        "git_url": "git://github.com/sdaftuar/bitcoin.git",
        "issue_comment_url": "https://api.github.com/repos/sdaftuar/bitcoin/issues/comments%7B/number%7D",
        "issue_events_url": "https://api.github.com/repos/sdaftuar/bitcoin/issues/events%7B/number%7D",
        "issues_url": "https://api.github.com/repos/sdaftuar/bitcoin/issues%7B/number%7D",
        "keys_url": "https://api.github.com/repos/sdaftuar/bitcoin/keys%7B/key_id%7D",
        "labels_url": "https://api.github.com/repos/sdaftuar/bitcoin/labels%7B/name%7D",
        "languages_url": "https://api.github.com/repos/sdaftuar/bitcoin/languages",
        "merges_url": "https://api.github.com/repos/sdaftuar/bitcoin/merges",
        "milestones_url": "https://api.github.com/repos/sdaftuar/bitcoin/milestones%7B/number%7D",
        "notifications_url": "https://api.github.com/repos/sdaftuar/bitcoin/notifications%7B?since,all,participating}",
        "pulls_url": "https://api.github.com/repos/sdaftuar/bitcoin/pulls%7B/number%7D",
        "releases_url": "https://api.github.com/repos/sdaftuar/bitcoin/releases%7B/id%7D",
        "ssh_url": "git@github.com:sdaftuar/bitcoin.git",
        "stargazers_url": "https://api.github.com/repos/sdaftuar/bitcoin/stargazers",
        "statuses_url": "https://api.github.com/repos/sdaftuar/bitcoin/statuses/%7Bsha%7D",
        "subscribers_url": "https://api.github.com/repos/sdaftuar/bitcoin/subscribers",
        "subscription_url": "https://api.github.com/repos/sdaftuar/bitcoin/subscription",
        "tags_url": "https://api.github.com/repos/sdaftuar/bitcoin/tags",
        "teams_url": "https://api.github.com/repos/sdaftuar/bitcoin/teams",
        "trees_url": "https://api.github.com/repos/sdaftuar/bitcoin/git/trees%7B/sha%7D",
        "clone_url": "https://github.com/sdaftuar/bitcoin.git",
        "hooks_url": "https://api.github.com/repos/sdaftuar/bitcoin/hooks",
        "svn_url": "https://github.com/sdaftuar/bitcoin",
        "homepage": "https://bitcoin.org/en/download",
        "language": "C++",
        "forks_count": 1,
        "stargazers_count": 3,
        "watchers_count": 3,
        "size": 245665,
        "default_branch": "master",
        "open_issues_count": 1,
        "is_template": false,
        "topics": [],
        "has_issues": false,
        "has_projects": true,
        "has_wiki": false,
        "has_pages": false,
        "has_downloads": false,
        "archived": false,
        "disabled": false,
        "visibility": "public",
        "pushed_at": "2023-06-06T22:41:53Z",
        "created_at": "2015-01-04T02:52:13Z",
        "updated_at": "2023-02-11T10:16:01Z",
        "license": {
          "key": "mit",
          "name": "MIT License",
          "node_id": "MDc6TGljZW5zZTEz",
          "spdx_id": "MIT",
          "url": "https://api.github.com/licenses/mit",
          "html_url": null,
          "description": null,
          "implementation": null,
          "permissions": null,
          "conditions": null,
          "limitations": null,
          "body": null,
          "featured": null
        }
      }
    },
    "base": {
      "label": "bitcoin:master",
      "ref": "master",
      "sha": "e191fac4f3c37820f0618f72f0a8e8b524531ab8",
      "user": {
        "login": "bitcoin",
        "id": 528860,
        "node_id": "MDEyOk9yZ2FuaXphdGlvbjUyODg2MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/528860?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/bitcoin",
        "html_url": "https://github.com/bitcoin",
        "followers_url": "https://api.github.com/users/bitcoin/followers",
        "following_url": "https://api.github.com/users/bitcoin/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/bitcoin/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/bitcoin/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/bitcoin/subscriptions",
        "organizations_url": "https://api.github.com/users/bitcoin/orgs",
        "repos_url": "https://api.github.com/users/bitcoin/repos",
        "events_url": "https://api.github.com/users/bitcoin/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/bitcoin/received_events",
        "type": "Organization",
        "site_admin": false
      },
      "repo": {
        "id": 1181927,
        "node_id": "MDEwOlJlcG9zaXRvcnkxMTgxOTI3",
        "name": "bitcoin",
        "full_name": "bitcoin/bitcoin",
        "owner": {
          "login": "bitcoin",
          "id": 528860,
          "node_id": "MDEyOk9yZ2FuaXphdGlvbjUyODg2MA==",
          "avatar_url": "https://avatars.githubusercontent.com/u/528860?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/bitcoin",
          "html_url": "https://github.com/bitcoin",
          "followers_url": "https://api.github.com/users/bitcoin/followers",
          "following_url": "https://api.github.com/users/bitcoin/following%7B/other_user%7D",
          "gists_url": "https://api.github.com/users/bitcoin/gists%7B/gist_id%7D",
          "starred_url": "https://api.github.com/users/bitcoin/starred%7B/owner%7D%7B/repo%7D",
          "subscriptions_url": "https://api.github.com/users/bitcoin/subscriptions",
          "organizations_url": "https://api.github.com/users/bitcoin/orgs",
          "repos_url": "https://api.github.com/users/bitcoin/repos",
          "events_url": "https://api.github.com/users/bitcoin/events%7B/privacy%7D",
          "received_events_url": "https://api.github.com/users/bitcoin/received_events",
          "type": "Organization",
          "site_admin": false
        },
        "private": false,
        "html_url": "https://github.com/bitcoin/bitcoin",
        "description": "Bitcoin Core integration/staging tree",
        "fork": false,
        "url": "https://api.github.com/repos/bitcoin/bitcoin",
        "archive_url": "https://api.github.com/repos/bitcoin/bitcoin/%7Barchive_format%7D%7B/ref%7D",
        "assignees_url": "https://api.github.com/repos/bitcoin/bitcoin/assignees%7B/user%7D",
        "blobs_url": "https://api.github.com/repos/bitcoin/bitcoin/git/blobs%7B/sha%7D",
        "branches_url": "https://api.github.com/repos/bitcoin/bitcoin/branches%7B/branch%7D",
        "collaborators_url": "https://api.github.com/repos/bitcoin/bitcoin/collaborators%7B/collaborator%7D",
        "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/comments%7B/number%7D",
        "commits_url": "https://api.github.com/repos/bitcoin/bitcoin/commits%7B/sha%7D",
        "compare_url": "https://api.github.com/repos/bitcoin/bitcoin/compare/%7Bbase%7D...%7Bhead%7D",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/%7B+path%7D",
        "contributors_url": "https://api.github.com/repos/bitcoin/bitcoin/contributors",
        "deployments_url": "https://api.github.com/repos/bitcoin/bitcoin/deployments",
        "downloads_url": "https://api.github.com/repos/bitcoin/bitcoin/downloads",
        "events_url": "https://api.github.com/repos/bitcoin/bitcoin/events",
        "forks_url": "https://api.github.com/repos/bitcoin/bitcoin/forks",
        "git_commits_url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits%7B/sha%7D",
        "git_refs_url": "https://api.github.com/repos/bitcoin/bitcoin/git/refs%7B/sha%7D",
        "git_tags_url": "https://api.github.com/repos/bitcoin/bitcoin/git/tags%7B/sha%7D",
        "git_url": "git://github.com/bitcoin/bitcoin.git",
        "issue_comment_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments%7B/number%7D",
        "issue_events_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events%7B/number%7D",
        "issues_url": "https://api.github.com/repos/bitcoin/bitcoin/issues%7B/number%7D",
        "keys_url": "https://api.github.com/repos/bitcoin/bitcoin/keys%7B/key_id%7D",
        "labels_url": "https://api.github.com/repos/bitcoin/bitcoin/labels%7B/name%7D",
        "languages_url": "https://api.github.com/repos/bitcoin/bitcoin/languages",
        "merges_url": "https://api.github.com/repos/bitcoin/bitcoin/merges",
        "milestones_url": "https://api.github.com/repos/bitcoin/bitcoin/milestones%7B/number%7D",
        "notifications_url": "https://api.github.com/repos/bitcoin/bitcoin/notifications%7B?since,all,participating}",
        "pulls_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls%7B/number%7D",
        "releases_url": "https://api.github.com/repos/bitcoin/bitcoin/releases%7B/id%7D",
        "ssh_url": "git@github.com:bitcoin/bitcoin.git",
        "stargazers_url": "https://api.github.com/repos/bitcoin/bitcoin/stargazers",
        "statuses_url": "https://api.github.com/repos/bitcoin/bitcoin/statuses/%7Bsha%7D",
        "subscribers_url": "https://api.github.com/repos/bitcoin/bitcoin/subscribers",
        "subscription_url": "https://api.github.com/repos/bitcoin/bitcoin/subscription",
        "tags_url": "https://api.github.com/repos/bitcoin/bitcoin/tags",
        "teams_url": "https://api.github.com/repos/bitcoin/bitcoin/teams",
        "trees_url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees%7B/sha%7D",
        "clone_url": "https://github.com/bitcoin/bitcoin.git",
        "hooks_url": "https://api.github.com/repos/bitcoin/bitcoin/hooks",
        "svn_url": "https://github.com/bitcoin/bitcoin",
        "homepage": "https://bitcoincore.org/en/download",
        "language": "C++",
        "forks_count": 34324,
        "stargazers_count": 69818,
        "watchers_count": 69818,
        "size": 233879,
        "default_branch": "master",
        "open_issues_count": 627,
        "is_template": false,
        "topics": [
          "bitcoin",
          "c-plus-plus",
          "cryptocurrency",
          "cryptography",
          "p2p"
        ],
        "has_issues": true,
        "has_projects": true,
        "has_wiki": false,
        "has_pages": false,
        "has_downloads": false,
        "archived": false,
        "disabled": false,
        "visibility": "public",
        "pushed_at": "2023-06-07T05:35:10Z",
        "created_at": "2010-12-19T15:16:43Z",
        "updated_at": "2023-06-07T06:49:43Z",
        "license": {
          "key": "mit",
          "name": "MIT License",
          "node_id": "MDc6TGljZW5zZTEz",
          "spdx_id": "MIT",
          "url": "https://api.github.com/licenses/mit",
          "html_url": null,
          "description": null,
          "implementation": null,
          "permissions": null,
          "conditions": null,
          "limitations": null,
          "body": null,
          "featured": null
        }
      }
    },
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
      }
    },
    "author_association": "MEMBER",
    "draft": false,
    "additions": 2710,
    "deletions": 149,
    "changed_files": 55,
    "commits": 14,
    "review_comments": 326,
    "comments": 68
  },
  "events": [
    {
      "event": "labeled",
      "id": 7068009458,
      "node_id": "LE_lADOABII585OmiYczwAAAAGlSUPy",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7068009458",
      "actor": {
        "login": "DrahtBot",
        "id": 39886733,
        "node_id": "MDQ6VXNlcjM5ODg2NzMz",
        "avatar_url": "https://avatars.githubusercontent.com/u/39886733?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/DrahtBot",
        "html_url": "https://github.com/DrahtBot",
        "followers_url": "https://api.github.com/users/DrahtBot/followers",
        "following_url": "https://api.github.com/users/DrahtBot/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/DrahtBot/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/DrahtBot/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/DrahtBot/subscriptions",
        "organizations_url": "https://api.github.com/users/DrahtBot/orgs",
        "repos_url": "https://api.github.com/users/DrahtBot/repos",
        "events_url": "https://api.github.com/users/DrahtBot/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/DrahtBot/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-26T20:43:01Z",
      "label": {
        "name": "P2P",
        "color": "006b75"
      }
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7068064706,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGlShvC",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7068064706",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-26T20:53:17Z"
    },
    {
      "event": "commented",
      "id": 1196096355,
      "node_id": "IC_kwDOABII585HSvtj",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1196096355",
      "actor": {
        "login": "jamesob",
        "id": 73197,
        "node_id": "MDQ6VXNlcjczMTk3",
        "avatar_url": "https://avatars.githubusercontent.com/u/73197?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jamesob",
        "html_url": "https://github.com/jamesob",
        "followers_url": "https://api.github.com/users/jamesob/followers",
        "following_url": "https://api.github.com/users/jamesob/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jamesob/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jamesob/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jamesob/subscriptions",
        "organizations_url": "https://api.github.com/users/jamesob/orgs",
        "repos_url": "https://api.github.com/users/jamesob/repos",
        "events_url": "https://api.github.com/users/jamesob/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jamesob/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-26T23:32:30Z",
      "updated_at": "2022-07-26T23:32:30Z",
      "author_association": "MEMBER",
      "body": "Concept ACK, will review soon",
      "user": {
        "login": "jamesob",
        "id": 73197,
        "node_id": "MDQ6VXNlcjczMTk3",
        "avatar_url": "https://avatars.githubusercontent.com/u/73197?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jamesob",
        "html_url": "https://github.com/jamesob",
        "followers_url": "https://api.github.com/users/jamesob/followers",
        "following_url": "https://api.github.com/users/jamesob/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jamesob/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jamesob/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jamesob/subscriptions",
        "organizations_url": "https://api.github.com/users/jamesob/orgs",
        "repos_url": "https://api.github.com/users/jamesob/repos",
        "events_url": "https://api.github.com/users/jamesob/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jamesob/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1196096355",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "reviewed",
      "id": 1051733475,
      "node_id": "PRR_kwDOABII584-sC3j",
      "url": null,
      "actor": null,
      "commit_id": "fed7ff4ea5810f0e6eb850355a594697d18e041d",
      "commit_url": null,
      "created_at": null,
      "author_association": "NONE",
      "user": {
        "login": "lish2099",
        "id": 97258533,
        "node_id": "U_kgDOBcwMJQ",
        "avatar_url": "https://avatars.githubusercontent.com/u/97258533?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/lish2099",
        "html_url": "https://github.com/lish2099",
        "followers_url": "https://api.github.com/users/lish2099/followers",
        "following_url": "https://api.github.com/users/lish2099/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/lish2099/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/lish2099/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/lish2099/subscriptions",
        "organizations_url": "https://api.github.com/users/lish2099/orgs",
        "repos_url": "https://api.github.com/users/lish2099/repos",
        "events_url": "https://api.github.com/users/lish2099/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/lish2099/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1051733475",
      "submitted_at": "2022-07-26T23:49:38Z",
      "state": "APPROVED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "commented",
      "id": 1196300260,
      "node_id": "IC_kwDOABII585HThfk",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1196300260",
      "actor": {
        "login": "DrahtBot",
        "id": 39886733,
        "node_id": "MDQ6VXNlcjM5ODg2NzMz",
        "avatar_url": "https://avatars.githubusercontent.com/u/39886733?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/DrahtBot",
        "html_url": "https://github.com/DrahtBot",
        "followers_url": "https://api.github.com/users/DrahtBot/followers",
        "following_url": "https://api.github.com/users/DrahtBot/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/DrahtBot/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/DrahtBot/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/DrahtBot/subscriptions",
        "organizations_url": "https://api.github.com/users/DrahtBot/orgs",
        "repos_url": "https://api.github.com/users/DrahtBot/repos",
        "events_url": "https://api.github.com/users/DrahtBot/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/DrahtBot/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T06:03:31Z",
      "updated_at": "2022-08-26T11:05:14Z",
      "author_association": "MEMBER",
      "body": "<!--e57a25ab6845829454e8d69fc972939a-->\n\nThe following sections might be updated with supplementary metadata relevant to reviewers and maintainers.\n\n<!--174a7506f384e20aa4161008e828411d-->\n### Conflicts\nReviewers, this pull request conflicts with the following ones:\n\n* [#25923](https://github.com/bitcoin/bitcoin/pull/25923) (p2p, bugfix: always provide CNodeStateStats, ensure v23 getpeerinfo API for v24 by jonatack)\n* [#25725](https://github.com/bitcoin/bitcoin/pull/25725) (consensus: Remove mainnet checkpoints by sdaftuar)\n* [#25673](https://github.com/bitcoin/bitcoin/pull/25673) (refactor: make member functions const when applicable by aureleoules)\n* [#25555](https://github.com/bitcoin/bitcoin/pull/25555) (refactor: Move m_num_unconnecting_headers_msgs out of cs_main guard by MarcoFalke)\n* [#24571](https://github.com/bitcoin/bitcoin/pull/24571) (p2p: Prevent block index fingerprinting by sending additional getheaders messages by dergoegge)\n* [#23443](https://github.com/bitcoin/bitcoin/pull/23443) (p2p: Erlay support signaling by naumenkogs)\n* [#23352](https://github.com/bitcoin/bitcoin/pull/23352) (test: Extend stale_tip_peer_management test by MarcoFalke)\n* [#18933](https://github.com/bitcoin/bitcoin/pull/18933) (rpc: Add submit option to generateblock by MarcoFalke)\n* [#18470](https://github.com/bitcoin/bitcoin/pull/18470) (test: Extend stale tip test by MarcoFalke)\n* [#17860](https://github.com/bitcoin/bitcoin/pull/17860) (fuzz: BIP 42, BIP 30, CVE-2018-17144 by MarcoFalke)\n* [#16981](https://github.com/bitcoin/bitcoin/pull/16981) (Improve runtime performance of --reindex by LarryRuane)\n* [#10102](https://github.com/bitcoin/bitcoin/pull/10102) (Multiprocess bitcoin by ryanofsky)\n\nIf you consider this pull request important, please also help to review the conflicting pull requests. Ideally, start with the one that should be merged first.",
      "user": {
        "login": "DrahtBot",
        "id": 39886733,
        "node_id": "MDQ6VXNlcjM5ODg2NzMz",
        "avatar_url": "https://avatars.githubusercontent.com/u/39886733?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/DrahtBot",
        "html_url": "https://github.com/DrahtBot",
        "followers_url": "https://api.github.com/users/DrahtBot/followers",
        "following_url": "https://api.github.com/users/DrahtBot/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/DrahtBot/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/DrahtBot/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/DrahtBot/subscriptions",
        "organizations_url": "https://api.github.com/users/DrahtBot/orgs",
        "repos_url": "https://api.github.com/users/DrahtBot/repos",
        "events_url": "https://api.github.com/users/DrahtBot/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/DrahtBot/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1196300260",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "review_requested",
      "id": 7072340932,
      "node_id": "RRE_lADOABII585OmiYczwAAAAGli1vE",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7072340932",
      "actor": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T12:03:12Z",
      "requested_reviewer": {
        "login": "dergoegge",
        "id": 8077169,
        "node_id": "MDQ6VXNlcjgwNzcxNjk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8077169?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/dergoegge",
        "html_url": "https://github.com/dergoegge",
        "followers_url": "https://api.github.com/users/dergoegge/followers",
        "following_url": "https://api.github.com/users/dergoegge/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/dergoegge/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/dergoegge/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/dergoegge/subscriptions",
        "organizations_url": "https://api.github.com/users/dergoegge/orgs",
        "repos_url": "https://api.github.com/users/dergoegge/repos",
        "events_url": "https://api.github.com/users/dergoegge/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/dergoegge/received_events",
        "type": "User",
        "site_admin": false
      },
      "review_requester": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      }
    },
    {
      "event": "review_requested",
      "id": 7072340939,
      "node_id": "RRE_lADOABII585OmiYczwAAAAGli1vL",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7072340939",
      "actor": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T12:03:12Z",
      "requested_reviewer": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "review_requester": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      }
    },
    {
      "event": "review_requested",
      "id": 7072342060,
      "node_id": "RRE_lADOABII585OmiYczwAAAAGli2As",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7072342060",
      "actor": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T12:03:22Z",
      "requested_reviewer": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "review_requester": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      }
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7073002808,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGllXU4",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7073002808",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T13:27:43Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7073472231,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGlnJ7n",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7073472231",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T14:22:27Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7073594974,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGlnn5e",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7073594974",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T14:36:35Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7073818396,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGloecc",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7073818396",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T15:01:45Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7074125362,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGlppYy",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7074125362",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T15:38:44Z"
    },
    {
      "event": "commented",
      "id": 1196948065,
      "node_id": "IC_kwDOABII585HV_ph",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1196948065",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T15:59:13Z",
      "updated_at": "2022-07-27T15:59:13Z",
      "author_association": "MEMBER",
      "body": "It's worth pointing out that initial headers sync will be slowed down by this PR, not just because we will download headers twice before storing, but also because the additional latency to make progress on syncing with our initial headers-sync-peer will mean that we're more likely to receive a block announcement while waiting for headers sync to finish, which in turn will trigger a headers sync with all our peers that announce the block. (And our starting point for those syncs will be further behind, because we don't make progress on adding headers to our block index until phase 2.)\r\n\r\nI opened #25720 as a mitigation for this effect; many strategies are possible and I think they are orthogonal to the change proposed here, so I'd prefer to leave the discussion of this particular issue to that PR.",
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1196948065",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1197297765,
      "node_id": "IC_kwDOABII585HXVBl",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1197297765",
      "actor": {
        "login": "luke-jr",
        "id": 1095675,
        "node_id": "MDQ6VXNlcjEwOTU2NzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/luke-jr",
        "html_url": "https://github.com/luke-jr",
        "followers_url": "https://api.github.com/users/luke-jr/followers",
        "following_url": "https://api.github.com/users/luke-jr/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/luke-jr/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/luke-jr/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
        "organizations_url": "https://api.github.com/users/luke-jr/orgs",
        "repos_url": "https://api.github.com/users/luke-jr/repos",
        "events_url": "https://api.github.com/users/luke-jr/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/luke-jr/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T19:53:09Z",
      "updated_at": "2022-07-27T19:53:09Z",
      "author_association": "MEMBER",
      "body": "This seems to be based on the assumption that the DoS is attacking disk space, but bandwidth tends to be more limited than space, and it makes that worse even in the best scenario...?",
      "user": {
        "login": "luke-jr",
        "id": 1095675,
        "node_id": "MDQ6VXNlcjEwOTU2NzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/luke-jr",
        "html_url": "https://github.com/luke-jr",
        "followers_url": "https://api.github.com/users/luke-jr/followers",
        "following_url": "https://api.github.com/users/luke-jr/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/luke-jr/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/luke-jr/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
        "organizations_url": "https://api.github.com/users/luke-jr/orgs",
        "repos_url": "https://api.github.com/users/luke-jr/repos",
        "events_url": "https://api.github.com/users/luke-jr/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/luke-jr/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1197297765",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1197328410,
      "node_id": "IC_kwDOABII585HXcga",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1197328410",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T20:24:19Z",
      "updated_at": "2022-07-27T20:24:42Z",
      "author_association": "MEMBER",
      "body": "@luke-jr The DoS concern is primarily about memory usage: filling mapBlockIndex and other data structures with low-difficulty headers before a checkpoint is reached.\r\n\r\nBandwidth is a concern for sure, but:\r\n* There are many more effective ways to perform bandwidth DoS (like spamming INV messages, ADDR messages, PING messages, continuously requesting non-existing transactions or blocks, ...).\r\n* The proper solution to bandwidth DoS is just throttling peers that waste too much of it.\r\n* In non-attack scenarios, the added bandwidth by this PR (especially when combined with #25720) is in the order of 10s of MB over a node's lifetime, which is negligible compared to the full block download.\r\n",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1197328410",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7076323184,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGlyB9w",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7076323184",
      "actor": {
        "login": "luke-jr",
        "id": 1095675,
        "node_id": "MDQ6VXNlcjEwOTU2NzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/luke-jr",
        "html_url": "https://github.com/luke-jr",
        "followers_url": "https://api.github.com/users/luke-jr/followers",
        "following_url": "https://api.github.com/users/luke-jr/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/luke-jr/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/luke-jr/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
        "organizations_url": "https://api.github.com/users/luke-jr/orgs",
        "repos_url": "https://api.github.com/users/luke-jr/repos",
        "events_url": "https://api.github.com/users/luke-jr/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/luke-jr/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T20:24:20Z"
    },
    {
      "event": "subscribed",
      "id": 7076323190,
      "node_id": "SE_lADOABII585OmiYczwAAAAGlyB92",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7076323190",
      "actor": {
        "login": "luke-jr",
        "id": 1095675,
        "node_id": "MDQ6VXNlcjEwOTU2NzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/luke-jr",
        "html_url": "https://github.com/luke-jr",
        "followers_url": "https://api.github.com/users/luke-jr/followers",
        "following_url": "https://api.github.com/users/luke-jr/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/luke-jr/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/luke-jr/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
        "organizations_url": "https://api.github.com/users/luke-jr/orgs",
        "repos_url": "https://api.github.com/users/luke-jr/repos",
        "events_url": "https://api.github.com/users/luke-jr/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/luke-jr/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T20:24:20Z"
    },
    {
      "event": "commented",
      "id": 1197336868,
      "node_id": "IC_kwDOABII585HXekk",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1197336868",
      "actor": {
        "login": "luke-jr",
        "id": 1095675,
        "node_id": "MDQ6VXNlcjEwOTU2NzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/luke-jr",
        "html_url": "https://github.com/luke-jr",
        "followers_url": "https://api.github.com/users/luke-jr/followers",
        "following_url": "https://api.github.com/users/luke-jr/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/luke-jr/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/luke-jr/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
        "organizations_url": "https://api.github.com/users/luke-jr/orgs",
        "repos_url": "https://api.github.com/users/luke-jr/repos",
        "events_url": "https://api.github.com/users/luke-jr/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/luke-jr/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T20:33:17Z",
      "updated_at": "2022-07-27T20:33:17Z",
      "author_association": "MEMBER",
      "body": "Is there a reason to not just prune the block index under some conditions?",
      "user": {
        "login": "luke-jr",
        "id": 1095675,
        "node_id": "MDQ6VXNlcjEwOTU2NzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/luke-jr",
        "html_url": "https://github.com/luke-jr",
        "followers_url": "https://api.github.com/users/luke-jr/followers",
        "following_url": "https://api.github.com/users/luke-jr/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/luke-jr/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/luke-jr/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
        "organizations_url": "https://api.github.com/users/luke-jr/orgs",
        "repos_url": "https://api.github.com/users/luke-jr/repos",
        "events_url": "https://api.github.com/users/luke-jr/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/luke-jr/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1197336868",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1197375266,
      "node_id": "IC_kwDOABII585HXn8i",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1197375266",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T21:14:13Z",
      "updated_at": "2022-07-28T02:33:26Z",
      "author_association": "MEMBER",
      "body": "Here is the script to compute the optimal parameters used, with lots of explanation: https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1 (it takes around 20s for me in `pypy3`, though over 2 minutes in `python3`).\r\n\r\nIt may make sense to include the script in the repository (as I can imagine it being re-run to tune things occasionally, but there should not be a need to do it more than every few years). It could also live on the devwiki or just linked-to in the code. Opinions?\r\n\r\n",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1197375266",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1197439146,
      "node_id": "IC_kwDOABII585HX3iq",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1197439146",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T22:29:56Z",
      "updated_at": "2022-07-27T22:29:56Z",
      "author_association": "MEMBER",
      "body": "@luke-jr I think that's a potentially useful but orthogonal improvement, though there is probably much less need for it after this PR.",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1197439146",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7076922275,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGl0UOj",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7076922275",
      "actor": {
        "login": "luke-jr",
        "id": 1095675,
        "node_id": "MDQ6VXNlcjEwOTU2NzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/luke-jr",
        "html_url": "https://github.com/luke-jr",
        "followers_url": "https://api.github.com/users/luke-jr/followers",
        "following_url": "https://api.github.com/users/luke-jr/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/luke-jr/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/luke-jr/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
        "organizations_url": "https://api.github.com/users/luke-jr/orgs",
        "repos_url": "https://api.github.com/users/luke-jr/repos",
        "events_url": "https://api.github.com/users/luke-jr/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/luke-jr/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T22:29:56Z"
    },
    {
      "event": "subscribed",
      "id": 7076922280,
      "node_id": "SE_lADOABII585OmiYczwAAAAGl0UOo",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7076922280",
      "actor": {
        "login": "luke-jr",
        "id": 1095675,
        "node_id": "MDQ6VXNlcjEwOTU2NzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/luke-jr",
        "html_url": "https://github.com/luke-jr",
        "followers_url": "https://api.github.com/users/luke-jr/followers",
        "following_url": "https://api.github.com/users/luke-jr/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/luke-jr/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/luke-jr/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
        "organizations_url": "https://api.github.com/users/luke-jr/orgs",
        "repos_url": "https://api.github.com/users/luke-jr/repos",
        "events_url": "https://api.github.com/users/luke-jr/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/luke-jr/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-27T22:29:56Z"
    },
    {
      "event": "reviewed",
      "id": 1054040362,
      "node_id": "PRR_kwDOABII584-02Eq",
      "url": null,
      "actor": null,
      "commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "Concept & Approach ACK\r\n\r\nLeft some comments (mostly about the added functional test)",
      "user": {
        "login": "dergoegge",
        "id": 8077169,
        "node_id": "MDQ6VXNlcjgwNzcxNjk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8077169?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/dergoegge",
        "html_url": "https://github.com/dergoegge",
        "followers_url": "https://api.github.com/users/dergoegge/followers",
        "following_url": "https://api.github.com/users/dergoegge/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/dergoegge/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/dergoegge/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/dergoegge/subscriptions",
        "organizations_url": "https://api.github.com/users/dergoegge/orgs",
        "repos_url": "https://api.github.com/users/dergoegge/repos",
        "events_url": "https://api.github.com/users/dergoegge/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/dergoegge/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1054040362",
      "submitted_at": "2022-07-28T14:41:32Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "reviewed",
      "id": 1054116591,
      "node_id": "PRR_kwDOABII584-1Irv",
      "url": null,
      "actor": null,
      "commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "Just comments on the headerssync.* module so far.",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1054116591",
      "submitted_at": "2022-07-28T14:52:00Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "commented",
      "id": 1198490639,
      "node_id": "IC_kwDOABII585Hb4QP",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1198490639",
      "actor": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-28T18:22:55Z",
      "updated_at": "2022-07-28T18:22:55Z",
      "author_association": "MEMBER",
      "body": "concept ACK, cool to see such an old idea actually get implemented",
      "user": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1198490639",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "reviewed",
      "id": 1054584671,
      "node_id": "PRR_kwDOABII584-269f",
      "url": null,
      "actor": null,
      "commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "",
      "user": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1054584671",
      "submitted_at": "2022-07-28T18:50:21Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7085356223,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGmUfS_",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7085356223",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-28T22:10:41Z"
    },
    {
      "event": "reviewed",
      "id": 1055265006,
      "node_id": "PRR_kwDOABII584-5hDu",
      "url": null,
      "actor": null,
      "commit_id": "ec2723b39f4a994938d2d7f14586dcde823fe37e",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "",
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1055265006",
      "submitted_at": "2022-07-29T09:54:43Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7089688472,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGmlA-Y",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7089688472",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-07-29T13:56:56Z"
    },
    {
      "event": "reviewed",
      "id": 1055426992,
      "node_id": "PRR_kwDOABII584-6Imw",
      "url": null,
      "actor": null,
      "commit_id": "e3260960add016aeb43d0be4c5b735418e37c4a7",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "Approach ACK, still reviewing the code. Ran this on mainnet a few times as a sanity check, synced fine. Ran the fuzzer for a while.\r\n\r\n> It may make sense to include the script in the repository (as I can imagine it being re-run to tune things occasionally, but there should not be a need to do it more than every few years). It could also live on the devwiki or just linked-to in the code. Opinions?\r\n\r\nMaybe contrib/devtools? Devwiki seems fine too, no strong opinion.\r\n\r\nI have some questions trying to understand headerssync_params.py. Why is `BLOCK_INTERVAL = timedelta(seconds=598, microseconds=800000)` rather than exactly 10min ([link](https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1#file-headersync_params-py-L13))? Is it for expected hashrate increase? And based on \"especially as this attack is only possible before the victim has learned about the honest chain\" ([link](https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1#file-headersync_params-py-L86-L89)) does this mean we should always attempt to sync headers from all/multiple outbound peers at once?",
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1055426992",
      "submitted_at": "2022-08-01T16:19:08Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "reviewed",
      "id": 1055897986,
      "node_id": "PRR_kwDOABII584-77mC",
      "url": null,
      "actor": null,
      "commit_id": "e3260960add016aeb43d0be4c5b735418e37c4a7",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1055897986",
      "submitted_at": "2022-08-01T21:30:45Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "commented",
      "id": 1201763035,
      "node_id": "IC_kwDOABII585HoXLb",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1201763035",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-01T21:56:07Z",
      "updated_at": "2022-08-02T16:49:50Z",
      "author_association": "MEMBER",
      "body": "@glozow \r\n\r\n> > It may make sense to include the script in the repository (as I can imagine it being re-run to tune things occasionally, but there should not be a need to do it more than every few years). It could also live on the devwiki or just linked-to in the code. Opinions?\r\n>\r\n> Maybe contrib/devtools? Devwiki seems fine too, no strong opinion.\r\n\r\nMy current thinking is to add it to contrib/devtools in a follow-up PR, together with instructions in the release-process to run/update it. No need to add the review burden of that here, as the current values are likely more than sufficient for at least a few years.\r\n\r\n> I have some questions trying to understand headerssync_params.py.\r\n> Why is `BLOCK_INTERVAL = timedelta(seconds=598, microseconds=800000)` rather than exactly 10min ([link](https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1#file-headersync_params-py-L13))? Is it for expected hashrate increase?\r\n\r\nYes, the number is just the average block rate in 2021, approximately. It barely matters, so I've changed it to just 600 seconds. A few percent off on this value isn't going to change the result.\r\n\r\n> And based on \"especially as this attack is only possible before the victim has learned about the honest chain\" ([link](https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1#file-headersync_params-py-L86-L89)) does this mean we should always attempt to sync headers from all/multiple outbound peers at once?\r\n\r\nI don't think so; that also worsens the attack potential in addition to reducing it, because it increases the chance that at least one of the nodes synced from will be an attacker, and gives them a window while the first (eventually) successful hasn't reached the second stage yet.\r\n\r\nAnd syncing from all peers at once is a pretty extreme position to take from a bandwidth optimization perspective (in non-attack scenarios). For most aspects of the P2P protocol, we attempt to never request the same thing twice simultaneously (this is true for transactions and blocks, except the high-bandwidth compact block mode which makes a bounded number of exceptions). Headers are small, so strictly requiring only a single header sync in flight is pretty extreme and opens up the ability for peers to stall the sync for a long time, but fetching from all at once means wasting possibly several GB of volume. #25720 picks something in between: start one sync for each new block announcement.",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1201763035",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7105151660,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGngAKs",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7105151660",
      "actor": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-01T21:56:07Z"
    },
    {
      "event": "subscribed",
      "id": 7105151663,
      "node_id": "SE_lADOABII585OmiYczwAAAAGngAKv",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7105151663",
      "actor": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-01T21:56:07Z"
    },
    {
      "event": "commented",
      "id": 1202908602,
      "node_id": "IC_kwDOABII585Hsu26",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1202908602",
      "actor": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-02T16:03:40Z",
      "updated_at": "2022-08-02T16:03:40Z",
      "author_association": "MEMBER",
      "body": "@sipa Makes sense, thanks for explaining!",
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1202908602",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7111313872,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGn3gnQ",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7111313872",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-02T16:03:40Z"
    },
    {
      "event": "subscribed",
      "id": 7111313879,
      "node_id": "SE_lADOABII585OmiYczwAAAAGn3gnX",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7111313879",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-02T16:03:40Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7113587772,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGoALw8",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7113587772",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-02T21:25:45Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7113645044,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGoAZv0",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7113645044",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-02T21:38:00Z"
    },
    {
      "event": "reviewed",
      "id": 1059486854,
      "node_id": "PRR_kwDOABII584_JnyG",
      "url": null,
      "actor": null,
      "commit_id": "e94ea7f3617db3f4b1972dd9c229baad4d4a9656",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "Concept ACK\r\n\r\nNot finished with the review yet, but I left some comments I had so far.\r\n\r\n(With the latest push, `feature_block.py` and `rpc_blockchain.py` currently fail)",
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1059486854",
      "submitted_at": "2022-08-03T05:03:18Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7118526980,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGoTBoE",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7118526980",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-03T13:14:07Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7120059128,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGoY3r4",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7120059128",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-03T16:12:06Z"
    },
    {
      "event": "reviewed",
      "id": 1062449560,
      "node_id": "PRR_kwDOABII584_U7GY",
      "url": null,
      "actor": null,
      "commit_id": "6ef78a8af3c9cec7f877f9cca9fc761d472134bf",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "",
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1062449560",
      "submitted_at": "2022-08-05T16:01:55Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "reviewed",
      "id": 1063495599,
      "node_id": "PRR_kwDOABII584_Y6ev",
      "url": null,
      "actor": null,
      "commit_id": "6ef78a8af3c9cec7f877f9cca9fc761d472134bf",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "Very cool stuff. Some initial comments. Review test hint: ` -debug=net -stopatheight=100`\r\n\r\n_Update 2022-08-06_: why do we need `bitdeque` instead of just `std::vector<bool>`? Is it only because we call `m_header_commitments.pop_front()`? Couldn't we also just keep the commitments, track our index with an integer and then delete the whole thing when we're done?",
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1063495599",
      "submitted_at": "2022-08-05T16:16:58Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "reviewed",
      "id": 1064303425,
      "node_id": "PRR_kwDOABII584_b_tB",
      "url": null,
      "actor": null,
      "commit_id": "6ef78a8af3c9cec7f877f9cca9fc761d472134bf",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "",
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1064303425",
      "submitted_at": "2022-08-06T12:51:00Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "reviewed",
      "id": 1064306189,
      "node_id": "PRR_kwDOABII584_cAYN",
      "url": null,
      "actor": null,
      "commit_id": "6ef78a8af3c9cec7f877f9cca9fc761d472134bf",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "",
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1064306189",
      "submitted_at": "2022-08-06T13:26:29Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "reviewed",
      "id": 1064330845,
      "node_id": "PRR_kwDOABII584_cGZd",
      "url": null,
      "actor": null,
      "commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1064330845",
      "submitted_at": "2022-08-06T18:04:21Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "reviewed",
      "id": 1064332620,
      "node_id": "PRR_kwDOABII584_cG1M",
      "url": null,
      "actor": null,
      "commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1064332620",
      "submitted_at": "2022-08-06T18:28:36Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "commented",
      "id": 1207404500,
      "node_id": "IC_kwDOABII585H94fU",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1207404500",
      "actor": {
        "login": "pinheadmz",
        "id": 2084648,
        "node_id": "MDQ6VXNlcjIwODQ2NDg=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2084648?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/pinheadmz",
        "html_url": "https://github.com/pinheadmz",
        "followers_url": "https://api.github.com/users/pinheadmz/followers",
        "following_url": "https://api.github.com/users/pinheadmz/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/pinheadmz/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/pinheadmz/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/pinheadmz/subscriptions",
        "organizations_url": "https://api.github.com/users/pinheadmz/orgs",
        "repos_url": "https://api.github.com/users/pinheadmz/repos",
        "events_url": "https://api.github.com/users/pinheadmz/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/pinheadmz/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-07T13:05:27Z",
      "updated_at": "2022-08-07T13:06:46Z",
      "author_association": "MEMBER",
      "body": "This subject reminds me of the \"Chain Width Expansion\" attack described in https://bcoin.io/papers/bitcoin-chain-expansion.pdf \r\n\r\nbitcoin-dev thread: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-October/017354.html\r\n\r\nIn this paper the author suggested a solution (section 4.3) using proof-of-work to limit the rate of downloading headers (as well as pruning alternate header chain branches aka \"chain width\").\r\n\r\nI wonder how this strategy compares to the double-download strategy in this PR. They might be close in total elapsed time (slow downloading vs. downloading twice) but would obviously save on bandwidth.\r\n\r\nConceptually I love the idea of replacing hard-coded checkpoints, I'm just curious how this alternate solution measures up.",
      "user": {
        "login": "pinheadmz",
        "id": 2084648,
        "node_id": "MDQ6VXNlcjIwODQ2NDg=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2084648?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/pinheadmz",
        "html_url": "https://github.com/pinheadmz",
        "followers_url": "https://api.github.com/users/pinheadmz/followers",
        "following_url": "https://api.github.com/users/pinheadmz/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/pinheadmz/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/pinheadmz/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/pinheadmz/subscriptions",
        "organizations_url": "https://api.github.com/users/pinheadmz/orgs",
        "repos_url": "https://api.github.com/users/pinheadmz/repos",
        "events_url": "https://api.github.com/users/pinheadmz/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/pinheadmz/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1207404500",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1207831433,
      "node_id": "IC_kwDOABII585H_guJ",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1207831433",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-08T08:36:32Z",
      "updated_at": "2022-08-08T08:39:24Z",
      "author_association": "MEMBER",
      "body": "@pinheadmz  of the three attack scenarios in the paper, 1.3 is prevented because we ignore unsolicited blocks with less work than the tip (see [p2p_unrequested_blocks.py](https://github.com/bitcoin/bitcoin/blob/master/test/functional/p2p_unrequested_blocks.py#L22-L24)). The paper also points this out in (2). So the two attacks to worry about are 1.1 and 1.2. If you can solve 1.2 without the use of checkpoints, then 1.1 is automatically solved.\r\n\r\nSo the comparison boils down to: how does this PR stack up against the proposal in the paper in fixing 1.2.\r\n\r\nSpecifically the solutions we're looking at are 4.2 and 4.3 in the paper, since 4.1 (headers-first sync) is already implemented.\r\n\r\nNote that bandwidth for headers is only a fraction of bandwidth for blocks. All things equal less bandwidth is better, but it's not a huge factor on its own when comparing the approaches.\r\n\r\nThe same goes for elapsed time. Downloading headers takes only a fraction of the time of downloading blocks.\r\n\r\nAlso, because we can receive headers unsolicited, the amount of time and bandwidth we spend downloading useless headers is somewhat out of our control (rate limiting based on PoW could help though, see my comment on 4.3).\r\n\r\nI think there's two metrics we should care about, given an ongoing spam attack and a single honest node whispering the truth:\r\n1. how long does it take until we know enough to go ahead and download the corresponding blocks\r\n2. how much of disk space gets wasted with spam headers (and can we prune that before it becomes a serious problem)\r\n\r\nOne meta issue is that afaik there's no Bitcoin Core pull request of 4.1 and 4.2 from the paper, so that works in favor of this PR which does have working code (from a [rfc7282](https://www.rfc-editor.org/rfc/rfc7282) point of view). But the paper states there _is_ an implementation for Bcoin, so this is not a strong argument against. What do we know of how these implementations work in practice?\r\n\r\nWith all that out the way, how do the proposals 4.2 and 4.3 from the paper actually compare to this PR?\r\n\r\n4.2 suggests picking a maximum number of header chains to store, before pruning the shortest. It suggests storing 10 chains for a total of 480MB.\r\na) that's far more memory use than this PR requires (~700 KiB per peer, regardless of the amount of spam fired at us)\r\nb) In terms of bandwidth, under nice circumstances the approach uses less bandwidth, because headers are only downloaded once. But under attack circumstance most of the bandwidth is spent on downloading spam headers, in which case the redownload of correct headers is negligible. That goes for 4.2 as well as this PR.\r\nc) I'm not sure how the proposal holds up in a timewarp attack; I suppose it could perform the sane initial sanity checks on all header chains, e.g. checking the difficulty adjustment is allowed. Dealing with a timewarp attack is important because it would dramatically increase the space needed to store a header chain.\r\n\r\n4.3 Rate limiting header requests: I'm not sure how to evaluate that (others on the mailinglist also expressed difficulty with that). I do notice the paper points to advantages for CPU use, but I'm more worried about storage. I'm also not sure if this approach would put an \"honest\" massive reorg (by alien attacker) at an unfair disadvantage, e.g.  if said reorg started with lots of low difficulty blocks. But I guess eventually we'd find it.",
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1207831433",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7145008053,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGp4Cu1",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7145008053",
      "actor": {
        "login": "pinheadmz",
        "id": 2084648,
        "node_id": "MDQ6VXNlcjIwODQ2NDg=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2084648?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/pinheadmz",
        "html_url": "https://github.com/pinheadmz",
        "followers_url": "https://api.github.com/users/pinheadmz/followers",
        "following_url": "https://api.github.com/users/pinheadmz/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/pinheadmz/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/pinheadmz/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/pinheadmz/subscriptions",
        "organizations_url": "https://api.github.com/users/pinheadmz/orgs",
        "repos_url": "https://api.github.com/users/pinheadmz/repos",
        "events_url": "https://api.github.com/users/pinheadmz/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/pinheadmz/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-08T08:36:33Z"
    },
    {
      "event": "subscribed",
      "id": 7145008059,
      "node_id": "SE_lADOABII585OmiYczwAAAAGp4Cu7",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7145008059",
      "actor": {
        "login": "pinheadmz",
        "id": 2084648,
        "node_id": "MDQ6VXNlcjIwODQ2NDg=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2084648?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/pinheadmz",
        "html_url": "https://github.com/pinheadmz",
        "followers_url": "https://api.github.com/users/pinheadmz/followers",
        "following_url": "https://api.github.com/users/pinheadmz/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/pinheadmz/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/pinheadmz/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/pinheadmz/subscriptions",
        "organizations_url": "https://api.github.com/users/pinheadmz/orgs",
        "repos_url": "https://api.github.com/users/pinheadmz/repos",
        "events_url": "https://api.github.com/users/pinheadmz/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/pinheadmz/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-08T08:36:33Z"
    },
    {
      "event": "commented",
      "id": 1207936111,
      "node_id": "IC_kwDOABII585H_6Rv",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1207936111",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-08T10:19:29Z",
      "updated_at": "2022-08-08T10:19:29Z",
      "author_association": "MEMBER",
      "body": "> But the paper states there is an implementation for Bcoin […] What do we know of how these implementations work in practice?\r\n\r\nFound the Bcoin client implementation, with the rate limiting part in https://github.com/bcoin-org/bcoin/commit/cf9d364d6a9424b9f4146bd620d43034af43b056. Unfortunately there hasn't been any review on it, and it's unmerged, so we don't have data on how this performs in practice.\r\n",
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1207936111",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "reviewed",
      "id": 1064970764,
      "node_id": "PRR_kwDOABII584_eioM",
      "url": null,
      "actor": null,
      "commit_id": "6ef78a8af3c9cec7f877f9cca9fc761d472134bf",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "",
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1064970764",
      "submitted_at": "2022-08-08T11:40:08Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "commented",
      "id": 1208652062,
      "node_id": "IC_kwDOABII585ICpEe",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1208652062",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-08T21:54:22Z",
      "updated_at": "2022-08-10T19:59:22Z",
      "author_association": "MEMBER",
      "body": "I've pushed a branch to https://github.com/sipa/bitcoin/commits/pr25717b with some suggested changes:\r\n* Switch to randomized commitment offset (which the simulation script shows reduces the attacker's power a bit, permitting lower per-peer memory usage)\r\n* Send locators that contain mixes of the best-known global state (m_chainman.m_best_header) and the current per-peer headerssync state. This seems a very robust way to avoid a whole lot of duplicate work/bandwidth. It means that if we've received (in 2nd phase) headers from peer A up to height H, while we're simultaneously syncing headers (phase 1 or 2) from peer B at height below H, the locator we send to B will make them instantly skip ahead to wherever we already are with A (starting a new phase 1 there). If we've finished syncing with A, it will cause an abort with B if they only have the same headers.\r\n* Add a bunch of logging\r\n* Interface changes:\r\n  * The `StartInitialSynchronization` function is gone; part of its arguments are moved to the constructor, the initial headers argument to it is moved to the caller having to invoke `ProcessNextHeaders`. This removes some duplication in functionality, and simplifies the implementation too, as now `PeerManagerImpl::TryLowWorkHeadersSync` can invoke `PeerManagerImpl::IsContinuationOfLowWorkHeadersSync` to actually deal with the headers, removing some duplication that was introduced by the tip-mixing above.\r\n  * The `MakeNextHeadersRequest` function is now part of the public interface, and is expected to be invoked by the caller when `ProcessNextHeaders` reports `request_more` (rather than being invoke in-line by `ProcessNextHeaders` directly). This avoids the need to lock `cs_main` when nothing is to be requested (to get the global best header tip).\r\n\r\nA rebased, squashed, version of all the commits: https://github.com/sipa/bitcoin/commits/202208_headerssync",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1208652062",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1210461522,
      "node_id": "IC_kwDOABII585IJi1S",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1210461522",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-10T10:13:18Z",
      "updated_at": "2022-08-10T10:17:21Z",
      "author_association": "MEMBER",
      "body": "@sipa [here's a log](https://gist.github.com/Sjors/f56645f1109f53b007f00c1586dd1f3c) when I tried your rebased branch on mainnet. Just as it reached 90% synced headers with one of the peers, it started with another peer, quickly reached the sufficient work and started redownloading. But then it aborted with \"non-continuous headers at height=680041 (redownload phase)\". It does eventually complete the download, with this new peer.\r\n\r\nThere was no new block produced at the time where it started to sync with the new peer. Unfortunately I didn't log enough other things to see what could have triggered it.",
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1210461522",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7162463371,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGq6oSL",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7162463371",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-10T10:13:18Z"
    },
    {
      "event": "subscribed",
      "id": 7162463377,
      "node_id": "SE_lADOABII585OmiYczwAAAAGq6oSR",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7162463377",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-10T10:13:18Z"
    },
    {
      "event": "commented",
      "id": 1210890084,
      "node_id": "IC_kwDOABII585ILLdk",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1210890084",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-10T15:42:22Z",
      "updated_at": "2022-08-10T15:42:22Z",
      "author_association": "MEMBER",
      "body": "@Sjors Cool, that abort/restart of peer=3 at the same height was surprising to me, but I've reproduced it.",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1210890084",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7165032412,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGrEbfc",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7165032412",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-10T15:42:22Z"
    },
    {
      "event": "subscribed",
      "id": 7165032425,
      "node_id": "SE_lADOABII585OmiYczwAAAAGrEbfp",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7165032425",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-10T15:42:22Z"
    },
    {
      "event": "commented",
      "id": 1211155233,
      "node_id": "IC_kwDOABII585IMMMh",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1211155233",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-10T19:19:02Z",
      "updated_at": "2022-08-10T19:48:04Z",
      "author_association": "MEMBER",
      "body": "@Sjors Fixed it in both branches. The problem was that when asking for the initial headers from a peer (outside of the HeadersSyncState logic), we ask for one block earlier than our actual tip, so that we always get a meaningful result back, even if the peer has the same tip already. However, in my logic to mix in the tip into HeadersSyncState getheader requests, I didn't make it make this one-step-back. The result was that if you transition from one peer to another when headers syncing, you'd initialize at one block backward, but then the first headers arriving after that would start one further, causing an abort and restart.",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1211155233",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7166532438,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGrKJtW",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7166532438",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-10T19:19:03Z"
    },
    {
      "event": "subscribed",
      "id": 7166532446,
      "node_id": "SE_lADOABII585OmiYczwAAAAGrKJte",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7166532446",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-10T19:19:03Z"
    },
    {
      "event": "commented",
      "id": 1211683635,
      "node_id": "IC_kwDOABII585IONMz",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1211683635",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T08:18:45Z",
      "updated_at": "2022-08-11T08:18:45Z",
      "author_association": "MEMBER",
      "body": "Aside from the bug, what triggers the transition to another peer? It definitely wasn't a timeout.",
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1211683635",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1211900888,
      "node_id": "IC_kwDOABII585IPCPY",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1211900888",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T12:06:56Z",
      "updated_at": "2022-08-11T12:06:56Z",
      "author_association": "MEMBER",
      "body": "@Sjors They may have just disconnected for whatever reason.",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1211900888",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7171080671,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGrbgHf",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7171080671",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T12:06:56Z"
    },
    {
      "event": "subscribed",
      "id": 7171080675,
      "node_id": "SE_lADOABII585OmiYczwAAAAGrbgHj",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7171080675",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T12:06:57Z"
    },
    {
      "event": "commented",
      "id": 1212078841,
      "node_id": "IC_kwDOABII585IPtr5",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1212078841",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T14:38:25Z",
      "updated_at": "2022-08-11T14:38:25Z",
      "author_association": "MEMBER",
      "body": "> 4.2 suggests picking a maximum number of header chains to store, before pruning the shortest. It suggests storing 10 chains for a total of 480MB.\r\n\r\n@pinheadmz @sjors Absent other details, I think this approach would fail at guaranteeing that we eventually reach consensus with our peers.  Imagine that some adversary has given us 10 bogus chains, and we connect to an honest peer for the first time.  What do we do during headers sync with that peer, in the window of time that the headers we're downloading are far behind (in work) from the work on the bogus chains we have stored already?  If we prune as we go, then we will make no progress towards downloading the honest chain; but if we don't, then we're open to an attack from an adversary that would serve us a huge, low-work chain (eg due to timewarp).\r\n\r\n",
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1212078841",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7172264254,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGrgBE-",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7172264254",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T14:38:25Z"
    },
    {
      "event": "subscribed",
      "id": 7172264266,
      "node_id": "SE_lADOABII585OmiYczwAAAAGrgBFK",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7172264266",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T14:38:25Z"
    },
    {
      "event": "mentioned",
      "id": 7172264292,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGrgBFk",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7172264292",
      "actor": {
        "login": "pinheadmz",
        "id": 2084648,
        "node_id": "MDQ6VXNlcjIwODQ2NDg=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2084648?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/pinheadmz",
        "html_url": "https://github.com/pinheadmz",
        "followers_url": "https://api.github.com/users/pinheadmz/followers",
        "following_url": "https://api.github.com/users/pinheadmz/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/pinheadmz/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/pinheadmz/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/pinheadmz/subscriptions",
        "organizations_url": "https://api.github.com/users/pinheadmz/orgs",
        "repos_url": "https://api.github.com/users/pinheadmz/repos",
        "events_url": "https://api.github.com/users/pinheadmz/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/pinheadmz/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T14:38:26Z"
    },
    {
      "event": "subscribed",
      "id": 7172264306,
      "node_id": "SE_lADOABII585OmiYczwAAAAGrgBFy",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7172264306",
      "actor": {
        "login": "pinheadmz",
        "id": 2084648,
        "node_id": "MDQ6VXNlcjIwODQ2NDg=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2084648?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/pinheadmz",
        "html_url": "https://github.com/pinheadmz",
        "followers_url": "https://api.github.com/users/pinheadmz/followers",
        "following_url": "https://api.github.com/users/pinheadmz/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/pinheadmz/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/pinheadmz/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/pinheadmz/subscriptions",
        "organizations_url": "https://api.github.com/users/pinheadmz/orgs",
        "repos_url": "https://api.github.com/users/pinheadmz/repos",
        "events_url": "https://api.github.com/users/pinheadmz/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/pinheadmz/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T14:38:26Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7172780064,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGrh_Ag",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7172780064",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T15:39:15Z"
    },
    {
      "event": "milestoned",
      "id": 7174320734,
      "node_id": "MIE_lADOABII585OmiYczwAAAAGrn3Je",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7174320734",
      "actor": {
        "login": "laanwj",
        "id": 126646,
        "node_id": "MDQ6VXNlcjEyNjY0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/laanwj",
        "html_url": "https://github.com/laanwj",
        "followers_url": "https://api.github.com/users/laanwj/followers",
        "following_url": "https://api.github.com/users/laanwj/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/laanwj/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/laanwj/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
        "organizations_url": "https://api.github.com/users/laanwj/orgs",
        "repos_url": "https://api.github.com/users/laanwj/repos",
        "events_url": "https://api.github.com/users/laanwj/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/laanwj/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T19:18:48Z",
      "milestone": {
        "title": "24.0"
      }
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7174395344,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGroJXQ",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7174395344",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T19:31:39Z"
    },
    {
      "event": "commented",
      "id": 1212412201,
      "node_id": "IC_kwDOABII585IQ_Ep",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1212412201",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T19:39:45Z",
      "updated_at": "2022-08-11T19:39:45Z",
      "author_association": "MEMBER",
      "body": "> Update 2022-08-06: why do we need bitdeque instead of just std::vector<bool>? Is it only because we call m_header_commitments.pop_front()? Couldn't we also just keep the commitments, track our index with an integer and then delete the whole thing when we're done?\r\n\r\nTwo reasons, I think -- one is that we use more memory that way, the other is that the bookkeeping seemed more complicated to think about than if we pop entries as we go.  ",
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1212412201",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1212438798,
      "node_id": "IC_kwDOABII585IRFkO",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1212438798",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T20:09:20Z",
      "updated_at": "2022-08-11T20:09:20Z",
      "author_association": "MEMBER",
      "body": "> Two reasons, I think -- one is that we use more memory that way, the other is that the bookkeeping seemed more complicated to think about than if we pop entries as we go.\r\n\r\nAnother is that with an anticipated (but not currently implemented) optimization, the m_chain_start point may move forward while commitments are being added, necessitating popping pieces off from the front of m_commitment_bits).",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1212438798",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7174832462,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGrp0FO",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7174832462",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T20:41:02Z"
    },
    {
      "event": "commented",
      "id": 1212474655,
      "node_id": "IC_kwDOABII585IROUf",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1212474655",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T20:44:00Z",
      "updated_at": "2022-08-11T20:44:00Z",
      "author_association": "MEMBER",
      "body": "Updated this PR to include the many improvements @sipa drafted in his branch (thank you!).  Also included a fuzz test by @mzumsande, and addressed many of the review comments from @sjors.",
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1212474655",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7174848194,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGrp37C",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7174848194",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T20:44:00Z"
    },
    {
      "event": "subscribed",
      "id": 7174848201,
      "node_id": "SE_lADOABII585OmiYczwAAAAGrp37J",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7174848201",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T20:44:00Z"
    },
    {
      "event": "mentioned",
      "id": 7174848209,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGrp37R",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7174848209",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T20:44:00Z"
    },
    {
      "event": "subscribed",
      "id": 7174848222,
      "node_id": "SE_lADOABII585OmiYczwAAAAGrp37e",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7174848222",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T20:44:00Z"
    },
    {
      "event": "mentioned",
      "id": 7174848234,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGrp37q",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7174848234",
      "actor": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T20:44:01Z"
    },
    {
      "event": "subscribed",
      "id": 7174848243,
      "node_id": "SE_lADOABII585OmiYczwAAAAGrp37z",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7174848243",
      "actor": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T20:44:01Z"
    },
    {
      "event": "commented",
      "id": 1212486205,
      "node_id": "IC_kwDOABII585IRRI9",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1212486205",
      "actor": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T20:58:42Z",
      "updated_at": "2022-08-11T20:59:47Z",
      "author_association": "MEMBER",
      "body": "Would be nice to give some log feedback that the anti-DoS header sync has at least started. Currently for testnet it's ~3 minutes of nothing of use being printed.\r\n\r\nrelated question: Is the sync state exposed anywhere via RPC? `getchaintips` showed nothing happening either.",
      "user": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1212486205",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1212490881,
      "node_id": "IC_kwDOABII585IRSSB",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1212490881",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T21:04:37Z",
      "updated_at": "2022-08-11T21:37:17Z",
      "author_association": "MEMBER",
      "body": "@instagibbs Yeah, there was some chatter about that, but it's genuinely hard to do: there isn't any global state that is even aware of the progress made until the second phase is hit (as the first phase takes place entirely in per-peer structures).\r\n\r\nMy best idea (but I haven't gotten past an idea):\r\n* Make net_processing keep track of the per-peer 1st phase progress, and e.g. aggregate it into a maximum.\r\n* Make net_processing report that to validation (because validation is reponsible currently for emitting callbacks about sync progress).\r\n* Change the callback interface for headers to not use CBlockIndex* anymore, but height/time/progress base.\r\n* Add ways to notify 1st/2nd phase along with this.\r\n* Make all listeners of these notifications aware of it.\r\n\r\nJust logging is a lot easier of course, but it's unclear how much should be unconditionally logged. I'm not sure a per-headerblob logging is even appropriate.",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1212490881",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7174956463,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGrqSWv",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7174956463",
      "actor": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T21:04:37Z"
    },
    {
      "event": "subscribed",
      "id": 7174956466,
      "node_id": "SE_lADOABII585OmiYczwAAAAGrqSWy",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7174956466",
      "actor": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T21:04:37Z"
    },
    {
      "event": "commented",
      "id": 1212491557,
      "node_id": "IC_kwDOABII585IRScl",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1212491557",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T21:05:20Z",
      "updated_at": "2022-08-11T21:05:20Z",
      "author_association": "MEMBER",
      "body": "@instagibbs Yeah, there was some chatter about that, but it's genuinely hard to do: there isn't any global state that is even aware of the progress made until the second phase is it (as the first phase takes place entirely in per-peer structures).\n\nMy best idea (but I haven't gotten past an idea):\n* Make net_processing keep track of the per-peer 1st phase progress, and e.g. aggregate it into a maximum.\n* Make net_processing report that to validation (because validation is reponsible currently for emitting callbacks about sync progress).\n* Change the callback interface for headers to not use CBlockIndex* anymore, but height/time/progress base.\n* Add ways to notify 1st/2nd phase along with this.\n* Make all listeners of these notifications aware of it.\n\nJust logging is a lot easier of course, but it's unclear how much should be unconditionally logged. I'm not sure a per-headerblob logging is even appropriate.",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1212491557",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7174960606,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGrqTXe",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7174960606",
      "actor": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T21:05:21Z"
    },
    {
      "event": "subscribed",
      "id": 7174960611,
      "node_id": "SE_lADOABII585OmiYczwAAAAGrqTXj",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7174960611",
      "actor": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-11T21:05:21Z"
    },
    {
      "event": "commented",
      "id": 1212695034,
      "node_id": "IC_kwDOABII585ISEH6",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1212695034",
      "actor": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-12T03:29:50Z",
      "updated_at": "2022-08-12T03:29:50Z",
      "author_association": "MEMBER",
      "body": "> related question: Is the sync state exposed anywhere via RPC? `getchaintips` showed nothing happening either.\r\n\r\nSeems like it might be good to at least report the per-peer value of `m_current_height` in `getpeerinfo`? Perhaps reporting the `nTime` or tracking the median time of the best initial sync block would be nice, then you could report the best one via the gui to give people some idea of progress?\r\n\r\nAlso, I'm not sure it makes sense to have the initial header sync phase limited to a single peer?\r\n\r\nAfter adding the above to `getpeerinfo`, and watching a header sync of testnet from scratch, it first did an initial sync up to about block 1M with peer 0, then peer 0 disconnected, and it immediately started an initial header sync with all peers, with peer=8 eventually winning the race and being the one to send all the actual headers.\r\n\r\nIt looks to me like a new block being found means I'll receive an INV from all nodes, which will then trigger me sending a GETHEADERS to all nodes, which then gets me doing an initial sync with all nodes anyway.\r\n\r\nI'm seeing some nodes get through initial sync at about twice the speed of others, so seems like it might be a worthwhile speedup even if your first node doesn't decide to disconnect.",
      "user": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1212695034",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7179453514,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGr7cRK",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7179453514",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-12T14:06:05Z"
    },
    {
      "event": "commented",
      "id": 1213159134,
      "node_id": "IC_kwDOABII585IT1be",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1213159134",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-12T14:16:05Z",
      "updated_at": "2022-08-12T14:19:54Z",
      "author_association": "MEMBER",
      "body": "> > related question: Is the sync state exposed anywhere via RPC? `getchaintips` showed nothing happening either.\r\n> \r\n> Seems like it might be good to at least report the per-peer value of `m_current_height` in `getpeerinfo`? Perhaps reporting the `nTime` or tracking the median time of the best initial sync block would be nice, then you could report the best one via the gui to give people some idea of progress?\r\n\r\nJust added a commit that exposes `m_current_height` in `getpeerinfo()`, and also updated the branch to include tests for the handling of low-work compact blocks and unrequested blocks.\r\n\r\n> Also, I'm not sure it makes sense to have the initial header sync phase limited to a single peer?\r\n\r\nDid you see [Pieter's comments](https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1201763035) on the bandwidth issue here?  Also, I think it makes most sense to discuss this topic in #25720, as the bandwidth vs. robustness issues around initial headers sync are largely independent of this change (though this PR does increase the bandwidth we'd expect to use).\r\n\r\nAt this point I think I've responded to or addressed all the comments so far (thank you reviewers!), with the exception of the logging issue that has been raised.  I will continue to think about that, but given [how invasive](https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1212491557) it may be to address this properly, I wonder what people think of deferring that improvement to a followup PR?",
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1213159134",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1214245464,
      "node_id": "IC_kwDOABII585IX-pY",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1214245464",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-13T23:24:09Z",
      "updated_at": "2022-08-13T23:24:09Z",
      "author_association": "MEMBER",
      "body": "I've pushed to https://github.com/sipa/bitcoin/commits/202208_headerssync_log a series of commits to:\r\n* There is a lock ordering bug in the current branch, which can lead to a deadlock when opening the peer view window in the GUI. There was a crossing of m_header_sync_mutex and cs_main; GetNodeStateStats is being called while holding cs_main sometimes, and it needs m_header_sync_mutex, which means that elsewhere we cannot grab cs_main while already holding m_header_sync_mutex. There is a commit that fixes this by pushing down the m_header_sync_mutex locking; an alternative was grabbing cs_main for all of the headers sync processing, but the more fine-grained lock approach is cleaner overall, I think.\r\n* Aggregate statistics about (the first phase of) low-work headers sync across all peers in net_processing\r\n* Report those statistics to validation\r\n* Let validation rate-limit those reports, and then use them for logging and emitting signals with progress updates\r\n* Let the GUI use those for reporting as well.\r\n\r\nIn the logs/UI and code related to it I've picked the term \"headers pre-synchronization\" to refer to the first phase of low-work headers sync (as that's a bit of a mouthful). Bikeshedding welcome - but whatever we pick it may be useful to be consistent in naming in other places too.\r\n\r\n",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1214245464",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1214415342,
      "node_id": "IC_kwDOABII585IYoHu",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1214415342",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-14T17:02:01Z",
      "updated_at": "2022-08-14T17:16:55Z",
      "author_association": "MEMBER",
      "body": "So a few open questions regarding the logging/progress:\n* Is it necessary to address logging/progress before 24.0? It's a somewhat bad UX if we don't, as there is no feedback indicating anything is happening at all, for possibly several minutes. Addressing it does seem to need some amount of complexity, it seems.\n* If Qt GUI changes for reporting 1st phase headers sync are considered needed, there is the concern that it adds a few more translation strings. Not having any reporting at all is however worse than untranslated ones, I would guess?\n* Should the GUI changes be done through a separate PR to the GUI repository?\n* My branch above doesn't add a GUI element in the peer view details for first-phase headers height yet; I'd consider that much lower priority than just having progress bar updates for it, and probably better done by someone familiar with Qt.",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1214415342",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1214424750,
      "node_id": "IC_kwDOABII585IYqau",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1214424750",
      "actor": {
        "login": "hebasto",
        "id": 32963518,
        "node_id": "MDQ6VXNlcjMyOTYzNTE4",
        "avatar_url": "https://avatars.githubusercontent.com/u/32963518?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/hebasto",
        "html_url": "https://github.com/hebasto",
        "followers_url": "https://api.github.com/users/hebasto/followers",
        "following_url": "https://api.github.com/users/hebasto/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/hebasto/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/hebasto/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/hebasto/subscriptions",
        "organizations_url": "https://api.github.com/users/hebasto/orgs",
        "repos_url": "https://api.github.com/users/hebasto/repos",
        "events_url": "https://api.github.com/users/hebasto/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/hebasto/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-14T18:08:18Z",
      "updated_at": "2022-08-14T18:09:07Z",
      "author_association": "MEMBER",
      "body": "> * My branch above doesn't add a GUI element in the peer view details for first-phase headers height yet; I'd consider that much lower priority than just having progress bar updates for it, and probably better done by someone familiar with Qt.\r\n\r\nAgree.\r\n\r\n> * Should the GUI changes be done through a separate PR to the GUI repository?\r\n\r\nI assume we have no such a strict policy. If reviewers are ok to review and test Qt code here, why not? But separating low-priority (in comparison to a new header sync logic) GUI changes into a dedicated PR in the GUI repo sounds better.\r\n\r\n> * If Qt GUI changes for reporting 1st phase headers sync are considered needed, there is the concern that it adds a few more translation strings. Not having any reporting at all is however worse than untranslated ones, I would guess?\r\n\r\nIt depends on how many \"a few\". I guess an additional week will be enough for translators to update their translations with a couple of new strings.",
      "user": {
        "login": "hebasto",
        "id": 32963518,
        "node_id": "MDQ6VXNlcjMyOTYzNTE4",
        "avatar_url": "https://avatars.githubusercontent.com/u/32963518?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/hebasto",
        "html_url": "https://github.com/hebasto",
        "followers_url": "https://api.github.com/users/hebasto/followers",
        "following_url": "https://api.github.com/users/hebasto/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/hebasto/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/hebasto/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/hebasto/subscriptions",
        "organizations_url": "https://api.github.com/users/hebasto/orgs",
        "repos_url": "https://api.github.com/users/hebasto/repos",
        "events_url": "https://api.github.com/users/hebasto/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/hebasto/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1214424750",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1214430587,
      "node_id": "IC_kwDOABII585IYr17",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1214430587",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-14T18:52:06Z",
      "updated_at": "2022-08-15T00:21:32Z",
      "author_association": "MEMBER",
      "body": "> It depends on how many \"a few\". I guess an additional week will be enough for translators to update their translations with a couple of new strings.\n\nTwo, both variations of \"Pre-syncing headers\" (assuming that's the terminology we'll settle on).",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1214430587",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "reviewed",
      "id": 1070569040,
      "node_id": "PRR_kwDOABII584_z5ZQ",
      "url": null,
      "actor": null,
      "commit_id": "ac4cd62b3abfb4b28135e4f126fc672ca606b643",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "With this commit (ac4cd62b3abfb4b28135e4f126fc672ca606b643) and #25720, I'm seeing failures after one peer completes its initial download, and the tip progresses past either where other nodes are up to in their initial download, or (less commonly) in their redownload.\r\n\r\nIn the former case, I think the only real bug is that they then immediately restart their initial download which makes for noisy logs and wastes bandwidth (since they're getting headers more slowly than the peer that's redownloading).\r\n\r\nIn the latter case, that wastes the entire initial download, which seems more wasteful, and seems like it might introduce some new (mild?) DoS behaviours.\r\n\r\nI've had a go at patching around that behaviour at https://github.com/ajtowns/bitcoin/commits/202208-headers-dos\r\n\r\n\r\nNot necessarily blocking, but it feels buggy/ugly to me as it stands.",
      "user": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1070569040",
      "submitted_at": "2022-08-15T11:14:47Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "commented",
      "id": 1215121609,
      "node_id": "IC_kwDOABII585IbUjJ",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1215121609",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-15T14:59:45Z",
      "updated_at": "2022-08-15T18:33:35Z",
      "author_association": "MEMBER",
      "body": "@ajtowns I believe you're seeing the effects of the mixing-in-`m_best_header`-based-skipping, but may be misunderstanding what is happening.\r\n\r\nFirst of all, `ValidatedAndStoreRedownloadedHeader` indeed doesn't deal with the peer deciding to continue from one of the mixed-in locator entries, but it doesn't need to; we will, in the same processing of the received headers message, abort the headers sync, and start a new one, with the first header in that received message as starting point. Assuming it is an honest peer, doing so is always advantageous as it means it's skipping some headers sync work with that peer (the locator entries are sorted in such a way that this is the case). Note that while it's possible during INITIAL_DOWNLOAD phase that the actual height of synchronization goes backwards, this may still be advantageous as it means less work to redo during REDOWNLOAD (because while the tip moves backwards, the starting point moves forward).\r\n\r\nThe intention is that once we start synchronizing headers with a peer, we don't ever stop doing so; it up to them to stop giving us headers (by reaching the sync end). We mix in headers from the m_best_header tip to give the peer a chance to skip ahead if there is some part both sides already have, but it is up to them. If there is one peer faster than all others, and in the process of moving ours m_best_header tip forward with every received headers blob, then this means all other peers will continuously be skipping ahead to keep up, but always stay behind this faster peer, until the faster peer reaches the end of the synchronization, at which point all other peers will skip ahead to the end as well, and stop. So in short: while headers sync is ongoing, we will effectively be continuously asking for headers from all headers-syncing peers all the time, until it's done. This is already the case in the current codebase actually, though this PR makes it worse by (a) taking longer overall before synchronization completes even with a single peer and (b) only making progress in synchronization visible to other, slower peers, during the second phase. Some of this downside is mitigated by this mixing in, causing headers sync with other (honest) peers to stop as soon as sync with one peer completes, rather than needing to wait for them to get to the end on their own.\r\n\r\nIt's not impossible to detect certain conditions, like a peer giving us things we're clearly not interested in anymore, to just give up on the synchronization with them, but there are some scary edge cases to think about then. For example, imagine a peer who has a deep reorg of the honest chain (however unlikely that may be, we should be able to deal with it), which forks off more than 2000 headers past the highest previous locator entry in the m_best_header based locators. A newly started header sync at that point will just result in 2000 headers we already have, and aborting the sync entirely and starting over later from m_best_header again will not make any progress towards ever actually getting the peer's chain. For that reason, it seems safer to just keep fetching, and letting the \"skipping\" be done by the remote party - they know best what they're trying to give us.\r\n\r\nThere are two other improvements possible to consider that can reduce bandwidth waste:\r\n* Being more careful about which peers we start synchronizing with, in a controlled way that still results in us eventually trying all peers, so we'll certainly eventually get whatever headers any and all of them have to give us. #25720 is a step in that direction.\r\n* Moving the start point forward: if we receive headers from a peer that we already have, in some cases it is possible to just move the HeadersSyncState m_chain_start point forward (and correspondingly pop off commitments from the buffers in it), so that we restart redownload further ahead when we get there. I have a half-finished commit that does this, but it needs thinking about correctness (again related to this deep reorg scenario above), and adds a bunch of complexity we may not want in this PR.",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1215121609",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7189340065,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGshJ-h",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7189340065",
      "actor": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-15T14:59:45Z"
    },
    {
      "event": "subscribed",
      "id": 7189340076,
      "node_id": "SE_lADOABII585OmiYczwAAAAGshJ-s",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7189340076",
      "actor": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-15T14:59:46Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7189837945,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGsjDh5",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7189837945",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-15T16:16:07Z"
    },
    {
      "event": "commented",
      "id": 1215277846,
      "node_id": "IC_kwDOABII585Ib6sW",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1215277846",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-15T16:17:55Z",
      "updated_at": "2022-08-15T16:17:55Z",
      "author_association": "MEMBER",
      "body": "Thanks @sipa, I have pushed your latest branch with the logging code here (and I squashed the lock-inversion fix into the commit where I had introduced the bug).  ",
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1215277846",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7189849806,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGsjGbO",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7189849806",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-15T16:17:55Z"
    },
    {
      "event": "subscribed",
      "id": 7189849820,
      "node_id": "SE_lADOABII585OmiYczwAAAAGsjGbc",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7189849820",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-15T16:17:55Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7190584529,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGsl5zR",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7190584529",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-15T18:21:10Z"
    },
    {
      "event": "commented",
      "id": 1215780032,
      "node_id": "IC_kwDOABII585Id1TA",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1215780032",
      "actor": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-15T20:52:22Z",
      "updated_at": "2022-08-15T20:52:22Z",
      "author_association": "MEMBER",
      "body": "> First of all, `ValidatedAndStoreRedownloadedHeader` indeed doesn't deal with the peer deciding to continue from one of the mixed-in locator entries, but it doesn't need to; we will, in the same processing of the received headers message, abort the headers sync, and start a new one, with the first header in that received message as starting point. Assuming it is an honest peer, doing so is always advantageous [...]\r\n\r\nI don't think that's correct. Restarting headers sync pushes you back into initial download, so the behaviour on testnet for a peer that's finished initial sync but skips some blocks during redownload looks like:\r\n\r\n * download 2M headers for initial sync\r\n * download 500k headers for redownload phase\r\n * find that the tip is up to height 550k\r\n * download headers from 550k to 552k that don't connect at 500k, and restart\r\n * find that in the meantime the tip is now up to height 560k, and your faster peer has died\r\n * download 1.44M headers (for blocks at height 560k to 2M) for initial sync\r\n * redownload the 1.44M headers to actually update the tip\r\n\r\nThat is, you've saved downloading 50k headers, at a cost of downloading 1.44M headers in the redundant initial sync. That's not advantageous...\r\n\r\nBeing able to drop another peer from redownload phase back to initial sync seems like it has scary dos opportunities to me, and just searching through the history of the tip that we sent to them seems a very easy way of mitigating the problem in the honest-peer case.\r\n\r\nIt's definitely less important if you're only in the initial sync phase, while some other peer is further along in the redownload phase; you are only getting pushed forward there.\r\n\r\nBut at that point you've already been \"lapped\", and the other peer is demonstrably currently supplying valid headers faster than you. In the normal case there, you're not going to win; and even if you're assuming the other node is an attacker, their attack is just \"delivering valid headers quickly\"... Continuing to try initial headers normally in those circumstances just seems like a waste of bandwidth -- so waiting until the good nodes disconnect or a new block comes in seems like it would be better logic for the non-normal cases.\r\n\r\nI guess at the very least, if that path is normal and expected, then constantly logging it as \"aborted non-connecting // headers sync started\" doesn't seem helpful.\r\n\r\n> It's not impossible to detect certain conditions, like a peer giving us things we're clearly not interested in anymore, to just give up on the synchronization with them, but there are some scary edge cases to think about then. For example, imagine a peer who has a deep reorg of the honest chain (however unlikely that may be, we should be able to deal with it), which forks off more than 2000 headers past the highest previous locator entry in the m_best_header based locators. A newly started header sync at that point will just result in 2000 headers we already have,\r\n\r\nBut we don't consider repeated headers a problem, just ones that don't connect? And if we've already accepted them, they'll connect fine?",
      "user": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1215780032",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1215791131,
      "node_id": "IC_kwDOABII585Id4Ab",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1215791131",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-15T20:58:33Z",
      "updated_at": "2022-08-15T21:00:26Z",
      "author_association": "MEMBER",
      "body": "> Restarting headers sync pushes you back into initial download, so the behaviour on testnet for a peer that's finished initial sync but skips some blocks during redownload looks like:\r\n\r\nThat's not what the code is supposed to do. The double-height computation should in the scenario you describe treat continuing the current sync as better than restarting, order the locator entries to account for that, and assuming the peer picks the first locator entry of the chain they want to send, just continue syncing.\r\n\r\nSpecifically, once 2M headers have downloaded in INITIAL_DOWNLOAD, and 500k headers have downloaded in REDOWNLOAD, the double-height score for continuing should be 2500000. The double-height score for restarting from 550k should be 1100000. Continuing has a higher score, so should be placed first in the locator.\r\n\r\nIs that not what you're seeing? The code may be buggy of course, or is it possible you were using an older version (this branch briefly had mixing-in logic without the double-height weighing last week, which would behave as you describe here).",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1215791131",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1215868434,
      "node_id": "IC_kwDOABII585IeK4S",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1215868434",
      "actor": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-15T21:31:22Z",
      "updated_at": "2022-08-15T21:40:09Z",
      "author_association": "MEMBER",
      "body": "> Is that not what you're seeing?\r\n\r\nHere's some logs (for commit fc2e19e61261cedade18d1a5b9de2075523fa6aa merged onto 22d96d76ab02fc73e7fe0d810bacee4c982df085). I reset chainwork so that 1,156,000 testnet headers is enough. Peers 0 and 1 managed to complete initial sync. Peer 0's log entries look like:\r\n\r\n```\r\n2022-08-15T21:14:38Z [headerssync] Initial headers sync started with peer=0: height=0, max_commitments=3738869, min_work=00000000000000000000000000000000000000000000002830dab7f76dbb7d63\r\n2022-08-15T21:17:30Z [headerssync] Initial headers sync transition with peer=0: reached sufficient work at height=1156000, redownloading from height=0\r\n2022-08-15T21:19:57Z [headerssync] Initial headers sync complete with peer=0: releasing all at height=1156000 (redownload phase)\r\n```\r\n\r\nPeer 1's log entries look like this:\r\n\r\n```\r\n2022-08-15T21:15:12Z [headerssync] Initial headers sync started with peer=1: height=0, max_commitments=3738870, min_work=00000000000000000000000000000000000000000000002830dab7f76dbb7d63\r\n2022-08-15T21:18:26Z [headerssync] Initial headers sync transition with peer=1: reached sufficient work at height=1156000, redownloading from height=0\r\n2022-08-15T21:18:57Z [headerssync] Initial headers sync aborted with peer=1: non-continuous headers at height=176001 (redownload phase)\r\n2022-08-15T21:18:57Z [headerssync] Initial headers sync started with peer=1: height=666040, max_commitments=2124319, min_work=00000000000000000000000000000000000000000000002830dab7f76dbb7d63\r\n2022-08-15T21:18:57Z [headerssync] Initial headers sync aborted with peer=1: non-continuous headers at height=668040 (commitment phase)\r\n```\r\n\r\nalong with another ~70 started/aborted pairs, as peer 1's initial sync keeps getting reset to match peer 0's successful redownload.\r\n\r\nPeers 3 and 5 contribute another 379 aborted/started pairs; they never made it out of commitment phase though, so aren't particularly interesting.\r\n\r\n~I don't think the doubling counts matter here at all~ [EDIT: I think the double height logic is delaying the restart as designed; but there's still no reason to restart; I've got the data the peer skipped, and can easily validate it against the peer's commitment, so there's no need to restart] -- peer 1 is just failing due to the `if (header.hashPrevBlock != m_redownload_buffer_last_hash)` check in `ValidateAndStoreRedownloadedHeader`.",
      "user": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1215868434",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1215885234,
      "node_id": "IC_kwDOABII585IeO-y",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1215885234",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-15T21:41:19Z",
      "updated_at": "2022-08-15T21:42:03Z",
      "author_association": "MEMBER",
      "body": "```\r\n2022-08-15T21:18:57Z [headerssync] Initial headers sync aborted with peer=1: non-continuous headers at height=176001 (redownload phase)\r\n2022-08-15T21:18:57Z [headerssync] Initial headers sync started with peer=1: height=666040, max_commitments=2124319, min_work=00000000000000000000000000000000000000000000002830dab7f76dbb7d63\r\n```\r\n\r\nSo, this is good. Peer 1 used to be in a state where it would still need to redownload another 1156000-176001=979999 blocks. By restarting at height 666040, it only has to download (1156000-666040) headers (twice, once in commitment, once in redownload phase), or 979920 headers, anymore. It's only a tiny improvement, but it's a strict reduction in the amount of downloading left.\r\n\r\n> peer 1 is just failing due to the if (header.hashPrevBlock != m_redownload_buffer_last_hash) check in ValidateAndStoreRedownloadedHeader.\r\n\r\nI think you're confusing cause and effect. That check is failing because the peer sent us non-continuous headers. And the peer is doing that because we want it to: restarting is less work than continuing.\r\n",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1215885234",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "reviewed",
      "id": 1073217147,
      "node_id": "PRR_kwDOABII584_9_57",
      "url": null,
      "actor": null,
      "commit_id": "fc2e19e61261cedade18d1a5b9de2075523fa6aa",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "",
      "user": {
        "login": "achow101",
        "id": 3782274,
        "node_id": "MDQ6VXNlcjM3ODIyNzQ=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3782274?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/achow101",
        "html_url": "https://github.com/achow101",
        "followers_url": "https://api.github.com/users/achow101/followers",
        "following_url": "https://api.github.com/users/achow101/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/achow101/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/achow101/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/achow101/subscriptions",
        "organizations_url": "https://api.github.com/users/achow101/orgs",
        "repos_url": "https://api.github.com/users/achow101/repos",
        "events_url": "https://api.github.com/users/achow101/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/achow101/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1073217147",
      "submitted_at": "2022-08-15T22:13:56Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "reviewed",
      "id": 1072958922,
      "node_id": "PRR_kwDOABII584_9A3K",
      "url": null,
      "actor": null,
      "commit_id": "fc2e19e61261cedade18d1a5b9de2075523fa6aa",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "",
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1072958922",
      "submitted_at": "2022-08-15T22:38:26Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7198259216,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGtDLgQ",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7198259216",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-16T15:20:44Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7198426757,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGtD0aF",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7198426757",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-16T15:41:54Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7198493775,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGtEExP",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7198493775",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-16T15:50:48Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7198535260,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGtEO5c",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7198535260",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-16T15:56:14Z"
    },
    {
      "event": "commented",
      "id": 1216850444,
      "node_id": "IC_kwDOABII585Ih6oM",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1216850444",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-16T16:10:32Z",
      "updated_at": "2022-08-16T16:10:32Z",
      "author_association": "MEMBER",
      "body": "After seeing the discussion between @ajtowns and @sipa regarding bandwidth optimization involving what locators we send and how we process \"skipping ahead\" in our headers download (in the situation where one of our peers is serving us headers at a faster rate than another), I think it makes sense to drop any such optimization from this PR.  Here's my thinking:\r\n\r\nThere are a number of ways (such as was done in #25720) to reduce our bandwidth usage during initial headers sync.  While this PR increases bandwidth used by a node starting up for the first time, the overall bandwidth usage is still far below what a new node will end up using during block download, particularly now that #25720 has been merged (mitigating bad behavior that existed prior to this PR).  Further improvements to bandwidth usage are certainly possible, but the set of strategies we might consider for doing so goes beyond the code touched here.\r\n\r\nThat is, while we might consider using tricks with this logic to reduce bandwidth usage -- whether by mixing in hashes from `m_best_header` into the locators we send, and/or using headers that are either received from a peer or accepted into our block index to move a given peer's sync forward -- there are also strategies that may be even more effective that don't involve code changed by this PR.  For example, a refinement to #25720 that further reduces the likelihood of performing initial sync with more than one peer simultaneously (such as by introducing a minimum amount of time that we allow our initial headers-sync peer to complete sync, before we add any other peers, even via the block INV mechanism) would materially affect the impact of optimizations designed to improve the behavior when we have multiple sync peers.\r\n\r\nAlternatively, deploying a p2p protocol change to compress headers message (as has been discussed in the past, such as by omitting prevHash to save 32 bytes, and compressing other fields such as nVersion and nTime) could lead to bandwidth savings of close to 50%, which would eliminate most of the overhead introduced by this PR, at least for upgraded software that supports such a proposed new protocol.\r\n\r\nSo given that we have alternatives like these to consider when thinking about bandwidth usage, and particularly given how complex this PR is already, I think deferring bandwidth minimization strategies to future work makes the most sense.  For now I've removed the optimization that had been in this PR to mix-in locators from `m_best_header`, which I hope makes the logic easier to reason about.\r\n\r\nI've also addressed the latest review feedback from @achow101 and @mzumsande, and incorporated the fixups from @sipa to use the PRESYNC terminology in place of INITIAL_DOWNLOAD. ",
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1216850444",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7198648188,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGtEqd8",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7198648188",
      "actor": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-16T16:10:32Z"
    },
    {
      "event": "subscribed",
      "id": 7198648201,
      "node_id": "SE_lADOABII585OmiYczwAAAAGtEqeJ",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7198648201",
      "actor": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-16T16:10:32Z"
    },
    {
      "event": "mentioned",
      "id": 7198648225,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGtEqeh",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7198648225",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-16T16:10:32Z"
    },
    {
      "event": "subscribed",
      "id": 7198648244,
      "node_id": "SE_lADOABII585OmiYczwAAAAGtEqe0",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7198648244",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-16T16:10:32Z"
    },
    {
      "event": "mentioned",
      "id": 7198648264,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGtEqfI",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7198648264",
      "actor": {
        "login": "achow101",
        "id": 3782274,
        "node_id": "MDQ6VXNlcjM3ODIyNzQ=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3782274?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/achow101",
        "html_url": "https://github.com/achow101",
        "followers_url": "https://api.github.com/users/achow101/followers",
        "following_url": "https://api.github.com/users/achow101/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/achow101/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/achow101/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/achow101/subscriptions",
        "organizations_url": "https://api.github.com/users/achow101/orgs",
        "repos_url": "https://api.github.com/users/achow101/repos",
        "events_url": "https://api.github.com/users/achow101/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/achow101/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-16T16:10:32Z"
    },
    {
      "event": "subscribed",
      "id": 7198648278,
      "node_id": "SE_lADOABII585OmiYczwAAAAGtEqfW",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7198648278",
      "actor": {
        "login": "achow101",
        "id": 3782274,
        "node_id": "MDQ6VXNlcjM3ODIyNzQ=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3782274?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/achow101",
        "html_url": "https://github.com/achow101",
        "followers_url": "https://api.github.com/users/achow101/followers",
        "following_url": "https://api.github.com/users/achow101/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/achow101/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/achow101/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/achow101/subscriptions",
        "organizations_url": "https://api.github.com/users/achow101/orgs",
        "repos_url": "https://api.github.com/users/achow101/repos",
        "events_url": "https://api.github.com/users/achow101/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/achow101/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-16T16:10:33Z"
    },
    {
      "event": "mentioned",
      "id": 7198648291,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGtEqfj",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7198648291",
      "actor": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-16T16:10:33Z"
    },
    {
      "event": "subscribed",
      "id": 7198648303,
      "node_id": "SE_lADOABII585OmiYczwAAAAGtEqfv",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7198648303",
      "actor": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-16T16:10:33Z"
    },
    {
      "event": "reviewed",
      "id": 1074622378,
      "node_id": "PRR_kwDOABII585ADW-q",
      "url": null,
      "actor": null,
      "commit_id": "ab4c41af5b0e1c6f7ec8b8c3aedcf2a5f2f5c242",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "ACK, for the code not written by me here.\r\n\r\nI've reviewed the code, did several experiments on testnet and mainnet, and observed behavior with `-debug=headerssync`.\r\n\r\nA few nits below, which are non-blocking.",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1074622378",
      "submitted_at": "2022-08-16T19:45:24Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "reviewed",
      "id": 1074585147,
      "node_id": "PRR_kwDOABII585ADN47",
      "url": null,
      "actor": null,
      "commit_id": "d45562fc0844d3e961ce0113830a92a9fcc44984",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "",
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1074585147",
      "submitted_at": "2022-08-16T20:27:39Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7201035265,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGtNxQB",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7201035265",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-16T22:49:59Z"
    },
    {
      "event": "commented",
      "id": 1217458325,
      "node_id": "IC_kwDOABII585IkPCV",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1217458325",
      "actor": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-17T04:49:49Z",
      "updated_at": "2022-08-17T04:49:49Z",
      "author_association": "MEMBER",
      "body": "> After seeing the discussion between @ajtowns and @sipa regarding bandwidth optimization involving what locators we send and how we process \"skipping ahead\" in our headers download (in the situation where one of our peers is serving us headers at a faster rate than another), I think it makes sense to drop any such optimization from this PR. Here's my thinking:\r\n\r\nI think that results in the following observable differences:\r\n\r\n * more bandwidth usage\r\n * if the fastest peer you're syncing with disconnects while in redownload, it will take some time for the next fastest peer to catch up with your best header and you won't be making any progress until then\r\n * previously, once your best header reached min chainwork, you'd start syncing blocks with all nodes; now you'll have to wait until you actually catch up with the tip, or until each peer meets min chainwork independently\r\n\r\nThose don't seem too bad to me. :+1: \r\n\r\n> Alternatively, deploying a p2p protocol change to compress headers message (as has been discussed in the past, such as by omitting prevHash to save 32 bytes, and compressing other fields such as nVersion and nTime)\r\n\r\nFWIW, since BIP320 has been adopted by miners (effectively providing 16 extra nonce bits), I think it's hard to compress nVersion below 2 bytes, and nTime seems to need at least 12 bits; not sure the difference between 3.5 bytes and 8 bytes in the normal case is worth the complexity of dealing with the special cases? Batching headers with a common nBits together would save ~4B per header though, since it only changes every 2016 blocks. (If you did a consensus change to have difficulty change every block, you could still have nBits as a constant value over x,000 blocks describing the minimum allowed difficulty in that period)\r\n\r\n> could lead to bandwidth savings of close to 50%, which would eliminate most of the overhead introduced by this PR, at least for upgraded software that supports such a proposed new protocol.\r\n\r\nUpping the number of headers per message from 2000 to 4000 would halve the number of round-trips necessary (or more? if you increased the message size, 22k headers would fit in a 1MB message at 44B/header, and you'd only need ~35 round trips to sync mainnet instead of ~350...), which I think would cut header sync times down pretty dramatically.",
      "user": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1217458325",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7202221033,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGtSSvp",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7202221033",
      "actor": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-17T04:49:49Z"
    },
    {
      "event": "subscribed",
      "id": 7202221039,
      "node_id": "SE_lADOABII585OmiYczwAAAAGtSSvv",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7202221039",
      "actor": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-17T04:49:49Z"
    },
    {
      "event": "mentioned",
      "id": 7202221048,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGtSSv4",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7202221048",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-17T04:49:49Z"
    },
    {
      "event": "subscribed",
      "id": 7202221051,
      "node_id": "SE_lADOABII585OmiYczwAAAAGtSSv7",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7202221051",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-17T04:49:49Z"
    },
    {
      "event": "commented",
      "id": 1218056480,
      "node_id": "IC_kwDOABII585ImhEg",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1218056480",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-17T14:04:53Z",
      "updated_at": "2022-08-17T14:04:53Z",
      "author_association": "MEMBER",
      "body": "The pre-sync phase looks like this:\r\n<img width=\"623\" alt=\"presync\" src=\"https://user-images.githubusercontent.com/10217/185154132-5a86c1ab-6a10-4f92-b2f7-f50b4065abda.png\">\r\n\r\n\r\nI don't think it's critical to translate. As long as something is making progress, I suspect most new users will be happy :-)",
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1218056480",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "reviewed",
      "id": 1075991624,
      "node_id": "PRR_kwDOABII585AIlRI",
      "url": null,
      "actor": null,
      "commit_id": "07b068e1568efdb6473f1d1eb4b870eb339c748b",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1075991624",
      "submitted_at": "2022-08-17T15:38:07Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "commented",
      "id": 1218225296,
      "node_id": "IC_kwDOABII585InKSQ",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1218225296",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-17T16:10:11Z",
      "updated_at": "2022-08-17T16:11:25Z",
      "author_association": "MEMBER",
      "body": "The `LocatorEntries` helper function in 5ea6f9a34bf3e8751fbca8f6107e99a9f553e31f was introduced in this push: https://github.com/bitcoin/bitcoin/pull/25717#event-7174395344. It looks correct to me, but what was the motivation for adding it? I guess we were using a `CChain` object in an earlier version somewhere? ",
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1218225296",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1218302659,
      "node_id": "IC_kwDOABII585IndLD",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1218302659",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-17T17:27:22Z",
      "updated_at": "2022-08-17T17:27:22Z",
      "author_association": "MEMBER",
      "body": "@Sjors An earlier version of the code had net_processing construct a CBlockLocator normally, hand that to HeadersSyncState at creation time which would remember it, and then the locator returned was constructed by concatenating the presync/redownload tip hash with the entries in the locator that was passed down from net_processing at construction.\r\n\r\nI later introduced the commit you're referring to (LocatorEntries etc) for the purpose of being able to do tip header mixing, as it needed a way to compute the locator entries, plus it needed more information about those entries (their height).\r\n\r\nTip header mixing was reverted, and some of the complexity in that commit with it. Arguably we don't strictly need the LocatorEntries() anymore, it could just use GetLocator and grab the innards of the returned locator rather than using LocatorEntries. It doesn't hurt though, it's a bit cleaner I think, and a simpler change later if we want to introduce the header tip mixing again.",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1218302659",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7207922939,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGtoCz7",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7207922939",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-17T17:27:22Z"
    },
    {
      "event": "subscribed",
      "id": 7207922943,
      "node_id": "SE_lADOABII585OmiYczwAAAAGtoCz_",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7207922943",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-17T17:27:22Z"
    },
    {
      "event": "commented",
      "id": 1218419862,
      "node_id": "IC_kwDOABII585In5yW",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1218419862",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-17T19:43:01Z",
      "updated_at": "2022-08-17T19:44:05Z",
      "author_association": "MEMBER",
      "body": "> it's a bit cleaner I think\r\n\r\nAgree in the context it's used here.\r\n\r\n> It doesn't hurt though\r\n\r\nLocators are used in various places. I'm not sure how to confirm that the performance decrease o(1) to o(log(n)) is negligible in all the places it's used.",
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1218419862",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1218524855,
      "node_id": "IC_kwDOABII585IoTa3",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1218524855",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-17T21:54:03Z",
      "updated_at": "2022-08-17T21:55:52Z",
      "author_association": "MEMBER",
      "body": "> I'm not sure how to confirm that the performance decrease o(1) to o(log(n)) is negligible in all the places it's used.\r\n\r\nAs far as I know, they're only constructed (a) in order to be sent to the network for requesting headers, or (b) when storing in the wallet or indexes when flushing what the current sync position is. Chasing a few dozen pointers is a sub-microsecond operation. In both scenarios, that's dwarfed by just network processing times or the disk I/O that also needs to happen.",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1218524855",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1223367486,
      "node_id": "IC_kwDOABII585I6xs-",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1223367486",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-23T00:30:48Z",
      "updated_at": "2022-08-23T00:30:48Z",
      "author_association": "MEMBER",
      "body": "Pushed two commits to address the following:\r\n\r\n- If a peer has a chain whose work is less than minchainwork, but is a subset of the main chain (m_best_header or chainActive), then we wouldn't ever process the peer's headers and update pindexBestKnownBlock, preventing us from being able to use the peer as a download peer for the portion of the chain they have.  Fix this by checking to see if the last header in a headers message is (a) known to us and (b) an ancestor of our best header or tip, and if so, bypass the DoS checks (which doesn't cost us anything because processing these headers wouldn't use more memory anyway).\r\n- This change means that for a node which has synced the honest chain, it's no longer possible for an attacker to try to achieve a collision during REDOWNLOAD by feeding us headers we already have during PRESYNC.  This also means that we would not initiate a low-work headers sync until receiving a headers message that contains something new, which I think is conceptually simpler than what was happening previously.\r\n- I noticed a bug in `ProcessHeadersMessage()` where, in a couple of places, we would be operating on the headers received off the network instead of the headers returned by the headers sync module. The second commit tries to clean this logic up in a less error-prone way by eliminating the `headers_to_process` pointer, in favor of just changing the `headers` vector directly when `previously_downloaded_headers` are returned.\r\n\r\nFinally, I'm still contemplating whether any logic change is warranted if we receive a low-work header via compact block (right now we would ignore them, I think this may be fine but since this is a behavior change it's worth considering whether this could be problematic at all).\r\n\r\n(Will squash these commits into place once reviewers have had a chance to take a look)",
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1223367486",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1223693933,
      "node_id": "IC_kwDOABII585I8BZt",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1223693933",
      "actor": {
        "login": "fanquake",
        "id": 863730,
        "node_id": "MDQ6VXNlcjg2MzczMA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/863730?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/fanquake",
        "html_url": "https://github.com/fanquake",
        "followers_url": "https://api.github.com/users/fanquake/followers",
        "following_url": "https://api.github.com/users/fanquake/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/fanquake/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/fanquake/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/fanquake/subscriptions",
        "organizations_url": "https://api.github.com/users/fanquake/orgs",
        "repos_url": "https://api.github.com/users/fanquake/repos",
        "events_url": "https://api.github.com/users/fanquake/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/fanquake/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-23T07:54:18Z",
      "updated_at": "2022-08-23T07:54:18Z",
      "author_association": "MEMBER",
      "body": "@sdaftuar with your latest push:\r\n```bash\r\nheaderssync.cpp:41:46: error: invalid operands to binary expression ('NodeClock::time_point' (aka 'time_point<NodeClock>') and 'int64_t' (aka 'long long'))\r\n    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\r\n```",
      "user": {
        "login": "fanquake",
        "id": 863730,
        "node_id": "MDQ6VXNlcjg2MzczMA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/863730?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/fanquake",
        "html_url": "https://github.com/fanquake",
        "followers_url": "https://api.github.com/users/fanquake/followers",
        "following_url": "https://api.github.com/users/fanquake/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/fanquake/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/fanquake/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/fanquake/subscriptions",
        "organizations_url": "https://api.github.com/users/fanquake/orgs",
        "repos_url": "https://api.github.com/users/fanquake/repos",
        "events_url": "https://api.github.com/users/fanquake/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/fanquake/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1223693933",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7239403719,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGvgIjH",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7239403719",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-23T07:54:18Z"
    },
    {
      "event": "subscribed",
      "id": 7239403728,
      "node_id": "SE_lADOABII585OmiYczwAAAAGvgIjQ",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7239403728",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-23T07:54:18Z"
    },
    {
      "event": "reviewed",
      "id": 1082319701,
      "node_id": "PRR_kwDOABII585AguNV",
      "url": null,
      "actor": null,
      "commit_id": "96140e947937dfd1d275a2af9fd9782e5f17c41e",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1082319701",
      "submitted_at": "2022-08-23T14:47:53Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "C_kwDOABII59oAKDFkNGNmYTQyNzJjZjJjOGI5ODBjYzg3NjJjMWZmMjIyMGQzZThkNTE",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/1d4cfa4272cf2c8b980cc8762c1ff2220d3e8d51",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bitcoin/commit/1d4cfa4272cf2c8b980cc8762c1ff2220d3e8d51",
      "tree": {
        "sha": "4d1338b17702e4ed0998ece0f00f2d4b8b9ba062",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/4d1338b17702e4ed0998ece0f00f2d4b8b9ba062"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/2bd9aa5a44b88c866c4d98f8a7bf7154049cba31",
          "sha": "2bd9aa5a44b88c866c4d98f8a7bf7154049cba31",
          "html_url": "https://github.com/bitcoin/bitcoin/commit/2bd9aa5a44b88c866c4d98f8a7bf7154049cba31"
        }
      ],
      "message": "Add function to validate difficulty changes\n\nThe rule against difficulty adjustments changing by more than a factor of 4 can\nbe helpful for anti-DoS measures in contexts where we lack a full headers\nchain, so expose this functionality separately and in the narrow case where we\nonly know the height, new value, and old value.\n\nIncludes fuzz test by Martin Zumsande.",
      "committer": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-08-23T15:34:10Z"
      },
      "author": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-05-25T14:16:56Z"
      },
      "sha": "1d4cfa4272cf2c8b980cc8762c1ff2220d3e8d51"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "C_kwDOABII59oAKDg0ODUyYmI2YmIzNTc5ZTQ3NWNlNzhmZTcyOWZkMTI1ZGRiYzcxNWY",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/84852bb6bb3579e475ce78fe729fd125ddbc715f",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bitcoin/commit/84852bb6bb3579e475ce78fe729fd125ddbc715f",
      "tree": {
        "sha": "98e62e860465dd6423d0a247faf4b4a0fb94e3ff",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/98e62e860465dd6423d0a247faf4b4a0fb94e3ff"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/1d4cfa4272cf2c8b980cc8762c1ff2220d3e8d51",
          "sha": "1d4cfa4272cf2c8b980cc8762c1ff2220d3e8d51",
          "html_url": "https://github.com/bitcoin/bitcoin/commit/1d4cfa4272cf2c8b980cc8762c1ff2220d3e8d51"
        }
      ],
      "message": "Add bitdeque, an std::deque<bool> analogue that does bit packing.",
      "committer": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-08-23T15:34:10Z"
      },
      "author": {
        "name": "Pieter Wuille",
        "email": "pieter@wuille.net",
        "date": "2022-02-23T01:34:20Z"
      },
      "sha": "84852bb6bb3579e475ce78fe729fd125ddbc715f"
    },
    {
      "event": "reviewed",
      "id": 1080677272,
      "node_id": "PRR_kwDOABII585AadOY",
      "url": null,
      "actor": null,
      "commit_id": "ec8aed0cf57340345771eeebebee3a3c4918baf6",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "",
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1080677272",
      "submitted_at": "2022-08-23T15:34:20Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7243311488,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGvvCmA",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7243311488",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-23T15:50:43Z"
    },
    {
      "event": "reviewed",
      "id": 1080118855,
      "node_id": "PRR_kwDOABII585AYU5H",
      "url": null,
      "actor": null,
      "commit_id": "c68fb8bf5704bb9a73beedc3ccc1ac49d848469d",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "Have stared at each commit enough to convince myself this is safe. Tested this a few times on mainnet and testnet with some sanity check assertions thrown in. I don't know much about qt but the code looks correct, and when testing, both gui and debug.log seemed to display information at a reasonable cadence.",
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1080118855",
      "submitted_at": "2022-08-23T16:48:02Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7244616370,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGv0BKy",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7244616370",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-23T18:39:43Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7245091626,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGv11Mq",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7245091626",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-23T19:49:40Z"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "C_kwDOABII59oAKGVkNDcwOTQwY2RkYmViNDA0MjU5NjBkNTFjZWZlZWM0OTQ4ZmViZTQ",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/ed470940cddbeb40425960d51cefeec4948febe4",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bitcoin/commit/ed470940cddbeb40425960d51cefeec4948febe4",
      "tree": {
        "sha": "91f598eec25c5714404c3c085a0e3d98be9cb73c",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/91f598eec25c5714404c3c085a0e3d98be9cb73c"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/84852bb6bb3579e475ce78fe729fd125ddbc715f",
          "sha": "84852bb6bb3579e475ce78fe729fd125ddbc715f",
          "html_url": "https://github.com/bitcoin/bitcoin/commit/84852bb6bb3579e475ce78fe729fd125ddbc715f"
        }
      ],
      "message": "Add functions to construct locators without CChain\n\nThis introduces an insignificant performance penalty, as it means locator\nconstruction needs to use the skiplist-based CBlockIndex::GetAncestor()\nfunction instead of the lookup-based CChain, but avoids the need for\ncallers to have access to a relevant CChain object.",
      "committer": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-08-23T20:05:00Z"
      },
      "author": {
        "name": "Pieter Wuille",
        "email": "pieter@wuille.net",
        "date": "2022-08-08T00:56:17Z"
      },
      "sha": "ed470940cddbeb40425960d51cefeec4948febe4"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7245184823,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGv2L83",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7245184823",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-23T20:05:19Z"
    },
    {
      "event": "commented",
      "id": 1224789481,
      "node_id": "IC_kwDOABII585JAM3p",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1224789481",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-23T20:10:41Z",
      "updated_at": "2022-08-23T20:10:41Z",
      "author_association": "MEMBER",
      "body": "Addressed review feedback and included some changes to further clean up the logic in `ProcessHeadersMessage()`.  Also, I included a new test for large reorg handling.",
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1224789481",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7245798774,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGv4h12",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7245798774",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-23T21:49:41Z"
    },
    {
      "event": "reviewed",
      "id": 1082900947,
      "node_id": "PRR_kwDOABII585Ai8HT",
      "url": null,
      "actor": null,
      "commit_id": "5d492952165ff396480e170ac20adc6e8005af91",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "user": {
        "login": "fjahr",
        "id": 1322187,
        "node_id": "MDQ6VXNlcjEzMjIxODc=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1322187?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/fjahr",
        "html_url": "https://github.com/fjahr",
        "followers_url": "https://api.github.com/users/fjahr/followers",
        "following_url": "https://api.github.com/users/fjahr/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/fjahr/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/fjahr/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/fjahr/subscriptions",
        "organizations_url": "https://api.github.com/users/fjahr/orgs",
        "repos_url": "https://api.github.com/users/fjahr/repos",
        "events_url": "https://api.github.com/users/fjahr/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/fjahr/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1082900947",
      "submitted_at": "2022-08-23T22:12:25Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7245911657,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGv49Zp",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7245911657",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-23T22:15:01Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7249883260,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGwIHB8",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7249883260",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-24T11:03:30Z"
    },
    {
      "event": "reviewed",
      "id": 1083645668,
      "node_id": "PRR_kwDOABII585Alx7k",
      "url": null,
      "actor": null,
      "commit_id": "bbef38fdac90a03020ad70cbe6b97fbf09fccbc6",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "Tested bbef38fdac90a03020ad70cbe6b97fbf09fccbc6 on the GUI:\r\n\r\n![Screenshot from 2022-08-24 12-40-08](https://user-images.githubusercontent.com/32963518/186409992-2a465617-4c82-4a03-ba77-2e74059decf2.png)\r\n\r\n![Screenshot from 2022-08-24 12-40-47](https://user-images.githubusercontent.com/32963518/186410190-0a0dc8b8-1543-45cb-b123-0868fc05e464.png)\r\n\r\n![Screenshot from 2022-08-24 12-41-59](https://user-images.githubusercontent.com/32963518/186410096-6aee8461-60eb-4cc0-b239-571aa06efab6.png)\r\n",
      "user": {
        "login": "hebasto",
        "id": 32963518,
        "node_id": "MDQ6VXNlcjMyOTYzNTE4",
        "avatar_url": "https://avatars.githubusercontent.com/u/32963518?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/hebasto",
        "html_url": "https://github.com/hebasto",
        "followers_url": "https://api.github.com/users/hebasto/followers",
        "following_url": "https://api.github.com/users/hebasto/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/hebasto/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/hebasto/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/hebasto/subscriptions",
        "organizations_url": "https://api.github.com/users/hebasto/orgs",
        "repos_url": "https://api.github.com/users/hebasto/repos",
        "events_url": "https://api.github.com/users/hebasto/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/hebasto/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1083645668",
      "submitted_at": "2022-08-24T11:45:46Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "reviewed",
      "id": 1083775678,
      "node_id": "PRR_kwDOABII585AmRq-",
      "url": null,
      "actor": null,
      "commit_id": "bbef38fdac90a03020ad70cbe6b97fbf09fccbc6",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "user": {
        "login": "hebasto",
        "id": 32963518,
        "node_id": "MDQ6VXNlcjMyOTYzNTE4",
        "avatar_url": "https://avatars.githubusercontent.com/u/32963518?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/hebasto",
        "html_url": "https://github.com/hebasto",
        "followers_url": "https://api.github.com/users/hebasto/followers",
        "following_url": "https://api.github.com/users/hebasto/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/hebasto/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/hebasto/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/hebasto/subscriptions",
        "organizations_url": "https://api.github.com/users/hebasto/orgs",
        "repos_url": "https://api.github.com/users/hebasto/repos",
        "events_url": "https://api.github.com/users/hebasto/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/hebasto/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1083775678",
      "submitted_at": "2022-08-24T13:05:28Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "commented",
      "id": 1225721076,
      "node_id": "IC_kwDOABII585JDwT0",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1225721076",
      "actor": {
        "login": "hebasto",
        "id": 32963518,
        "node_id": "MDQ6VXNlcjMyOTYzNTE4",
        "avatar_url": "https://avatars.githubusercontent.com/u/32963518?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/hebasto",
        "html_url": "https://github.com/hebasto",
        "followers_url": "https://api.github.com/users/hebasto/followers",
        "following_url": "https://api.github.com/users/hebasto/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/hebasto/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/hebasto/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/hebasto/subscriptions",
        "organizations_url": "https://api.github.com/users/hebasto/orgs",
        "repos_url": "https://api.github.com/users/hebasto/repos",
        "events_url": "https://api.github.com/users/hebasto/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/hebasto/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-24T13:23:14Z",
      "updated_at": "2022-08-24T13:40:10Z",
      "author_association": "MEMBER",
      "body": "I have reviewed the GUI-specific code and it looks OK, tested it with `QT_FATAL_WARNINGS=1`. I agree it can be merged.\r\n\r\nUPDATE:\r\nThis PR adds two new translatable strings, but that is ok as we are [still](https://bitcoin-irc.chaincode.com/bitcoin-core-dev/2022-08-24#845728;) in pre-\"Translation string freeze\".",
      "user": {
        "login": "hebasto",
        "id": 32963518,
        "node_id": "MDQ6VXNlcjMyOTYzNTE4",
        "avatar_url": "https://avatars.githubusercontent.com/u/32963518?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/hebasto",
        "html_url": "https://github.com/hebasto",
        "followers_url": "https://api.github.com/users/hebasto/followers",
        "following_url": "https://api.github.com/users/hebasto/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/hebasto/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/hebasto/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/hebasto/subscriptions",
        "organizations_url": "https://api.github.com/users/hebasto/orgs",
        "repos_url": "https://api.github.com/users/hebasto/repos",
        "events_url": "https://api.github.com/users/hebasto/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/hebasto/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1225721076",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7251506447,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGwOTUP",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7251506447",
      "actor": {
        "login": "hebasto",
        "id": 32963518,
        "node_id": "MDQ6VXNlcjMyOTYzNTE4",
        "avatar_url": "https://avatars.githubusercontent.com/u/32963518?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/hebasto",
        "html_url": "https://github.com/hebasto",
        "followers_url": "https://api.github.com/users/hebasto/followers",
        "following_url": "https://api.github.com/users/hebasto/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/hebasto/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/hebasto/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/hebasto/subscriptions",
        "organizations_url": "https://api.github.com/users/hebasto/orgs",
        "repos_url": "https://api.github.com/users/hebasto/repos",
        "events_url": "https://api.github.com/users/hebasto/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/hebasto/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-24T14:26:32Z"
    },
    {
      "event": "subscribed",
      "id": 7251506454,
      "node_id": "SE_lADOABII585OmiYczwAAAAGwOTUW",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7251506454",
      "actor": {
        "login": "hebasto",
        "id": 32963518,
        "node_id": "MDQ6VXNlcjMyOTYzNTE4",
        "avatar_url": "https://avatars.githubusercontent.com/u/32963518?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/hebasto",
        "html_url": "https://github.com/hebasto",
        "followers_url": "https://api.github.com/users/hebasto/followers",
        "following_url": "https://api.github.com/users/hebasto/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/hebasto/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/hebasto/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/hebasto/subscriptions",
        "organizations_url": "https://api.github.com/users/hebasto/orgs",
        "repos_url": "https://api.github.com/users/hebasto/repos",
        "events_url": "https://api.github.com/users/hebasto/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/hebasto/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-24T14:26:32Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7253364239,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGwVY4P",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7253364239",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-24T18:31:55Z"
    },
    {
      "event": "reviewed",
      "id": 1083725409,
      "node_id": "PRR_kwDOABII585AmFZh",
      "url": null,
      "actor": null,
      "commit_id": "bbef38fdac90a03020ad70cbe6b97fbf09fccbc6",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "Reviewed the main commit 1fca7ed49eca6c2cd71d37415fa2d419779cf212. It makes sense to me now.  Mostly documentation and aesthetics comments.",
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1083725409",
      "submitted_at": "2022-08-24T19:36:23Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7254889331,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGwbNNz",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7254889331",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-24T23:23:23Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7254986949,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGwblDF",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7254986949",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-24T23:45:10Z"
    },
    {
      "event": "reviewed",
      "id": 1085071070,
      "node_id": "PRR_kwDOABII585ArN7e",
      "url": null,
      "actor": null,
      "commit_id": "acee94b4fdb1a28db1c217d82ca0611d7ba5a56d",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "01a9c4dc2869708957ce3d9d4a391740e074363c: I'm not sure how robust the `min_pow_checked` \"attestation\" is against breaking things in the future (it looks correct now). It's important to have test coverage for low-work headers trying to sneak in via the various call sites. This commit adds a few, so that's good.",
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1085071070",
      "submitted_at": "2022-08-25T09:15:16Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "reviewed",
      "id": 1085263210,
      "node_id": "PRR_kwDOABII585Ar81q",
      "url": null,
      "actor": null,
      "commit_id": "acee94b4fdb1a28db1c217d82ca0611d7ba5a56d",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "tACK acee94b4fdb1a28db1c217d82ca0611d7ba5a56d\r\n\r\n* did not review the deque implementation and fuzzer code\r\n* did not check the actual per peer memory usage\r\n* only lightly reviewed the stats logging code in bb60d93657df10702a48ea3094d2eee732a80e52",
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1085263210",
      "submitted_at": "2022-08-25T11:51:53Z",
      "state": "APPROVED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "commented",
      "id": 1227415353,
      "node_id": "IC_kwDOABII585JKN85",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1227415353",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-25T15:23:59Z",
      "updated_at": "2022-08-25T15:23:59Z",
      "author_association": "MEMBER",
      "body": "Addressed latest review feedback from @sjors, unsquashed branch is [here](https://github.com/sdaftuar/bitcoin/commits/25717.6) for comparison.",
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1227415353",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7260551679,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGwwzn_",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7260551679",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-25T15:23:59Z"
    },
    {
      "event": "subscribed",
      "id": 7260551693,
      "node_id": "SE_lADOABII585OmiYczwAAAAGwwzoN",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7260551693",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-25T15:23:59Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7260552980,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGwwz8U",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7260552980",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-25T15:24:09Z"
    },
    {
      "event": "reviewed",
      "id": 1085914513,
      "node_id": "PRR_kwDOABII585Aub2R",
      "url": null,
      "actor": null,
      "commit_id": "bae408fc87b3111cf93d26db66f1947430b04f34",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "ACK bae408fc87b3111cf93d26db66f1947430b04f34\r\n\r\nI reviewed the code (minus the bitdeque implementation, where I just ran the extensive fuzz test for a while) and tested this multiple times on mainnet, signet and testnet, both with bitcoind and the GUI.",
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1085914513",
      "submitted_at": "2022-08-25T18:26:35Z",
      "state": "APPROVED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "reviewed",
      "id": 1086079463,
      "node_id": "PRR_kwDOABII585AvEHn",
      "url": null,
      "actor": null,
      "commit_id": "bae408fc87b3111cf93d26db66f1947430b04f34",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "ACK bae408fc87b3111cf93d26db66f1947430b04f34, after squashing (with the caveat that some of the code changes are mine).\r\n\r\nI've re-reviewed all commits, and separately reviewed code changes since my last review (by comparing with a rebased version of the old commit hash). I've also done several more synchronizations from scratch, on mainnet and testnet.\r\n\r\nOnly non-blocking nits below.",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1086079463",
      "submitted_at": "2022-08-25T21:18:38Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7263497125,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGw8Cul",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7263497125",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-25T23:56:13Z"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7263529299,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGw8KlT",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7263529299",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-26T00:06:27Z"
    },
    {
      "event": "commented",
      "id": 1227872635,
      "node_id": "IC_kwDOABII585JL9l7",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1227872635",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-26T00:07:53Z",
      "updated_at": "2022-08-26T00:07:53Z",
      "author_association": "MEMBER",
      "body": "Updated to address @sipa's latest comments; unsquashed version is [here](https://github.com/sdaftuar/bitcoin/commits/25717.9).",
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1227872635",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7263533670,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGw8Lpm",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7263533670",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-26T00:07:53Z"
    },
    {
      "event": "subscribed",
      "id": 7263533677,
      "node_id": "SE_lADOABII585OmiYczwAAAAGw8Lpt",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7263533677",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-26T00:07:53Z"
    },
    {
      "event": "commented",
      "id": 1228024314,
      "node_id": "IC_kwDOABII585JMin6",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1228024314",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-26T03:57:31Z",
      "updated_at": "2022-08-26T03:57:31Z",
      "author_association": "MEMBER",
      "body": "ACK f0dae7c6fa54d4d00489da02eebfa3e4ef40cb63 (for the parts that weren't authored by me).\r\n\r\nReviewed the diff with my previous review.",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1228024314",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "reviewed",
      "id": 1086408327,
      "node_id": "PRR_kwDOABII585AwUaH",
      "url": null,
      "actor": null,
      "commit_id": "f0dae7c6fa54d4d00489da02eebfa3e4ef40cb63",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "utACK f0dae7c6fa54d4d00489da02eebfa3e4ef40cb63 -- logic review more than code review",
      "user": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1086408327",
      "submitted_at": "2022-08-26T15:52:47Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "commented",
      "id": 1228795579,
      "node_id": "IC_kwDOABII585JPe67",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1228795579",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-26T18:27:57Z",
      "updated_at": "2022-08-26T18:27:57Z",
      "author_association": "MEMBER",
      "body": "@ajtowns I think both of your suggestions -- dropping the HEADERSSYNC log category in favor of using NET, and dropping the anti-DoS check for NoBan peers -- make sense, but I think both of those improvements are minor at this point and I'd prefer to postpone to a followup PR, just to minimize additional review burden for this.\r\n\r\nCreating a checklist so that these suggestions don't get forgotten:\r\n\r\n- [ ] https://github.com/bitcoin/bitcoin/pull/25717#discussion_r956119793\r\n- [ ] https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955657894",
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1228795579",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7269595894,
      "node_id": "MEE_lADOABII585OmiYczwAAAAGxTTr2",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7269595894",
      "actor": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-26T18:27:58Z"
    },
    {
      "event": "subscribed",
      "id": 7269595902,
      "node_id": "SE_lADOABII585OmiYczwAAAAGxTTr-",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7269595902",
      "actor": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-26T18:27:58Z"
    },
    {
      "event": "reviewed",
      "id": 1087980874,
      "node_id": "PRR_kwDOABII585A2UVK",
      "url": null,
      "actor": null,
      "commit_id": "f0dae7c6fa54d4d00489da02eebfa3e4ef40cb63",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "Will still review post-merge if this PR is evaluated as mature enough to land soon.",
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1087980874",
      "submitted_at": "2022-08-29T02:38:47Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "C_kwDOABII59oAKDU1MWE4ZDk1N2M0YzQ0YWZiZDBkNjA4ZmNkZjdjNmE0MzUyYmFiY2U",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/551a8d957c4c44afbd0d608fcdf7c6a4352babce",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bitcoin/commit/551a8d957c4c44afbd0d608fcdf7c6a4352babce",
      "tree": {
        "sha": "774014202e3ad8560a3be4d59f94b8443e672a67",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/774014202e3ad8560a3be4d59f94b8443e672a67"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/ed470940cddbeb40425960d51cefeec4948febe4",
          "sha": "ed470940cddbeb40425960d51cefeec4948febe4",
          "html_url": "https://github.com/bitcoin/bitcoin/commit/ed470940cddbeb40425960d51cefeec4948febe4"
        }
      ],
      "message": "Utilize anti-DoS headers download strategy\n\nAvoid permanently storing headers from a peer, unless the headers are part of a\nchain with sufficiently high work. This prevents memory attacks using low-work\nheaders.\n\nDesigned and co-authored with Pieter Wuille.",
      "committer": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-08-29T12:10:35Z"
      },
      "author": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-02-09T14:38:52Z"
      },
      "sha": "551a8d957c4c44afbd0d608fcdf7c6a4352babce"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "C_kwDOABII59oAKGVkNmNkZGQ5OGUzMjI2M2ZjMTE2YTQzODBhZjZkNjZkYTIwZGE5OTA",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/ed6cddd98e32263fc116a4380af6d66da20da990",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bitcoin/commit/ed6cddd98e32263fc116a4380af6d66da20da990",
      "tree": {
        "sha": "a17ba16c99aef7d0b774f8910e6e44439eb2bfe6",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/a17ba16c99aef7d0b774f8910e6e44439eb2bfe6"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/551a8d957c4c44afbd0d608fcdf7c6a4352babce",
          "sha": "551a8d957c4c44afbd0d608fcdf7c6a4352babce",
          "html_url": "https://github.com/bitcoin/bitcoin/commit/551a8d957c4c44afbd0d608fcdf7c6a4352babce"
        }
      ],
      "message": "Require callers of AcceptBlockHeader() to perform anti-dos checks\n\nIn order to prevent memory DoS, we must ensure that we don't accept a new\nheader into memory until we've performed anti-DoS checks, such as verifying\nthat the header is part of a sufficiently high work chain. This commit adds a\nnew argument to AcceptBlockHeader() so that we can ensure that all call-sites\nwhich might cause a new header to be accepted into memory have to grapple with\nthe question of whether the header is safe to accept, or needs further\nvalidation.\n\nThis patch also fixes two places where low-difficulty-headers could have been\nprocessed without such validation (processing an unrequested block from the\nnetwork, and processing a compact block).\n\nCredit to Niklas Gögge for noticing this issue, and thanks to Sjors Provoost\nfor test code.",
      "committer": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-08-29T12:10:35Z"
      },
      "author": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-08-02T20:48:57Z"
      },
      "sha": "ed6cddd98e32263fc116a4380af6d66da20da990"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "C_kwDOABII59oAKDgzYzZhMGM1MjQ5YzRlY2JkMTFmNzgyOGM4NGE1MGZiNDczZmFiYTM",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/83c6a0c5249c4ecbd11f7828c84a50fb473faba3",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bitcoin/commit/83c6a0c5249c4ecbd11f7828c84a50fb473faba3",
      "tree": {
        "sha": "32d5bdc4148604b694c6cdc0c089940d2c9ffccb",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/32d5bdc4148604b694c6cdc0c089940d2c9ffccb"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/ed6cddd98e32263fc116a4380af6d66da20da990",
          "sha": "ed6cddd98e32263fc116a4380af6d66da20da990",
          "html_url": "https://github.com/bitcoin/bitcoin/commit/ed6cddd98e32263fc116a4380af6d66da20da990"
        }
      ],
      "message": "Reduce spurious messages during headers sync\n\nDelay sending SENDHEADERS (BIP 130) message until we know our peer's best\nheader's chain has more than nMinimumChainWork. This reduces inadvertent\nheaders messages received during initial headers sync due to block\nannouncements, which throw off our sync algorithm.",
      "committer": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-08-29T12:10:35Z"
      },
      "author": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-03-03T22:40:15Z"
      },
      "sha": "83c6a0c5249c4ecbd11f7828c84a50fb473faba3"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "C_kwDOABII59oAKDBiNmFhODI2YjUzNDcwYzljYzhlZjRhMTUzZmE3MTBkY2U4MDg4MmY",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/0b6aa826b53470c9cc8ef4a153fa710dce80882f",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bitcoin/commit/0b6aa826b53470c9cc8ef4a153fa710dce80882f",
      "tree": {
        "sha": "8aa4dd504543e2eb4311aca3820076e211b96aa3",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/8aa4dd504543e2eb4311aca3820076e211b96aa3"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/83c6a0c5249c4ecbd11f7828c84a50fb473faba3",
          "sha": "83c6a0c5249c4ecbd11f7828c84a50fb473faba3",
          "html_url": "https://github.com/bitcoin/bitcoin/commit/83c6a0c5249c4ecbd11f7828c84a50fb473faba3"
        }
      ],
      "message": "Add unit test for HeadersSyncState",
      "committer": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-08-29T12:10:35Z"
      },
      "author": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-03-09T19:45:45Z"
      },
      "sha": "0b6aa826b53470c9cc8ef4a153fa710dce80882f"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "C_kwDOABII59oAKDE1MGE1NDg2ZGI1MGZmNzdjOTE3NjUzOTIxNDkwMDAwMjljOGEzMDk",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/150a5486db50ff77c91765392149000029c8a309",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bitcoin/commit/150a5486db50ff77c91765392149000029c8a309",
      "tree": {
        "sha": "6fbbdf48a6d1fb7f6d21899ddbda5a12a2df59bd",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/6fbbdf48a6d1fb7f6d21899ddbda5a12a2df59bd"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/0b6aa826b53470c9cc8ef4a153fa710dce80882f",
          "sha": "0b6aa826b53470c9cc8ef4a153fa710dce80882f",
          "html_url": "https://github.com/bitcoin/bitcoin/commit/0b6aa826b53470c9cc8ef4a153fa710dce80882f"
        }
      ],
      "message": "Test headers sync using minchainwork threshold",
      "committer": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-08-29T12:10:35Z"
      },
      "author": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-08-11T20:31:08Z"
      },
      "sha": "150a5486db50ff77c91765392149000029c8a309"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "C_kwDOABII59oAKDAzNzEyZGRkZmJiOWZlMGRjN2EyZWFkNTNjNjUxMDYxODlmNWM4MDM",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/03712dddfbb9fe0dc7a2ead53c65106189f5c803",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bitcoin/commit/03712dddfbb9fe0dc7a2ead53c65106189f5c803",
      "tree": {
        "sha": "26fa9f85ab867705e094f9b4a7826e7553021dfe",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/26fa9f85ab867705e094f9b4a7826e7553021dfe"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/150a5486db50ff77c91765392149000029c8a309",
          "sha": "150a5486db50ff77c91765392149000029c8a309",
          "html_url": "https://github.com/bitcoin/bitcoin/commit/150a5486db50ff77c91765392149000029c8a309"
        }
      ],
      "message": "Expose HeadersSyncState::m_current_height in getpeerinfo()",
      "committer": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-08-29T12:10:35Z"
      },
      "author": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-08-12T14:05:22Z"
      },
      "sha": "03712dddfbb9fe0dc7a2ead53c65106189f5c803"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "C_kwDOABII59oAKDM1NTU0NzMzNGY3ZDA4NjQwZWUxZmEyOTEyMjczNTZkNjExNDVkMWE",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/355547334f7d08640ee1fa291227356d61145d1a",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bitcoin/commit/355547334f7d08640ee1fa291227356d61145d1a",
      "tree": {
        "sha": "955cb7d78dd72167597499ec9f8ea9627af889dd",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/955cb7d78dd72167597499ec9f8ea9627af889dd"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/03712dddfbb9fe0dc7a2ead53c65106189f5c803",
          "sha": "03712dddfbb9fe0dc7a2ead53c65106189f5c803",
          "html_url": "https://github.com/bitcoin/bitcoin/commit/03712dddfbb9fe0dc7a2ead53c65106189f5c803"
        }
      ],
      "message": "Track headers presync progress and log it",
      "committer": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-08-29T12:10:35Z"
      },
      "author": {
        "name": "Pieter Wuille",
        "email": "pieter@wuille.net",
        "date": "2022-08-12T21:29:52Z"
      },
      "sha": "355547334f7d08640ee1fa291227356d61145d1a"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "C_kwDOABII59oAKDkzZWFlMjcwMzFhNjViNDE1NmRmNDkwMTVhZTQ1YjJiNTQxYjRlNWE",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/93eae27031a65b4156df49015ae45b2b541b4e5a",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bitcoin/commit/93eae27031a65b4156df49015ae45b2b541b4e5a",
      "tree": {
        "sha": "cb23983c23cacc26b3223c0d919a8356c0d0770d",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/cb23983c23cacc26b3223c0d919a8356c0d0770d"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/355547334f7d08640ee1fa291227356d61145d1a",
          "sha": "355547334f7d08640ee1fa291227356d61145d1a",
          "html_url": "https://github.com/bitcoin/bitcoin/commit/355547334f7d08640ee1fa291227356d61145d1a"
        }
      ],
      "message": "Test large reorgs with headerssync logic",
      "committer": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-08-29T12:10:35Z"
      },
      "author": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-08-23T18:06:29Z"
      },
      "sha": "93eae27031a65b4156df49015ae45b2b541b4e5a"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "C_kwDOABII59oAKDM3NjA4NmZjNWExODdmNWIyYWIzYTBkMTIwMmVkNGU2YzIyYmRiNTA",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/376086fc5a187f5b2ab3a0d1202ed4e6c22bdb50",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bitcoin/commit/376086fc5a187f5b2ab3a0d1202ed4e6c22bdb50",
      "tree": {
        "sha": "8e0b8ce74bb86b3defc3909b826c8a9e54332f96",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/8e0b8ce74bb86b3defc3909b826c8a9e54332f96"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/93eae27031a65b4156df49015ae45b2b541b4e5a",
          "sha": "93eae27031a65b4156df49015ae45b2b541b4e5a",
          "html_url": "https://github.com/bitcoin/bitcoin/commit/93eae27031a65b4156df49015ae45b2b541b4e5a"
        }
      ],
      "message": "Make validation interface capable of signalling header presync\n\nThis makes a number of changes:\n- Get rid of the verification_progress argument in the node interface\n  NotifyHeaderTip (it was always 0.0).\n- Instead of passing a CBlockIndex* in the UI interface's NotifyHeaderTip,\n  send separate height, timestamp fields. This is becuase in headers presync,\n  no actual CBlockIndex object is available.\n- Add a bool presync argument to both of the above, to identify signals\n  pertaining to the first headers sync phase.",
      "committer": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-08-29T12:10:35Z"
      },
      "author": {
        "name": "Pieter Wuille",
        "email": "pieter@wuille.net",
        "date": "2022-08-13T18:21:12Z"
      },
      "sha": "376086fc5a187f5b2ab3a0d1202ed4e6c22bdb50"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "C_kwDOABII59oAKDczODQyMWM1MGYyZGJkNzM5NWI1MGE1ZGJkZjYxNjhiMDc0MzVlNjI",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/738421c50f2dbd7395b50a5dbdf6168b07435e62",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bitcoin/commit/738421c50f2dbd7395b50a5dbdf6168b07435e62",
      "tree": {
        "sha": "2976c5ba04c4bdf180b6839c3431f104563507ee",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/2976c5ba04c4bdf180b6839c3431f104563507ee"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/376086fc5a187f5b2ab3a0d1202ed4e6c22bdb50",
          "sha": "376086fc5a187f5b2ab3a0d1202ed4e6c22bdb50",
          "html_url": "https://github.com/bitcoin/bitcoin/commit/376086fc5a187f5b2ab3a0d1202ed4e6c22bdb50"
        }
      ],
      "message": "Emit NotifyHeaderTip signals for pre-synchronization progress",
      "committer": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-08-29T12:10:35Z"
      },
      "author": {
        "name": "Pieter Wuille",
        "email": "pieter@wuille.net",
        "date": "2022-08-13T20:04:20Z"
      },
      "sha": "738421c50f2dbd7395b50a5dbdf6168b07435e62"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "C_kwDOABII59oAKDNhZGQyMzQ1NDYyNGM0Yzc5YzllZWJjMDYwYjZmYmVkNGUzMTMxYTc",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bitcoin/commit/3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "tree": {
        "sha": "26c6c99be030698ef856a30601f081b3127dfb0a",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/26c6c99be030698ef856a30601f081b3127dfb0a"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/738421c50f2dbd7395b50a5dbdf6168b07435e62",
          "sha": "738421c50f2dbd7395b50a5dbdf6168b07435e62",
          "html_url": "https://github.com/bitcoin/bitcoin/commit/738421c50f2dbd7395b50a5dbdf6168b07435e62"
        }
      ],
      "message": "ui: show header pre-synchronization progress",
      "committer": {
        "name": "Suhas Daftuar",
        "email": "sdaftuar@gmail.com",
        "date": "2022-08-29T12:10:35Z"
      },
      "author": {
        "name": "Pieter Wuille",
        "email": "pieter@wuille.net",
        "date": "2022-08-13T20:27:50Z"
      },
      "sha": "3add23454624c4c79c9eebc060b6fbed4e3131a7"
    },
    {
      "event": "head_ref_force_pushed",
      "id": 7278019370,
      "node_id": "HRFPE_lADOABII585OmiYczwAAAAGxzcMq",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7278019370",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-29T12:11:11Z"
    },
    {
      "event": "commented",
      "id": 1230203279,
      "node_id": "IC_kwDOABII585JU2mP",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1230203279",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-29T12:13:16Z",
      "updated_at": "2022-08-29T12:13:16Z",
      "author_association": "MEMBER",
      "body": "Updated to drop an extra `Misbehaving()` call that was unnecessary; unsquashed version for comparing against prior branch is [here](https://github.com/sdaftuar/bitcoin/commits/25717.10).",
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1230203279",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1230574107,
      "node_id": "IC_kwDOABII585JWRIb",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1230574107",
      "actor": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-29T16:53:22Z",
      "updated_at": "2022-08-29T16:53:22Z",
      "author_association": "MEMBER",
      "body": "re-tACK 3add23454624c4c79c9eebc060b6fbed4e3131a7\r\n\r\nSame caveats https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1085263210\r\n\r\nAgree that merging the new log into NET makes sense. In fact it's useful to look at both of them, e.g. to see that `sendheaders` is sent at the right time.",
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1230574107",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1230805142,
      "node_id": "IC_kwDOABII585JXJiW",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1230805142",
      "actor": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-29T20:06:45Z",
      "updated_at": "2022-08-29T20:06:45Z",
      "author_association": "MEMBER",
      "body": "re-ACK 3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1230805142",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1231545697,
      "node_id": "IC_kwDOABII585JZ-Vh",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1231545697",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-30T11:38:15Z",
      "updated_at": "2022-08-30T11:38:15Z",
      "author_association": "MEMBER",
      "body": "re-ACK 3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1231545697",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1231604645,
      "node_id": "IC_kwDOABII585JaMul",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1231604645",
      "actor": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-30T12:33:47Z",
      "updated_at": "2022-08-30T12:33:47Z",
      "author_association": "MEMBER",
      "body": "ACK 3add234546\r\n- I can't say I have a complete mental model of every possible interaction during IBD, end of IBD, reorgs, etc., but whiteboarded the syncing logic to the best of my ability. The comments are very helpful, and I reviewed the code to verify that it's doing what the comments say it does.\r\n- Tested on {mainnet, signet, testnet} {with, without} gui to manually check getpeerinfo \"presynced_headers,\" logging, ui\r\n- Reviewed the code, a few [notes](https://github.com/glozow/bitcoin-notes/blob/n25717/review-25717.md) if they help anyone. I did look at the bitdeque, though lightly. Fuzz looks pretty comprehensive to me.",
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1231604645",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "reviewed",
      "id": 1090377051,
      "node_id": "PRR_kwDOABII585A_dVb",
      "url": null,
      "actor": null,
      "commit_id": "355547334f7d08640ee1fa291227356d61145d1a",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "user": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1090377051",
      "submitted_at": "2022-08-30T13:58:26Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "commented",
      "id": 1231722908,
      "node_id": "IC_kwDOABII585Japmc",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1231722908",
      "actor": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-30T14:09:34Z",
      "updated_at": "2022-08-30T14:46:11Z",
      "author_association": "MEMBER",
      "body": "Logging looks great!\r\n\r\nedit: Gah, it just took my node forever to find a peer to give me all the headers after a disconnect I guess. peerinfo is showing a new peer is almost caught up and it should start printing again...\r\n\r\nedit2: everything normal and logging continuing as expected by the code. All good here.\r\n\r\nI guess the note is that if you have a flapping connection during presync, you'll waste some bandwidth and time.\r\n",
      "user": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1231722908",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "merged",
      "id": 7288259615,
      "node_id": "ME_lADOABII585OmiYczwAAAAGyagQf",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7288259615",
      "actor": {
        "login": "fanquake",
        "id": 863730,
        "node_id": "MDQ6VXNlcjg2MzczMA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/863730?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/fanquake",
        "html_url": "https://github.com/fanquake",
        "followers_url": "https://api.github.com/users/fanquake/followers",
        "following_url": "https://api.github.com/users/fanquake/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/fanquake/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/fanquake/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/fanquake/subscriptions",
        "organizations_url": "https://api.github.com/users/fanquake/orgs",
        "repos_url": "https://api.github.com/users/fanquake/repos",
        "events_url": "https://api.github.com/users/fanquake/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/fanquake/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": "e9035f867a36a430998e3811385958229ac79cf5",
      "commit_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/e9035f867a36a430998e3811385958229ac79cf5",
      "created_at": "2022-08-30T14:38:31Z"
    },
    {
      "event": "closed",
      "id": 7288259644,
      "node_id": "CE_lADOABII585OmiYczwAAAAGyagQ8",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7288259644",
      "actor": {
        "login": "fanquake",
        "id": 863730,
        "node_id": "MDQ6VXNlcjg2MzczMA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/863730?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/fanquake",
        "html_url": "https://github.com/fanquake",
        "followers_url": "https://api.github.com/users/fanquake/followers",
        "following_url": "https://api.github.com/users/fanquake/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/fanquake/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/fanquake/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/fanquake/subscriptions",
        "organizations_url": "https://api.github.com/users/fanquake/orgs",
        "repos_url": "https://api.github.com/users/fanquake/repos",
        "events_url": "https://api.github.com/users/fanquake/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/fanquake/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-30T14:38:32Z"
    },
    {
      "event": "referenced",
      "id": 7289835456,
      "node_id": "REFE_lADOABII585OmiYczwAAAAGygg_A",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7289835456",
      "actor": {
        "login": "sidhujag",
        "id": 6238042,
        "node_id": "MDQ6VXNlcjYyMzgwNDI=",
        "avatar_url": "https://avatars.githubusercontent.com/u/6238042?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sidhujag",
        "html_url": "https://github.com/sidhujag",
        "followers_url": "https://api.github.com/users/sidhujag/followers",
        "following_url": "https://api.github.com/users/sidhujag/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sidhujag/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sidhujag/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sidhujag/subscriptions",
        "organizations_url": "https://api.github.com/users/sidhujag/orgs",
        "repos_url": "https://api.github.com/users/sidhujag/repos",
        "events_url": "https://api.github.com/users/sidhujag/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sidhujag/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": "c19a7c43d025a5bfd16d15855f250b4d55b21e76",
      "commit_url": "https://api.github.com/repos/syscoin/syscoin/commits/c19a7c43d025a5bfd16d15855f250b4d55b21e76",
      "created_at": "2022-08-30T18:03:23Z"
    },
    {
      "event": "reviewed",
      "id": 1091090387,
      "node_id": "PRR_kwDOABII585BCLfT",
      "url": null,
      "actor": null,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "Ongoing review commit by commit until `551a8d957`. When done, will do another protocol-wise.",
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1091090387",
      "submitted_at": "2022-08-31T03:05:01Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "commented",
      "id": 1232529431,
      "node_id": "IC_kwDOABII585JdugX",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1232529431",
      "actor": {
        "login": "vasild",
        "id": 266751,
        "node_id": "MDQ6VXNlcjI2Njc1MQ==",
        "avatar_url": "https://avatars.githubusercontent.com/u/266751?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/vasild",
        "html_url": "https://github.com/vasild",
        "followers_url": "https://api.github.com/users/vasild/followers",
        "following_url": "https://api.github.com/users/vasild/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/vasild/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/vasild/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/vasild/subscriptions",
        "organizations_url": "https://api.github.com/users/vasild/orgs",
        "repos_url": "https://api.github.com/users/vasild/repos",
        "events_url": "https://api.github.com/users/vasild/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/vasild/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-08-31T06:45:41Z",
      "updated_at": "2022-08-31T06:45:41Z",
      "author_association": "MEMBER",
      "body": "[Coverage report](https://people.freebsd.org/~vd/pr25717_3add23454_coverage/modified_and_not_covered.html) for modified or added lines by this PR and not covered by unit or functional tests.\r\n",
      "user": {
        "login": "vasild",
        "id": 266751,
        "node_id": "MDQ6VXNlcjI2Njc1MQ==",
        "avatar_url": "https://avatars.githubusercontent.com/u/266751?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/vasild",
        "html_url": "https://github.com/vasild",
        "followers_url": "https://api.github.com/users/vasild/followers",
        "following_url": "https://api.github.com/users/vasild/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/vasild/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/vasild/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/vasild/subscriptions",
        "organizations_url": "https://api.github.com/users/vasild/orgs",
        "repos_url": "https://api.github.com/users/vasild/repos",
        "events_url": "https://api.github.com/users/vasild/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/vasild/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1232529431",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "reviewed",
      "id": 1091627552,
      "node_id": "PRR_kwDOABII585BEOog",
      "url": null,
      "actor": null,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "user": {
        "login": "jonatack",
        "id": 2415484,
        "node_id": "MDQ6VXNlcjI0MTU0ODQ=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2415484?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jonatack",
        "html_url": "https://github.com/jonatack",
        "followers_url": "https://api.github.com/users/jonatack/followers",
        "following_url": "https://api.github.com/users/jonatack/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jonatack/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jonatack/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jonatack/subscriptions",
        "organizations_url": "https://api.github.com/users/jonatack/orgs",
        "repos_url": "https://api.github.com/users/jonatack/repos",
        "events_url": "https://api.github.com/users/jonatack/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jonatack/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1091627552",
      "submitted_at": "2022-08-31T10:02:53Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "reviewed",
      "id": 1092668805,
      "node_id": "PRR_kwDOABII585BIM2F",
      "url": null,
      "actor": null,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "Still reviewing 551a8d95",
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1092668805",
      "submitted_at": "2022-09-01T02:59:12Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "referenced",
      "id": 7306241059,
      "node_id": "REFE_lADOABII585OmiYczwAAAAGzfGQj",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7306241059",
      "actor": {
        "login": "hebasto",
        "id": 32963518,
        "node_id": "MDQ6VXNlcjMyOTYzNTE4",
        "avatar_url": "https://avatars.githubusercontent.com/u/32963518?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/hebasto",
        "html_url": "https://github.com/hebasto",
        "followers_url": "https://api.github.com/users/hebasto/followers",
        "following_url": "https://api.github.com/users/hebasto/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/hebasto/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/hebasto/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/hebasto/subscriptions",
        "organizations_url": "https://api.github.com/users/hebasto/orgs",
        "repos_url": "https://api.github.com/users/hebasto/repos",
        "events_url": "https://api.github.com/users/hebasto/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/hebasto/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": "f79d612fbacf5f86b3cacf0c3455d6c6e3c157bf",
      "commit_url": "https://api.github.com/repos/bitcoin-core/gui/commits/f79d612fbacf5f86b3cacf0c3455d6c6e3c157bf",
      "created_at": "2022-09-01T15:11:34Z"
    },
    {
      "event": "reviewed",
      "id": 1093659307,
      "node_id": "PRR_kwDOABII585BL-qr",
      "url": null,
      "actor": null,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "Good with commit 551a8d957",
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1093659307",
      "submitted_at": "2022-09-01T18:11:48Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "reviewed",
      "id": 1095132796,
      "node_id": "PRR_kwDOABII585BRmZ8",
      "url": null,
      "actor": null,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "Reviewed until tip commit 3add2345",
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1095132796",
      "submitted_at": "2022-09-02T18:21:12Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "reviewed",
      "id": 1095448999,
      "node_id": "PRR_kwDOABII585BSzmn",
      "url": null,
      "actor": null,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "Post-Merge ACK 3add2345\r\n\r\nReviewed the code, the `HeadersSyncState` state machine correctness, the soundness of the anti-DoS mechanism design and partially the test coverage. \r\n\r\nFrom my understanding, the new `bitdeque<> m_headers_commitments` have been deliberately designed to avoid memory DoS and it should scale well if we had more outbound full-relay/block-relay-only peers in the future.\r\n\r\nThe commitment scheme is relying on SipHash, which has been deliberately designed for performance and combined with `m_max_commitments` should avoid new CPU DoS.\r\n\r\nI don't think the commitment scheme could be abused for node fingerprint, as even if the random offset is unique per-node, it should be cleanup once the peer is disconnected, and as such non-observable.\r\n\r\nThis anti-DoS mechanism should not introduce any message amplification attacks, as we don't increase the volume neither the frequency of our p2p messages based on the result of those anti-DoS checks, neither network-partitions vector as we stop processing on invalid/non-continuous headers. ",
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#pullrequestreview-1095448999",
      "submitted_at": "2022-09-03T02:33:11Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
    },
    {
      "event": "referenced",
      "id": 7323076903,
      "node_id": "REFE_lADOABII585OmiYczwAAAAG0fUkn",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7323076903",
      "actor": {
        "login": "domob1812",
        "id": 4943644,
        "node_id": "MDQ6VXNlcjQ5NDM2NDQ=",
        "avatar_url": "https://avatars.githubusercontent.com/u/4943644?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/domob1812",
        "html_url": "https://github.com/domob1812",
        "followers_url": "https://api.github.com/users/domob1812/followers",
        "following_url": "https://api.github.com/users/domob1812/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/domob1812/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/domob1812/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/domob1812/subscriptions",
        "organizations_url": "https://api.github.com/users/domob1812/orgs",
        "repos_url": "https://api.github.com/users/domob1812/repos",
        "events_url": "https://api.github.com/users/domob1812/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/domob1812/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": "047aebfeda407b1d32aad69bd7c95e5521305f7f",
      "commit_url": "https://api.github.com/repos/domob1812/namecoin-core/commits/047aebfeda407b1d32aad69bd7c95e5521305f7f",
      "created_at": "2022-09-05T13:19:44Z"
    },
    {
      "event": "commented",
      "id": 1239513298,
      "node_id": "IC_kwDOABII585J4XjS",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1239513298",
      "actor": {
        "login": "fresheneesz",
        "id": 149531,
        "node_id": "MDQ6VXNlcjE0OTUzMQ==",
        "avatar_url": "https://avatars.githubusercontent.com/u/149531?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/fresheneesz",
        "html_url": "https://github.com/fresheneesz",
        "followers_url": "https://api.github.com/users/fresheneesz/followers",
        "following_url": "https://api.github.com/users/fresheneesz/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/fresheneesz/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/fresheneesz/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/fresheneesz/subscriptions",
        "organizations_url": "https://api.github.com/users/fresheneesz/orgs",
        "repos_url": "https://api.github.com/users/fresheneesz/repos",
        "events_url": "https://api.github.com/users/fresheneesz/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/fresheneesz/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-09-07T15:04:58Z",
      "updated_at": "2022-09-07T15:04:58Z",
      "author_association": "NONE",
      "body": "If the primary attack being prevented is on memory usage, why require downloading the headers twice? Why not simply ensure the headers are removed from memory and store them on disk for later access? Easy enough to delete them if it turns out not enough PoW can be accumulated on that chain. \r\n\r\nThe larger question I have is: why are we putting in effort to remove the IBD checkpoint? The checkpoint is effective, efficient, and secure. \"Confusion about checkpoints\" is an absolutely terrible reason to remove them. Technical design decisions should not be made because some people are confused about \"the role\" effective solutions play. Please do not remove the checkpoint, its a critical piece of efficient IBD. ",
      "user": {
        "login": "fresheneesz",
        "id": 149531,
        "node_id": "MDQ6VXNlcjE0OTUzMQ==",
        "avatar_url": "https://avatars.githubusercontent.com/u/149531?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/fresheneesz",
        "html_url": "https://github.com/fresheneesz",
        "followers_url": "https://api.github.com/users/fresheneesz/followers",
        "following_url": "https://api.github.com/users/fresheneesz/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/fresheneesz/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/fresheneesz/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/fresheneesz/subscriptions",
        "organizations_url": "https://api.github.com/users/fresheneesz/orgs",
        "repos_url": "https://api.github.com/users/fresheneesz/repos",
        "events_url": "https://api.github.com/users/fresheneesz/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/fresheneesz/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1239513298",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1239528193,
      "node_id": "IC_kwDOABII585J4bMB",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1239528193",
      "actor": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-09-07T15:17:11Z",
      "updated_at": "2022-09-07T15:17:11Z",
      "author_association": "MEMBER",
      "body": "> Why not simply ensure the headers are removed from memory and store them on disk for later access?\r\n\r\n\"simply\" generally is not as simple as we'd like, and doesn't solve disk-filling attacks\r\n\r\n> Please do not remove the checkpoint, its a critical piece of efficient IBD.\r\n\r\nAFAIK it doesn't speed up sync in non-adversarial situations, that would be \"assumevalid\" type parameters which aren't being discussed for removal.",
      "user": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1239528193",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1239539406,
      "node_id": "IC_kwDOABII585J4d7O",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1239539406",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-09-07T15:26:23Z",
      "updated_at": "2022-09-07T15:37:02Z",
      "author_association": "MEMBER",
      "body": "> If the primary attack being prevented is on memory usage, why require downloading the headers twice? Why not simply ensure the headers are removed from memory and store them on disk for later access? Easy enough to delete them if it turns out not enough PoW can be accumulated on that chain.\r\n\r\nTo expand on what @instagibbs wrote, this would turn a memory attack into a disk-filling attack.  Moreover, deleting headers is not simple; in order to ensure we always can arrive at consensus, we need an algorithm for determining when it is safe to do so.  If a peer is serving us a chain that seems to have low-work, how do we know that it's safe to delete, and that if we wait long enough we won't eventually learn a high-work chain from this peer?  This PR implements an algorithm for making that determination in a robust way that should guarantee that we can always reach consensus with the network.\r\n\r\n(I do think that we could prune headers on startup/shutdown, though I imagine that after this PR there will be much less need to consider doing so.)\r\n\r\n> The larger question I have is: why are we putting in effort to remove the IBD checkpoint? \r\n\r\nThis PR does not touch the checkpoints; it solves a problem that the checkpoints do not solve (preventing DoS attacks against new nodes), and happens to do so in a way that would make checkpoints no longer necessary.  However it's more appropriate to discuss this in https://github.com/bitcoin/bitcoin/pull/25725, the PR where that is proposed.  \r\n\r\nAs an aside for reviewers who may not be familiar with how checkpoints work: there is not a single \"IBD checkpoint\", there are many heights for which we have cached blockhashes which we expect at those heights, and once we download those block headers matching those checkpointed hashes, we no longer accept new headers that would fork below those heights.  Note that this is not a performance optimization during IBD, it is merely a protection against spam from low-difficulty headers.",
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1239539406",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7339974375,
      "node_id": "MEE_lADOABII585OmiYczwAAAAG1fx7n",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7339974375",
      "actor": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-09-07T15:26:23Z"
    },
    {
      "event": "subscribed",
      "id": 7339974383,
      "node_id": "SE_lADOABII585OmiYczwAAAAG1fx7v",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7339974383",
      "actor": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-09-07T15:26:23Z"
    },
    {
      "event": "commented",
      "id": 1240215857,
      "node_id": "IC_kwDOABII585J7DEx",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1240215857",
      "actor": {
        "login": "fresheneesz",
        "id": 149531,
        "node_id": "MDQ6VXNlcjE0OTUzMQ==",
        "avatar_url": "https://avatars.githubusercontent.com/u/149531?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/fresheneesz",
        "html_url": "https://github.com/fresheneesz",
        "followers_url": "https://api.github.com/users/fresheneesz/followers",
        "following_url": "https://api.github.com/users/fresheneesz/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/fresheneesz/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/fresheneesz/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/fresheneesz/subscriptions",
        "organizations_url": "https://api.github.com/users/fresheneesz/orgs",
        "repos_url": "https://api.github.com/users/fresheneesz/repos",
        "events_url": "https://api.github.com/users/fresheneesz/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/fresheneesz/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-09-08T04:49:18Z",
      "updated_at": "2022-09-08T04:49:18Z",
      "author_association": "NONE",
      "body": "> this would turn a memory attack into a disk-filling attack\r\n\r\nThe block headers are currently 60 megabytes. In another 15 years they'll be 120 megabytes. That doesn't seem like enough to credibly attack any full node. Plus, this PR is about only downloading \"enough PoW\" which means you don't need anywhere near that much disk space. Seems like the ideal way to do this is to download the latest X block headers, since those are the ones with the most PoW per bytes of block headers.\r\n\r\n>  deleting headers is not simple; in order to ensure we always can arrive at consensus, we need an algorithm for determining when it is safe to do so.\r\n\r\nThis PR already deletes headers as soon as they're received. If it simply stores them somewhere (and I do mean simply, ie not connecting it up with anything that would make it hard to safely delete) then they can be retrieved from that location instead of redownloading them. \r\n\r\nBut maybe I just don't understand the attack that deleting the headers upon receipt is intended to prevent. Is that attack detailed anywhere? Why would the node need to download more than one set of headers at a time? A node should query its peers for the longest chain, and if there are competing longest chains, it should choose the longest/heaviest chain and verify via the headers that the chain is as heavy as it says, then start verifying the actual blocks. \r\n\r\nUnder what circumstance is there a possibility of a disk-filling attack here?",
      "user": {
        "login": "fresheneesz",
        "id": 149531,
        "node_id": "MDQ6VXNlcjE0OTUzMQ==",
        "avatar_url": "https://avatars.githubusercontent.com/u/149531?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/fresheneesz",
        "html_url": "https://github.com/fresheneesz",
        "followers_url": "https://api.github.com/users/fresheneesz/followers",
        "following_url": "https://api.github.com/users/fresheneesz/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/fresheneesz/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/fresheneesz/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/fresheneesz/subscriptions",
        "organizations_url": "https://api.github.com/users/fresheneesz/orgs",
        "repos_url": "https://api.github.com/users/fresheneesz/repos",
        "events_url": "https://api.github.com/users/fresheneesz/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/fresheneesz/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1240215857",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1240759648,
      "node_id": "IC_kwDOABII585J9H1g",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1240759648",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-09-08T14:00:58Z",
      "updated_at": "2022-09-08T19:29:04Z",
      "author_association": "MEMBER",
      "body": "> The block headers are currently 60 megabytes. In another 15 years they'll be 120 megabytes. That doesn't seem like enough to credibly attack any full node. \r\n\r\nDue to the timewarp bug[1], it is inexpensive to construct a consensus-valid blockchain where block timestamps advance by one second every 6 blocks.   Starting at the genesis block timestamp (January 3, 2009), it's possible to construct a chain that has over 2.5 *billion* blocks, where the last block's timestamp is less than 2 hours in the future from the current time (September 2022).  If an adversary produced such a chain and fed it to a new node, then storing it on disk while we calculate the accumulated work would mean we store almost 100 GB of data for a single peer (assuming we could, say, compress the headers from 80 bytes to 40 bytes).  Of course, nodes can have many peers, up to 100 or more -- so without complex logic to limit the number of concurrent headers syncs (which would also need to be carefully designed to thwart adversaries that seek to block with headers sync from an honest peer) -- that could be 10TB of data being stored on disk.\r\n\r\n> Plus, this PR is about only downloading \"enough PoW\" which means you don't need anywhere near that much disk space. Seems like the ideal way to do this is to download the latest X block headers, since those are the ones with the most PoW per bytes of block headers.\r\n\r\nThere is no functionality in the p2p protocol to \"download the latest X block headers\".[2] \r\n\r\nIt would be great to design a protocol that would allow for proving the work on a chain in a compact/efficient way[3], but even if we had such a proposal ready to go, that wouldn't solve our problem -- in order to ensure we don't break consensus with old nodes, we cannot deploy a new p2p protocol **and** then only sync headers with new software that implements the protocol (at least, not without a sufficiently long period of time to ensure adequate deployment of the new software).  Thus we'd remain open to the DoS vector fixed in this PR until such time that we turned off headers sync from older software -- and I think that is inferior to just fixing this issue for new software in a backwards compatible way, as this PR does.\r\n\r\n> But maybe I just don't understand the attack that deleting the headers upon receipt is intended to prevent. Is that attack detailed anywhere? \r\n\r\nI don't know if my description above of the time warp bug is enough for you to understand the issue, but if you want some more background perhaps you can try the bitcoin stack exchange?  There is also a code comment in #25970 that you might find helpful, starting [here](https://github.com/bitcoin/bitcoin/pull/25970/commits/bc35761b44a2e483e3eee467686e3eb50897cb51#diff-469e31f028190051953bfc41772d1c2245511069209f738503d47dd64a7674adR41).\r\n\r\n> Why would the node need to download more than one set of headers at a time?\r\n\r\nDifferent peers might be on different chain tips, and the simplest behavior we can have is to sync headers with all of our peers, without regard to one another.  This can be bandwidth-wasteful when peers are on the same tip, so we have optimizations on startup to try to only sync with a single peer at first.  But if that peer is adversarial or broken then they might not give us the honest chain, so at some point we have to try other peers as well, to ensure that as long as we have at least one honest peer, we'll eventually sync the consensus chain (see #25720 for more discussion of this bandwidth tradeoff).  While you could try to design a sync behavior that would throttle or rate-limit headers sync in some way, so that we don't have multiple peers with which we're syncing headers at the same time, that is a more complex logic than always being willing to sync headers with all peers that might have something to offer us (which only requires per-peer logic that is independent of what other peers might be giving us).\r\n\r\n> A node should query its peers for the longest chain, and if there are competing longest chains, it should choose the longest/heaviest chain and verify via the headers that the chain is as heavy as it says, then start verifying the actual blocks.\r\n\r\nYes, obviously.  The details around how that is accomplished are what is being discussed, and the way we had been doing it prior to this PR was vulnerable to DoS, and hopefully now that this PR has been merged that should no longer be the case.  If you have a concrete proposal for another way we could implement DoS protection while syncing headers in a way that will always keep us in consensus with the network, please suggest some code, but vague comments like this are not specific enough to discuss.\r\n\r\n---\r\n\r\n[1] See https://bitcointalk.org/index.php?topic=43692.msg521772#msg521772 for one of the earliest-known (to me) descriptions of this problem.\r\n\r\n[2] The way headers are requested is via a \"block locator\", which is a set of block hashes that describe the chain that a node is on.  When our peer receives the locator, they are expected to find the first block hash in the locator that is on their chain, and then start sending us headers from that point on, towards their tip.  Headers messages have a maximum of 2000 headers in them, so the node receiving a full headers message in response is expected to continue requesting headers from the last header in the message.\r\n\r\n[3] Many years ago, there was a [proposal](https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-March/015851.html) to revamp the way headers are requested to try to allow for something like this, but this effort stalled and was not implemented or deployed.  This might be an interesting avenue for future work.\r\n \r\n",
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1240759648",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "commented",
      "id": 1240770626,
      "node_id": "IC_kwDOABII585J9KhC",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1240770626",
      "actor": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-09-08T14:09:28Z",
      "updated_at": "2022-09-08T14:09:28Z",
      "author_association": "MEMBER",
      "body": "> Plus, this PR is about only downloading \"enough PoW\" which means you don't need anywhere near that much disk space. Seems like the ideal way to do this is to download the latest X block headers, since those are the ones with the most PoW per bytes of block headers.\r\n\r\n@sdaftuar comments aside, this is simply *not true*. Mining difficulty can drop, and we don't want to lose consensus due to this, with nodes online following lower PoW headers, and newly syncing nodes deciding it's not high enough.",
      "user": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1240770626",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "mentioned",
      "id": 7348214247,
      "node_id": "MEE_lADOABII585OmiYczwAAAAG1_Nnn",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7348214247",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-09-08T14:09:29Z"
    },
    {
      "event": "subscribed",
      "id": 7348214258,
      "node_id": "SE_lADOABII585OmiYczwAAAAG1_Nny",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7348214258",
      "actor": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-09-08T14:09:29Z"
    },
    {
      "event": "subscribed",
      "id": 7363015543,
      "node_id": "SE_lADOABII585OmiYczwAAAAG23rN3",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7363015543",
      "actor": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-09-11T22:15:47Z"
    },
    {
      "event": "commented",
      "id": 1249010613,
      "node_id": "IC_kwDOABII585KcmO1",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/1249010613",
      "actor": {
        "login": "jonatack",
        "id": 2415484,
        "node_id": "MDQ6VXNlcjI0MTU0ODQ=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2415484?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jonatack",
        "html_url": "https://github.com/jonatack",
        "followers_url": "https://api.github.com/users/jonatack/followers",
        "following_url": "https://api.github.com/users/jonatack/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jonatack/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jonatack/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jonatack/subscriptions",
        "organizations_url": "https://api.github.com/users/jonatack/orgs",
        "repos_url": "https://api.github.com/users/jonatack/repos",
        "events_url": "https://api.github.com/users/jonatack/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jonatack/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2022-09-16T07:20:43Z",
      "updated_at": "2022-09-16T07:20:43Z",
      "author_association": "MEMBER",
      "body": "> 2022-09-15  \\<sipa\\> For 25717 observing the pre-syncing phase is one thing (it should be there), but arguably the more interesting property is that syncing still works at all. It's only triggered when syncing a new node from scratch, or one that is ~months or more behind.\r\n\r\nCatching up with yesterday's IRC meeting, regarding https://www.erisian.com.au/bitcoin-core-dev/log-2022-09-15.html#l-258, I synced a new node from scratch on master a week ago without issue.",
      "user": {
        "login": "jonatack",
        "id": 2415484,
        "node_id": "MDQ6VXNlcjI0MTU0ODQ=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2415484?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jonatack",
        "html_url": "https://github.com/jonatack",
        "followers_url": "https://api.github.com/users/jonatack/followers",
        "following_url": "https://api.github.com/users/jonatack/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jonatack/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jonatack/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jonatack/subscriptions",
        "organizations_url": "https://api.github.com/users/jonatack/orgs",
        "repos_url": "https://api.github.com/users/jonatack/repos",
        "events_url": "https://api.github.com/users/jonatack/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jonatack/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1249010613",
      "issue_url": "https://api.github.com/repos/bitcoin/bitcoin/issues/25717"
    },
    {
      "event": "referenced",
      "id": 7451424824,
      "node_id": "REFE_lADOABII585OmiYczwAAAAG8I7g4",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7451424824",
      "actor": {
        "login": "LarryRuane",
        "id": 8321330,
        "node_id": "MDQ6VXNlcjgzMjEzMzA=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8321330?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/LarryRuane",
        "html_url": "https://github.com/LarryRuane",
        "followers_url": "https://api.github.com/users/LarryRuane/followers",
        "following_url": "https://api.github.com/users/LarryRuane/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/LarryRuane/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/LarryRuane/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/LarryRuane/subscriptions",
        "organizations_url": "https://api.github.com/users/LarryRuane/orgs",
        "repos_url": "https://api.github.com/users/LarryRuane/repos",
        "events_url": "https://api.github.com/users/LarryRuane/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/LarryRuane/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": "bdcafb913398f0cdaff9c880618f9ebfc85c7693",
      "commit_url": "https://api.github.com/repos/LarryRuane/bitcoin/commits/bdcafb913398f0cdaff9c880618f9ebfc85c7693",
      "created_at": "2022-09-24T06:15:57Z"
    },
    {
      "event": "referenced",
      "id": 7466953359,
      "node_id": "REFE_lADOABII585OmiYczwAAAAG9EKqP",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7466953359",
      "actor": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": "9fcdb9f3a044330d3d7515fa35709102c98534d2",
      "commit_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/9fcdb9f3a044330d3d7515fa35709102c98534d2",
      "created_at": "2022-09-27T10:04:24Z"
    },
    {
      "event": "referenced",
      "id": 7469484030,
      "node_id": "REFE_lADOABII585OmiYczwAAAAG9N0f-",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7469484030",
      "actor": {
        "login": "sidhujag",
        "id": 6238042,
        "node_id": "MDQ6VXNlcjYyMzgwNDI=",
        "avatar_url": "https://avatars.githubusercontent.com/u/6238042?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sidhujag",
        "html_url": "https://github.com/sidhujag",
        "followers_url": "https://api.github.com/users/sidhujag/followers",
        "following_url": "https://api.github.com/users/sidhujag/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sidhujag/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sidhujag/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sidhujag/subscriptions",
        "organizations_url": "https://api.github.com/users/sidhujag/orgs",
        "repos_url": "https://api.github.com/users/sidhujag/repos",
        "events_url": "https://api.github.com/users/sidhujag/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sidhujag/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": "46070827fcfdb5f6b5c8091e8f6870b5243a3ab5",
      "commit_url": "https://api.github.com/repos/syscoin/syscoin/commits/46070827fcfdb5f6b5c8091e8f6870b5243a3ab5",
      "created_at": "2022-09-27T14:56:46Z"
    },
    {
      "event": "referenced",
      "id": 7484960562,
      "node_id": "REFE_lADOABII585OmiYczwAAAAG-I28y",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7484960562",
      "actor": {
        "login": "fanquake",
        "id": 863730,
        "node_id": "MDQ6VXNlcjg2MzczMA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/863730?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/fanquake",
        "html_url": "https://github.com/fanquake",
        "followers_url": "https://api.github.com/users/fanquake/followers",
        "following_url": "https://api.github.com/users/fanquake/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/fanquake/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/fanquake/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/fanquake/subscriptions",
        "organizations_url": "https://api.github.com/users/fanquake/orgs",
        "repos_url": "https://api.github.com/users/fanquake/repos",
        "events_url": "https://api.github.com/users/fanquake/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/fanquake/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": "e4f7fd34615d05f6134b4f2e4d63aaa69b4a4f50",
      "commit_url": "https://api.github.com/repos/fanquake/bitcoin/commits/e4f7fd34615d05f6134b4f2e4d63aaa69b4a4f50",
      "created_at": "2022-09-29T09:30:41Z"
    },
    {
      "event": "referenced",
      "id": 7558072571,
      "node_id": "REFE_lADOABII585OmiYczwAAAAHCfwj7",
      "url": "https://api.github.com/repos/bitcoin/bitcoin/issues/events/7558072571",
      "actor": {
        "login": "fanquake",
        "id": 863730,
        "node_id": "MDQ6VXNlcjg2MzczMA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/863730?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/fanquake",
        "html_url": "https://github.com/fanquake",
        "followers_url": "https://api.github.com/users/fanquake/followers",
        "following_url": "https://api.github.com/users/fanquake/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/fanquake/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/fanquake/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/fanquake/subscriptions",
        "organizations_url": "https://api.github.com/users/fanquake/orgs",
        "repos_url": "https://api.github.com/users/fanquake/repos",
        "events_url": "https://api.github.com/users/fanquake/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/fanquake/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": "7e0bcfbfef61cb688bc92a96003c1219cad67935",
      "commit_url": "https://api.github.com/repos/fanquake/bitcoin/commits/7e0bcfbfef61cb688bc92a96003c1219cad67935",
      "created_at": "2022-10-11T01:20:37Z"
    }
  ],
  "comments": [
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932182732",
      "pull_request_review_id": 1054040362,
      "id": 932182732,
      "node_id": "PRRC_kwDOABII5843j_rM",
      "diff_hunk": "@@ -0,0 +1,246 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <chain.h>\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <arith_uint256.h>\n+#include <net.h> // For NodeId\n+#include <consensus/params.h>\n+#include <util/hasher.h>\n+#include <util/bitdeque.h>\n+\n+#include <vector>\n+#include <deque>",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 18,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": null,
      "user": {
        "login": "dergoegge",
        "id": 8077169,
        "node_id": "MDQ6VXNlcjgwNzcxNjk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8077169?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/dergoegge",
        "html_url": "https://github.com/dergoegge",
        "followers_url": "https://api.github.com/users/dergoegge/followers",
        "following_url": "https://api.github.com/users/dergoegge/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/dergoegge/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/dergoegge/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/dergoegge/subscriptions",
        "organizations_url": "https://api.github.com/users/dergoegge/orgs",
        "repos_url": "https://api.github.com/users/dergoegge/repos",
        "events_url": "https://api.github.com/users/dergoegge/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/dergoegge/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "nit: please sort these alphabetically",
      "created_at": "2022-07-28T12:55:03Z",
      "updated_at": "2022-07-28T14:41:32Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932182732",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932182732"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 8,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 17,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932184020",
      "pull_request_review_id": 1054040362,
      "id": 932184020,
      "node_id": "PRRC_kwDOABII5843j__U",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 5,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": null,
      "user": {
        "login": "dergoegge",
        "id": 8077169,
        "node_id": "MDQ6VXNlcjgwNzcxNjk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8077169?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/dergoegge",
        "html_url": "https://github.com/dergoegge",
        "followers_url": "https://api.github.com/users/dergoegge/followers",
        "following_url": "https://api.github.com/users/dergoegge/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/dergoegge/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/dergoegge/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/dergoegge/subscriptions",
        "organizations_url": "https://api.github.com/users/dergoegge/orgs",
        "repos_url": "https://api.github.com/users/dergoegge/repos",
        "events_url": "https://api.github.com/users/dergoegge/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/dergoegge/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "nit: please sort these alphabetically",
      "created_at": "2022-07-28T12:56:16Z",
      "updated_at": "2022-07-28T14:41:33Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932184020",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932184020"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 1,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 8,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932186749",
      "pull_request_review_id": 1054040362,
      "id": 932186749,
      "node_id": "PRRC_kwDOABII5843kAp9",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    // Overestimate the number of blocks that could possibly exist on this\n+    // chain using 6 blocks/second (fastest blockrate given the MTP rule) times\n+    // the number of seconds from the last allowed block until today. This\n+    // serves as a memory bound on how many commitments we might store from\n+    // this peer, and we can safely give up syncing if the peer exceeds this\n+    // bound, because it's not possible for a consensus-valid chain to be\n+    // longer than this.\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+// Process the next batch of headers received from our peer.\n+// Validate and store commitments, and compare total chainwork to our target to\n+// see if we can switch to REDOWNLOAD mode.\n+std::optional<CBlockLocator> HeadersSyncState::ProcessNextHeaders(const std::vector<CBlockHeader>&\n+        headers, bool full_headers_message, std::vector<CBlockHeader>&\n+        headers_to_process, bool& processing_success)\n+{\n+    processing_success = false;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return std::nullopt;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return std::nullopt;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+            // Somehow our peer gave us a header that doesn't connect.\n+            // This might be benign -- perhaps we issued an extra getheaders\n+            // message, such as after a block INV was received.\n+            // Or it could be that our peer is broken or malicious. If broken,\n+            // sending a new getheaders immediately could trigger an infinite\n+            // loop. Just give up for now; if our peer ever gives us an block\n+            // INV later we will fetch headers then, and likely retrigger this\n+            // logic.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return std::nullopt;\n+        }\n+        processing_success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return std::nullopt;\n+            }\n+        }\n+\n+        processing_success = true;\n+        // Return any headers that are ready for acceptance.\n+        headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return std::nullopt;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    }\n+    return std::nullopt;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (m_last_header_received.IsNull()) {\n+        m_last_header_received = m_chain_start->GetBlockHeader();\n+        m_current_height = m_chain_start->nHeight;\n+    }\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Doesn't connect to what we expect\n+        Finalize();\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr, m_current_height+1)) {\n+            Finalize();\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_blockhash_with_sufficient_work = headers.back().GetHash();\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash.SetNull();\n+        m_download_state = State::REDOWNLOAD;\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current, int64_t current_height)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, current_height,\n+                previous.nBits, current.nBits)) {\n+        return false;\n+    }\n+\n+    if (!CheckProofOfWork(current.GetHash(), current.nBits, m_consensus_params)) return false;\n+\n+    if ((current_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+        // Try to add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // TODO: disconnect this peer.",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 220,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": null,
      "user": {
        "login": "dergoegge",
        "id": 8077169,
        "node_id": "MDQ6VXNlcjgwNzcxNjk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8077169?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/dergoegge",
        "html_url": "https://github.com/dergoegge",
        "followers_url": "https://api.github.com/users/dergoegge/followers",
        "following_url": "https://api.github.com/users/dergoegge/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/dergoegge/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/dergoegge/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/dergoegge/subscriptions",
        "organizations_url": "https://api.github.com/users/dergoegge/orgs",
        "repos_url": "https://api.github.com/users/dergoegge/repos",
        "events_url": "https://api.github.com/users/dergoegge/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/dergoegge/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Why not do that in this PR? Maybe add a new state `FAILED` (or an extra bool on `HeadersSyncState`) that signals to `net_processing` that the peer should be disconnected? That seems simple enough and in obvious cases like this one (i.e. too many commitments or e.g. invalid PoW) the peer definitely behaved funky and should be punished.",
      "created_at": "2022-07-28T12:58:51Z",
      "updated_at": "2022-07-28T14:41:33Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932186749",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932186749"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 225,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932235084",
      "pull_request_review_id": 1054116591,
      "id": 932235084,
      "node_id": "PRRC_kwDOABII5843kMdM",
      "diff_hunk": "@@ -71,6 +71,54 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n     return bnNew.GetCompact();\n }\n \n+// Check that on difficulty adjustments, the new difficulty does not increase\n+// or decrease beyond the permitted limits.\n+bool PermittedDifficultyTransition(const Consensus::Params& params, int64_t height, uint32_t old_nbits, uint32_t new_nbits)\n+{\n+    if (params.fPowAllowMinDifficultyBlocks) return true;\n+\n+    if (height % params.DifficultyAdjustmentInterval() == 0) {\n+        int64_t smallest_timespan = params.nPowTargetTimespan/4;\n+        int64_t largest_timespan = params.nPowTargetTimespan*4;\n+\n+        const arith_uint256 bnPowLimit = UintToArith256(params.powLimit);\n+        arith_uint256 observed_new_target;\n+        observed_new_target.SetCompact(new_nbits);\n+\n+        // Calculate the largest difficulty value possible:\n+        arith_uint256 bnNew;\n+        bnNew.SetCompact(old_nbits);\n+        bnNew *= largest_timespan;\n+        bnNew /= params.nPowTargetTimespan;\n+\n+        if (bnNew > bnPowLimit)\n+            bnNew = bnPowLimit;",
      "path": "src/pow.cpp",
      "position": null,
      "original_position": 25,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cbf3069c7d76888a647d68dbb09d77e666dcaf50",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Nit in \"Add function to validate difficulty changes\":\r\n\r\nI realize this is mostly-copied code, but can you follow the style guide here (braces when the then statement isn't on the same line)?\r\n\r\n(here and below)",
      "created_at": "2022-07-28T13:42:00Z",
      "updated_at": "2022-07-28T14:52:00Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932235084",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932235084"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 95,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932238513",
      "pull_request_review_id": 1054116591,
      "id": 932238513,
      "node_id": "PRRC_kwDOABII5843kNSx",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 7,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Nit in \"Utilize anti-DoS headers download strategy\":\r\n\r\nPerhaps use doxygen comments for comments on a function/variable definition or declaration? (either `/** ... */` or `//! ...`). That way they get picked up by doxygen for generating documentation.",
      "created_at": "2022-07-28T13:45:03Z",
      "updated_at": "2022-07-28T14:52:00Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932238513",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932238513"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 7,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932252053",
      "pull_request_review_id": 1054116591,
      "id": 932252053,
      "node_id": "PRRC_kwDOABII5843kQmV",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()",
      "path": "src/headerssync.cpp",
      "position": 51,
      "original_position": 24,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Maybe also wipe `m_chain_start_locator`?",
      "created_at": "2022-07-28T13:56:14Z",
      "updated_at": "2022-07-28T14:52:00Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932252053",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932252053"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 51,
      "original_line": 51,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932256694",
      "pull_request_review_id": 1054116591,
      "id": 932256694,
      "node_id": "PRRC_kwDOABII5843kRu2",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 23,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "In commit \"Utilize anti-DoS headers download strategy\"\r\n\r\nPerhaps document that this is required to guarantee that the object isn't reused with the same SaltedTxidHasher for another sync attempt.",
      "created_at": "2022-07-28T14:00:08Z",
      "updated_at": "2022-07-28T14:52:00Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932256694",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932256694"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 23,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932259718",
      "pull_request_review_id": 1054116591,
      "id": 932259718,
      "node_id": "PRRC_kwDOABII5843kSeG",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    // Overestimate the number of blocks that could possibly exist on this\n+    // chain using 6 blocks/second (fastest blockrate given the MTP rule) times\n+    // the number of seconds from the last allowed block until today. This\n+    // serves as a memory bound on how many commitments we might store from\n+    // this peer, and we can safely give up syncing if the peer exceeds this\n+    // bound, because it's not possible for a consensus-valid chain to be\n+    // longer than this.",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 61,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "In commit \"Utilize anti-DoS headers download strategy\"\r\n\r\nArguably, it's not exactly true that no consensus-valid chain can be longer, but it's not possible for the peer to have such a chain at the time the sync starts (it'd need a tip timestamp that is in the future at the time the sync starts).",
      "created_at": "2022-07-28T14:02:39Z",
      "updated_at": "2022-07-28T14:52:00Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932259718",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932259718"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 61,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932271718",
      "pull_request_review_id": 1054116591,
      "id": 932271718,
      "node_id": "PRRC_kwDOABII5843kVZm",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    // Overestimate the number of blocks that could possibly exist on this\n+    // chain using 6 blocks/second (fastest blockrate given the MTP rule) times\n+    // the number of seconds from the last allowed block until today. This\n+    // serves as a memory bound on how many commitments we might store from\n+    // this peer, and we can safely give up syncing if the peer exceeds this\n+    // bound, because it's not possible for a consensus-valid chain to be\n+    // longer than this.\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+// Process the next batch of headers received from our peer.\n+// Validate and store commitments, and compare total chainwork to our target to\n+// see if we can switch to REDOWNLOAD mode.\n+std::optional<CBlockLocator> HeadersSyncState::ProcessNextHeaders(const std::vector<CBlockHeader>&\n+        headers, bool full_headers_message, std::vector<CBlockHeader>&\n+        headers_to_process, bool& processing_success)\n+{\n+    processing_success = false;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return std::nullopt;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return std::nullopt;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+            // Somehow our peer gave us a header that doesn't connect.\n+            // This might be benign -- perhaps we issued an extra getheaders\n+            // message, such as after a block INV was received.\n+            // Or it could be that our peer is broken or malicious. If broken,\n+            // sending a new getheaders immediately could trigger an infinite\n+            // loop. Just give up for now; if our peer ever gives us an block\n+            // INV later we will fetch headers then, and likely retrigger this\n+            // logic.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return std::nullopt;\n+        }\n+        processing_success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return std::nullopt;\n+            }\n+        }\n+\n+        processing_success = true;\n+        // Return any headers that are ready for acceptance.\n+        headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return std::nullopt;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    }\n+    return std::nullopt;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (m_last_header_received.IsNull()) {",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 168,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "In commit \"Utilize anti-DoS headers download strategy\"\r\n\r\nAny reason to do this here, and not in `StartInitialDownload`? It seems a bit cleaner to set these values as soon as they're known, rather than on the fly.",
      "created_at": "2022-07-28T14:13:01Z",
      "updated_at": "2022-07-28T14:52:00Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932271718",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932271718"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 168,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932272722",
      "pull_request_review_id": 1054116591,
      "id": 932272722,
      "node_id": "PRRC_kwDOABII5843kVpS",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 47,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "In commit \"Utilize anti-DoS headers download strategy\"\r\n\r\nShould there be a restriction to not call StartInitialDownload twice on the same object?",
      "created_at": "2022-07-28T14:13:52Z",
      "updated_at": "2022-07-28T14:52:00Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932272722",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932272722"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 50,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932277428",
      "pull_request_review_id": 1054040362,
      "id": 932277428,
      "node_id": "PRRC_kwDOABII5843kWy0",
      "diff_hunk": "@@ -0,0 +1,68 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+import time\n+\n+NODE1_BLOCKS_REQUIRED = 16\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 16 blocks; node2 requires 2048 blocks\n+        self.extra_args = [[\"-minimumchainwork=0x0\"], [\"-minimumchainwork=0x1f\"], [\"-minimumchainwork=0x1000\"]]\n+\n+    def skip_test_if_missing_module(self):\n+        self.skip_if_no_wallet()\n+\n+    def test_chains_sync_when_long_enough(self):\n+        self.log.info(\"Generate blocks on the node with no required chainwork\")\n+        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED-2, sync_fun=self.no_op)\n+\n+        self.log.info(\"Verify nodes 1 and 2 have no new headers in their headers tree\")\n+        time.sleep(2)\n+        for i in range(1, 2):\n+            chaintips = self.nodes[i].getchaintips()\n+            assert(len(chaintips) == 1)\n+            assert {\n+                'height': 0,\n+                'hash': '0f9188f13cb7b2c71f2a335e3a4fc328bf5beb436012afca590b1a11466e2206',\n+                'branchlen': 0,\n+                'status': 'active',\n+            } in chaintips",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 39,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": null,
      "user": {
        "login": "dergoegge",
        "id": 8077169,
        "node_id": "MDQ6VXNlcjgwNzcxNjk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8077169?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/dergoegge",
        "html_url": "https://github.com/dergoegge",
        "followers_url": "https://api.github.com/users/dergoegge/followers",
        "following_url": "https://api.github.com/users/dergoegge/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/dergoegge/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/dergoegge/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/dergoegge/subscriptions",
        "organizations_url": "https://api.github.com/users/dergoegge/orgs",
        "repos_url": "https://api.github.com/users/dergoegge/repos",
        "events_url": "https://api.github.com/users/dergoegge/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/dergoegge/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "This only checks node 1.\r\n```suggestion\r\n        for node in self.nodes[1:]:\r\n            chaintips = node.getchaintips()\r\n            assert(len(chaintips) == 1)\r\n            assert {\r\n                'height': 0,\r\n                'hash': '0f9188f13cb7b2c71f2a335e3a4fc328bf5beb436012afca590b1a11466e2206',\r\n                'branchlen': 0,\r\n                'status': 'active',\r\n            } in chaintips\r\n```",
      "created_at": "2022-07-28T14:17:48Z",
      "updated_at": "2022-07-28T14:41:33Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932277428",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932277428"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 31,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 61,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932278011",
      "pull_request_review_id": 1054116591,
      "id": 932278011,
      "node_id": "PRRC_kwDOABII5843kW77",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    // Overestimate the number of blocks that could possibly exist on this\n+    // chain using 6 blocks/second (fastest blockrate given the MTP rule) times\n+    // the number of seconds from the last allowed block until today. This\n+    // serves as a memory bound on how many commitments we might store from\n+    // this peer, and we can safely give up syncing if the peer exceeds this\n+    // bound, because it's not possible for a consensus-valid chain to be\n+    // longer than this.\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+// Process the next batch of headers received from our peer.\n+// Validate and store commitments, and compare total chainwork to our target to\n+// see if we can switch to REDOWNLOAD mode.\n+std::optional<CBlockLocator> HeadersSyncState::ProcessNextHeaders(const std::vector<CBlockHeader>&\n+        headers, bool full_headers_message, std::vector<CBlockHeader>&\n+        headers_to_process, bool& processing_success)\n+{\n+    processing_success = false;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return std::nullopt;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return std::nullopt;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+            // Somehow our peer gave us a header that doesn't connect.\n+            // This might be benign -- perhaps we issued an extra getheaders\n+            // message, such as after a block INV was received.\n+            // Or it could be that our peer is broken or malicious. If broken,\n+            // sending a new getheaders immediately could trigger an infinite\n+            // loop. Just give up for now; if our peer ever gives us an block\n+            // INV later we will fetch headers then, and likely retrigger this\n+            // logic.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return std::nullopt;\n+        }\n+        processing_success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return std::nullopt;\n+            }\n+        }\n+\n+        processing_success = true;\n+        // Return any headers that are ready for acceptance.\n+        headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return std::nullopt;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    }\n+    return std::nullopt;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (m_last_header_received.IsNull()) {\n+        m_last_header_received = m_chain_start->GetBlockHeader();\n+        m_current_height = m_chain_start->nHeight;\n+    }\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Doesn't connect to what we expect\n+        Finalize();\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 179,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "In commit \"Utilize anti-DoS headers download strategy\"\r\n\r\nPerhaps document that we rely on the caller having verified that the headers are continuous (each has the previous one's hash as their prevhash)?",
      "created_at": "2022-07-28T14:18:19Z",
      "updated_at": "2022-07-28T14:52:00Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932278011",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932278011"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 158,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932281150",
      "pull_request_review_id": 1054040362,
      "id": 932281150,
      "node_id": "PRRC_kwDOABII5843kXs-",
      "diff_hunk": "@@ -0,0 +1,68 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+import time\n+\n+NODE1_BLOCKS_REQUIRED = 16\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 16 blocks; node2 requires 2048 blocks\n+        self.extra_args = [[\"-minimumchainwork=0x0\"], [\"-minimumchainwork=0x1f\"], [\"-minimumchainwork=0x1000\"]]\n+\n+    def skip_test_if_missing_module(self):\n+        self.skip_if_no_wallet()\n+\n+    def test_chains_sync_when_long_enough(self):\n+        self.log.info(\"Generate blocks on the node with no required chainwork\")\n+        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED-2, sync_fun=self.no_op)\n+\n+        self.log.info(\"Verify nodes 1 and 2 have no new headers in their headers tree\")\n+        time.sleep(2)",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 30,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": null,
      "user": {
        "login": "dergoegge",
        "id": 8077169,
        "node_id": "MDQ6VXNlcjgwNzcxNjk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8077169?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/dergoegge",
        "html_url": "https://github.com/dergoegge",
        "followers_url": "https://api.github.com/users/dergoegge/followers",
        "following_url": "https://api.github.com/users/dergoegge/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/dergoegge/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/dergoegge/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/dergoegge/subscriptions",
        "organizations_url": "https://api.github.com/users/dergoegge/orgs",
        "repos_url": "https://api.github.com/users/dergoegge/repos",
        "events_url": "https://api.github.com/users/dergoegge/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/dergoegge/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think you could get rid of the `time.sleep` with `assert_debug_log`.\r\n```suggestion\r\n    with self.nodes[1].assert_debug_log(expected_msgs=[\"[net] Ignoring low-work chain\"]), self.nodes[2].assert_debug_log(expected_msgs=[\"[net] Ignoring low-work chain\"]):\r\n         self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED-2, sync_fun=self.no_op)\r\n```",
      "created_at": "2022-07-28T14:21:05Z",
      "updated_at": "2022-07-28T14:41:33Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932281150",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932281150"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 27,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 30,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932281858",
      "pull_request_review_id": 1054116591,
      "id": 932281858,
      "node_id": "PRRC_kwDOABII5843kX4C",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    // Overestimate the number of blocks that could possibly exist on this\n+    // chain using 6 blocks/second (fastest blockrate given the MTP rule) times\n+    // the number of seconds from the last allowed block until today. This\n+    // serves as a memory bound on how many commitments we might store from\n+    // this peer, and we can safely give up syncing if the peer exceeds this\n+    // bound, because it's not possible for a consensus-valid chain to be\n+    // longer than this.\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+// Process the next batch of headers received from our peer.\n+// Validate and store commitments, and compare total chainwork to our target to\n+// see if we can switch to REDOWNLOAD mode.\n+std::optional<CBlockLocator> HeadersSyncState::ProcessNextHeaders(const std::vector<CBlockHeader>&\n+        headers, bool full_headers_message, std::vector<CBlockHeader>&\n+        headers_to_process, bool& processing_success)\n+{\n+    processing_success = false;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return std::nullopt;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return std::nullopt;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+            // Somehow our peer gave us a header that doesn't connect.\n+            // This might be benign -- perhaps we issued an extra getheaders\n+            // message, such as after a block INV was received.\n+            // Or it could be that our peer is broken or malicious. If broken,\n+            // sending a new getheaders immediately could trigger an infinite\n+            // loop. Just give up for now; if our peer ever gives us an block\n+            // INV later we will fetch headers then, and likely retrigger this\n+            // logic.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return std::nullopt;\n+        }\n+        processing_success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return std::nullopt;\n+            }\n+        }\n+\n+        processing_success = true;\n+        // Return any headers that are ready for acceptance.\n+        headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return std::nullopt;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    }\n+    return std::nullopt;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (m_last_header_received.IsNull()) {\n+        m_last_header_received = m_chain_start->GetBlockHeader();\n+        m_current_height = m_chain_start->nHeight;\n+    }\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Doesn't connect to what we expect\n+        Finalize();\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr, m_current_height+1)) {\n+            Finalize();\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_blockhash_with_sufficient_work = headers.back().GetHash();\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash.SetNull();\n+        m_download_state = State::REDOWNLOAD;\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current, int64_t current_height)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, current_height,\n+                previous.nBits, current.nBits)) {\n+        return false;\n+    }\n+\n+    if (!CheckProofOfWork(current.GetHash(), current.nBits, m_consensus_params)) return false;\n+\n+    if ((current_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+        // Try to add a commitment.",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 216,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Nit in \"Utilize anti-DoS headers download strategy\"\r\n\r\nWhy \"try\"? It doesn't look like the adding of a commitment can fail.",
      "created_at": "2022-07-28T14:21:42Z",
      "updated_at": "2022-07-28T14:52:00Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932281858",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932281858"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 216,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932282250",
      "pull_request_review_id": 1054040362,
      "id": 932282250,
      "node_id": "PRRC_kwDOABII5843kX-K",
      "diff_hunk": "@@ -0,0 +1,68 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+import time\n+\n+NODE1_BLOCKS_REQUIRED = 16\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 16 blocks; node2 requires 2048 blocks\n+        self.extra_args = [[\"-minimumchainwork=0x0\"], [\"-minimumchainwork=0x1f\"], [\"-minimumchainwork=0x1000\"]]\n+\n+    def skip_test_if_missing_module(self):\n+        self.skip_if_no_wallet()\n+\n+    def test_chains_sync_when_long_enough(self):\n+        self.log.info(\"Generate blocks on the node with no required chainwork\")\n+        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED-2, sync_fun=self.no_op)\n+\n+        self.log.info(\"Verify nodes 1 and 2 have no new headers in their headers tree\")",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 29,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": null,
      "user": {
        "login": "dergoegge",
        "id": 8077169,
        "node_id": "MDQ6VXNlcjgwNzcxNjk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8077169?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/dergoegge",
        "html_url": "https://github.com/dergoegge",
        "followers_url": "https://api.github.com/users/dergoegge/followers",
        "following_url": "https://api.github.com/users/dergoegge/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/dergoegge/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/dergoegge/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/dergoegge/subscriptions",
        "organizations_url": "https://api.github.com/users/dergoegge/orgs",
        "repos_url": "https://api.github.com/users/dergoegge/repos",
        "events_url": "https://api.github.com/users/dergoegge/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/dergoegge/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "node 0 and node 2 are not connected in this test but it looks like that is an assumption here?",
      "created_at": "2022-07-28T14:22:02Z",
      "updated_at": "2022-07-28T14:41:33Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932282250",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932282250"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 29,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932283353",
      "pull_request_review_id": 1054040362,
      "id": 932283353,
      "node_id": "PRRC_kwDOABII5843kYPZ",
      "diff_hunk": "@@ -0,0 +1,68 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+import time\n+\n+NODE1_BLOCKS_REQUIRED = 16\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 16 blocks; node2 requires 2048 blocks",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 19,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": null,
      "user": {
        "login": "dergoegge",
        "id": 8077169,
        "node_id": "MDQ6VXNlcjgwNzcxNjk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8077169?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/dergoegge",
        "html_url": "https://github.com/dergoegge",
        "followers_url": "https://api.github.com/users/dergoegge/followers",
        "following_url": "https://api.github.com/users/dergoegge/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/dergoegge/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/dergoegge/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/dergoegge/subscriptions",
        "organizations_url": "https://api.github.com/users/dergoegge/orgs",
        "repos_url": "https://api.github.com/users/dergoegge/repos",
        "events_url": "https://api.github.com/users/dergoegge/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/dergoegge/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Why does `NODE1_BLOCKS_REQUIRED` match with the comment but `NODE2_BLOCKS_REQUIRED` doesn't?",
      "created_at": "2022-07-28T14:22:56Z",
      "updated_at": "2022-07-28T14:41:33Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932283353",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932283353"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 19,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932286853",
      "pull_request_review_id": 1054040362,
      "id": 932286853,
      "node_id": "PRRC_kwDOABII5843kZGF",
      "diff_hunk": "@@ -0,0 +1,68 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+import time\n+\n+NODE1_BLOCKS_REQUIRED = 16\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 16 blocks; node2 requires 2048 blocks\n+        self.extra_args = [[\"-minimumchainwork=0x0\"], [\"-minimumchainwork=0x1f\"], [\"-minimumchainwork=0x1000\"]]\n+\n+    def skip_test_if_missing_module(self):\n+        self.skip_if_no_wallet()\n+\n+    def test_chains_sync_when_long_enough(self):\n+        self.log.info(\"Generate blocks on the node with no required chainwork\")\n+        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED-2, sync_fun=self.no_op)\n+\n+        self.log.info(\"Verify nodes 1 and 2 have no new headers in their headers tree\")\n+        time.sleep(2)\n+        for i in range(1, 2):\n+            chaintips = self.nodes[i].getchaintips()\n+            assert(len(chaintips) == 1)\n+            assert {\n+                'height': 0,\n+                'hash': '0f9188f13cb7b2c71f2a335e3a4fc328bf5beb436012afca590b1a11466e2206',\n+                'branchlen': 0,\n+                'status': 'active',\n+            } in chaintips\n+\n+        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED - self.nodes[0].getblockcount(), sync_fun=self.no_op)\n+        self.log.info(\"Verify that node1 syncs node0's chain\")\n+        time.sleep(2)",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 43,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": null,
      "user": {
        "login": "dergoegge",
        "id": 8077169,
        "node_id": "MDQ6VXNlcjgwNzcxNjk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8077169?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/dergoegge",
        "html_url": "https://github.com/dergoegge",
        "followers_url": "https://api.github.com/users/dergoegge/followers",
        "following_url": "https://api.github.com/users/dergoegge/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/dergoegge/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/dergoegge/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/dergoegge/subscriptions",
        "organizations_url": "https://api.github.com/users/dergoegge/orgs",
        "repos_url": "https://api.github.com/users/dergoegge/repos",
        "events_url": "https://api.github.com/users/dergoegge/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/dergoegge/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think this `time.sleep` is not needed given the `sync_blocks` call below?",
      "created_at": "2022-07-28T14:25:13Z",
      "updated_at": "2022-07-28T14:41:33Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932286853",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932286853"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 43,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932288256",
      "pull_request_review_id": 1054040362,
      "id": 932288256,
      "node_id": "PRRC_kwDOABII5843kZcA",
      "diff_hunk": "@@ -0,0 +1,68 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+import time\n+\n+NODE1_BLOCKS_REQUIRED = 16\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 16 blocks; node2 requires 2048 blocks\n+        self.extra_args = [[\"-minimumchainwork=0x0\"], [\"-minimumchainwork=0x1f\"], [\"-minimumchainwork=0x1000\"]]\n+\n+    def skip_test_if_missing_module(self):\n+        self.skip_if_no_wallet()\n+\n+    def test_chains_sync_when_long_enough(self):\n+        self.log.info(\"Generate blocks on the node with no required chainwork\")\n+        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED-2, sync_fun=self.no_op)\n+\n+        self.log.info(\"Verify nodes 1 and 2 have no new headers in their headers tree\")\n+        time.sleep(2)\n+        for i in range(1, 2):\n+            chaintips = self.nodes[i].getchaintips()\n+            assert(len(chaintips) == 1)\n+            assert {\n+                'height': 0,\n+                'hash': '0f9188f13cb7b2c71f2a335e3a4fc328bf5beb436012afca590b1a11466e2206',\n+                'branchlen': 0,\n+                'status': 'active',\n+            } in chaintips\n+\n+        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED - self.nodes[0].getblockcount(), sync_fun=self.no_op)\n+        self.log.info(\"Verify that node1 syncs node0's chain\")\n+        time.sleep(2)\n+        self.sync_blocks(self.nodes[0:2])\n+\n+        self.log.info(\"Verify that node2 still has no new headers\")\n+        time.sleep(2)",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 47,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": null,
      "user": {
        "login": "dergoegge",
        "id": 8077169,
        "node_id": "MDQ6VXNlcjgwNzcxNjk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8077169?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/dergoegge",
        "html_url": "https://github.com/dergoegge",
        "followers_url": "https://api.github.com/users/dergoegge/followers",
        "following_url": "https://api.github.com/users/dergoegge/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/dergoegge/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/dergoegge/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/dergoegge/subscriptions",
        "organizations_url": "https://api.github.com/users/dergoegge/orgs",
        "repos_url": "https://api.github.com/users/dergoegge/repos",
        "events_url": "https://api.github.com/users/dergoegge/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/dergoegge/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Could also use `assert_debug_log` like suggested above instead of the sleep.",
      "created_at": "2022-07-28T14:26:00Z",
      "updated_at": "2022-07-28T14:41:33Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932288256",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932288256"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 46,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932294356",
      "pull_request_review_id": 1054116591,
      "id": 932294356,
      "node_id": "PRRC_kwDOABII5843ka7U",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    // Overestimate the number of blocks that could possibly exist on this\n+    // chain using 6 blocks/second (fastest blockrate given the MTP rule) times\n+    // the number of seconds from the last allowed block until today. This\n+    // serves as a memory bound on how many commitments we might store from\n+    // this peer, and we can safely give up syncing if the peer exceeds this\n+    // bound, because it's not possible for a consensus-valid chain to be\n+    // longer than this.\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+// Process the next batch of headers received from our peer.\n+// Validate and store commitments, and compare total chainwork to our target to\n+// see if we can switch to REDOWNLOAD mode.\n+std::optional<CBlockLocator> HeadersSyncState::ProcessNextHeaders(const std::vector<CBlockHeader>&\n+        headers, bool full_headers_message, std::vector<CBlockHeader>&\n+        headers_to_process, bool& processing_success)\n+{\n+    processing_success = false;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return std::nullopt;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return std::nullopt;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+            // Somehow our peer gave us a header that doesn't connect.\n+            // This might be benign -- perhaps we issued an extra getheaders\n+            // message, such as after a block INV was received.\n+            // Or it could be that our peer is broken or malicious. If broken,\n+            // sending a new getheaders immediately could trigger an infinite\n+            // loop. Just give up for now; if our peer ever gives us an block\n+            // INV later we will fetch headers then, and likely retrigger this\n+            // logic.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return std::nullopt;\n+        }\n+        processing_success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return std::nullopt;\n+            }\n+        }\n+\n+        processing_success = true;\n+        // Return any headers that are ready for acceptance.\n+        headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return std::nullopt;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    }\n+    return std::nullopt;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (m_last_header_received.IsNull()) {\n+        m_last_header_received = m_chain_start->GetBlockHeader();\n+        m_current_height = m_chain_start->nHeight;\n+    }\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Doesn't connect to what we expect\n+        Finalize();\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr, m_current_height+1)) {\n+            Finalize();\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_blockhash_with_sufficient_work = headers.back().GetHash();\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash.SetNull();\n+        m_download_state = State::REDOWNLOAD;\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current, int64_t current_height)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, current_height,\n+                previous.nBits, current.nBits)) {\n+        return false;\n+    }\n+\n+    if (!CheckProofOfWork(current.GetHash(), current.nBits, m_consensus_params)) return false;\n+\n+    if ((current_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+        // Try to add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // TODO: disconnect this peer.\n+            LogPrint(BCLog::NET, \"headers chain is too long; giving up sync peer=%d\\n\", m_id);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));",
      "path": "src/headerssync.cpp",
      "position": 208,
      "original_position": 226,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "In commit \"Utilize anti-DoS headers download strategy\"\r\n\r\nJust a thought for later, but it's rather ugly to have to construct a `CBlockIndex` object just to be able to call `GetBlockProof`. I think `GetBlockProof` should work (or have a variant that works) with a `CBlockHeader` too, or even just the nBits value.",
      "created_at": "2022-07-28T14:29:34Z",
      "updated_at": "2022-07-28T14:52:01Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932294356",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932294356"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 208,
      "original_line": 208,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932300194",
      "pull_request_review_id": 1054116591,
      "id": 932300194,
      "node_id": "PRRC_kwDOABII5843kcWi",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    // Overestimate the number of blocks that could possibly exist on this\n+    // chain using 6 blocks/second (fastest blockrate given the MTP rule) times\n+    // the number of seconds from the last allowed block until today. This\n+    // serves as a memory bound on how many commitments we might store from\n+    // this peer, and we can safely give up syncing if the peer exceeds this\n+    // bound, because it's not possible for a consensus-valid chain to be\n+    // longer than this.\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+// Process the next batch of headers received from our peer.\n+// Validate and store commitments, and compare total chainwork to our target to\n+// see if we can switch to REDOWNLOAD mode.\n+std::optional<CBlockLocator> HeadersSyncState::ProcessNextHeaders(const std::vector<CBlockHeader>&\n+        headers, bool full_headers_message, std::vector<CBlockHeader>&\n+        headers_to_process, bool& processing_success)\n+{\n+    processing_success = false;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return std::nullopt;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return std::nullopt;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+            // Somehow our peer gave us a header that doesn't connect.\n+            // This might be benign -- perhaps we issued an extra getheaders\n+            // message, such as after a block INV was received.\n+            // Or it could be that our peer is broken or malicious. If broken,\n+            // sending a new getheaders immediately could trigger an infinite\n+            // loop. Just give up for now; if our peer ever gives us an block\n+            // INV later we will fetch headers then, and likely retrigger this\n+            // logic.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return std::nullopt;\n+        }\n+        processing_success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return std::nullopt;\n+            }\n+        }\n+\n+        processing_success = true;\n+        // Return any headers that are ready for acceptance.\n+        headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return std::nullopt;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    }\n+    return std::nullopt;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (m_last_header_received.IsNull()) {\n+        m_last_header_received = m_chain_start->GetBlockHeader();\n+        m_current_height = m_chain_start->nHeight;\n+    }\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Doesn't connect to what we expect\n+        Finalize();\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr, m_current_height+1)) {\n+            Finalize();\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_blockhash_with_sufficient_work = headers.back().GetHash();\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash.SetNull();\n+        m_download_state = State::REDOWNLOAD;\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current, int64_t current_height)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, current_height,\n+                previous.nBits, current.nBits)) {\n+        return false;\n+    }\n+\n+    if (!CheckProofOfWork(current.GetHash(), current.nBits, m_consensus_params)) return false;\n+\n+    if ((current_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+        // Try to add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // TODO: disconnect this peer.\n+            LogPrint(BCLog::NET, \"headers chain is too long; giving up sync peer=%d\\n\", m_id);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));\n+    m_last_header_received = current;\n+    ++m_current_height;\n+\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreRedownloadedHeader(const CBlockHeader& header)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Ensure that we're working on a header that connects to the chain we're\n+    // downloading.\n+    if (m_redownloaded_headers.empty()) {\n+        if (header.hashPrevBlock != m_chain_start->GetBlockHash()) {",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 241,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "In commit \"Utilize anti-DoS headers download strategy\"\r\n\r\nIs it possible to set this `m_redownload_buffer_last_hash = m_chain_start->GetBlockHash(); m_redownload_buffer_last_height = m_chain_start->nHeight;` in `ValidateAndStoreHeadersCommitments` already when the transition to REDOWNLOAD state is made?",
      "created_at": "2022-07-28T14:33:58Z",
      "updated_at": "2022-07-28T14:52:01Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932300194",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932300194"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 241,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932308150",
      "pull_request_review_id": 1054116591,
      "id": 932308150,
      "node_id": "PRRC_kwDOABII5843keS2",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    // Overestimate the number of blocks that could possibly exist on this\n+    // chain using 6 blocks/second (fastest blockrate given the MTP rule) times\n+    // the number of seconds from the last allowed block until today. This\n+    // serves as a memory bound on how many commitments we might store from\n+    // this peer, and we can safely give up syncing if the peer exceeds this\n+    // bound, because it's not possible for a consensus-valid chain to be\n+    // longer than this.\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+// Process the next batch of headers received from our peer.\n+// Validate and store commitments, and compare total chainwork to our target to\n+// see if we can switch to REDOWNLOAD mode.\n+std::optional<CBlockLocator> HeadersSyncState::ProcessNextHeaders(const std::vector<CBlockHeader>&\n+        headers, bool full_headers_message, std::vector<CBlockHeader>&\n+        headers_to_process, bool& processing_success)\n+{\n+    processing_success = false;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return std::nullopt;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return std::nullopt;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+            // Somehow our peer gave us a header that doesn't connect.\n+            // This might be benign -- perhaps we issued an extra getheaders\n+            // message, such as after a block INV was received.\n+            // Or it could be that our peer is broken or malicious. If broken,\n+            // sending a new getheaders immediately could trigger an infinite\n+            // loop. Just give up for now; if our peer ever gives us an block\n+            // INV later we will fetch headers then, and likely retrigger this\n+            // logic.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return std::nullopt;\n+        }\n+        processing_success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return std::nullopt;\n+            }\n+        }\n+\n+        processing_success = true;\n+        // Return any headers that are ready for acceptance.\n+        headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return std::nullopt;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    }\n+    return std::nullopt;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (m_last_header_received.IsNull()) {\n+        m_last_header_received = m_chain_start->GetBlockHeader();\n+        m_current_height = m_chain_start->nHeight;\n+    }\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Doesn't connect to what we expect\n+        Finalize();\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr, m_current_height+1)) {\n+            Finalize();\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_blockhash_with_sufficient_work = headers.back().GetHash();\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash.SetNull();\n+        m_download_state = State::REDOWNLOAD;\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current, int64_t current_height)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, current_height,\n+                previous.nBits, current.nBits)) {\n+        return false;\n+    }\n+\n+    if (!CheckProofOfWork(current.GetHash(), current.nBits, m_consensus_params)) return false;\n+\n+    if ((current_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+        // Try to add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // TODO: disconnect this peer.\n+            LogPrint(BCLog::NET, \"headers chain is too long; giving up sync peer=%d\\n\", m_id);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));\n+    m_last_header_received = current;\n+    ++m_current_height;\n+\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreRedownloadedHeader(const CBlockHeader& header)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Ensure that we're working on a header that connects to the chain we're\n+    // downloading.\n+    if (m_redownloaded_headers.empty()) {\n+        if (header.hashPrevBlock != m_chain_start->GetBlockHash()) {\n+            return false;\n+        }\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+    } else if (header.hashPrevBlock != m_redownload_buffer_last_hash) {\n+        return false;\n+    }\n+\n+    int64_t next_height = m_redownload_buffer_last_height + 1;\n+\n+    // If we're at a header for which we previously stored a commitment, verify\n+    // it is correct. Failure will result in aborting download.\n+    if ((next_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+         bool commitment = m_hasher(header.GetHash()) & 1;\n+         if (m_header_commitments.size() == 0) {\n+            // Somehow our peer managed to feed us a different chain and\n+            // we've run out of commitments.\n+            return false;\n+        }\n+        bool expected_commitment = m_header_commitments.front();\n+        m_header_commitments.pop_front();\n+        if (commitment != expected_commitment) {\n+            return false;\n+        }\n+    }\n+\n+    // Store this header for later processing.\n+    m_redownloaded_headers.push_back(header);\n+    m_redownload_buffer_last_height = next_height;\n+    m_redownload_buffer_last_hash = header.GetHash();\n+\n+    // If we're processing our target block header, which we verified has",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 272,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "In commit \"Utilize anti-DoS headers download strategy\"\r\n\r\nCould there be a concern that the last or few last headers or so get reorganized during the second phase, resulting in a mismatch at the end? If so, perhaps it's possible to instead keep track of chainwork again in the second phase rather than remembering the exact hash at which the threshold was reached in the first phase?",
      "created_at": "2022-07-28T14:38:05Z",
      "updated_at": "2022-07-28T14:52:01Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932308150",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932308150"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 272,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932322801",
      "pull_request_review_id": 1054116591,
      "id": 932322801,
      "node_id": "PRRC_kwDOABII5843kh3x",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    // Overestimate the number of blocks that could possibly exist on this\n+    // chain using 6 blocks/second (fastest blockrate given the MTP rule) times\n+    // the number of seconds from the last allowed block until today. This\n+    // serves as a memory bound on how many commitments we might store from\n+    // this peer, and we can safely give up syncing if the peer exceeds this\n+    // bound, because it's not possible for a consensus-valid chain to be\n+    // longer than this.\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+// Process the next batch of headers received from our peer.\n+// Validate and store commitments, and compare total chainwork to our target to\n+// see if we can switch to REDOWNLOAD mode.\n+std::optional<CBlockLocator> HeadersSyncState::ProcessNextHeaders(const std::vector<CBlockHeader>&\n+        headers, bool full_headers_message, std::vector<CBlockHeader>&\n+        headers_to_process, bool& processing_success)\n+{\n+    processing_success = false;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return std::nullopt;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return std::nullopt;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+            // Somehow our peer gave us a header that doesn't connect.\n+            // This might be benign -- perhaps we issued an extra getheaders\n+            // message, such as after a block INV was received.\n+            // Or it could be that our peer is broken or malicious. If broken,\n+            // sending a new getheaders immediately could trigger an infinite\n+            // loop. Just give up for now; if our peer ever gives us an block\n+            // INV later we will fetch headers then, and likely retrigger this\n+            // logic.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return std::nullopt;\n+        }\n+        processing_success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return std::nullopt;\n+            }\n+        }\n+\n+        processing_success = true;\n+        // Return any headers that are ready for acceptance.\n+        headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return std::nullopt;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    }\n+    return std::nullopt;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (m_last_header_received.IsNull()) {\n+        m_last_header_received = m_chain_start->GetBlockHeader();\n+        m_current_height = m_chain_start->nHeight;\n+    }\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Doesn't connect to what we expect\n+        Finalize();\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr, m_current_height+1)) {\n+            Finalize();\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_blockhash_with_sufficient_work = headers.back().GetHash();\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash.SetNull();\n+        m_download_state = State::REDOWNLOAD;\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current, int64_t current_height)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, current_height,\n+                previous.nBits, current.nBits)) {\n+        return false;\n+    }\n+\n+    if (!CheckProofOfWork(current.GetHash(), current.nBits, m_consensus_params)) return false;\n+\n+    if ((current_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+        // Try to add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // TODO: disconnect this peer.\n+            LogPrint(BCLog::NET, \"headers chain is too long; giving up sync peer=%d\\n\", m_id);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));\n+    m_last_header_received = current;\n+    ++m_current_height;\n+\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreRedownloadedHeader(const CBlockHeader& header)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Ensure that we're working on a header that connects to the chain we're\n+    // downloading.\n+    if (m_redownloaded_headers.empty()) {\n+        if (header.hashPrevBlock != m_chain_start->GetBlockHash()) {\n+            return false;\n+        }\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+    } else if (header.hashPrevBlock != m_redownload_buffer_last_hash) {\n+        return false;\n+    }\n+\n+    int64_t next_height = m_redownload_buffer_last_height + 1;\n+\n+    // If we're at a header for which we previously stored a commitment, verify\n+    // it is correct. Failure will result in aborting download.\n+    if ((next_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+         bool commitment = m_hasher(header.GetHash()) & 1;\n+         if (m_header_commitments.size() == 0) {\n+            // Somehow our peer managed to feed us a different chain and\n+            // we've run out of commitments.\n+            return false;\n+        }\n+        bool expected_commitment = m_header_commitments.front();\n+        m_header_commitments.pop_front();\n+        if (commitment != expected_commitment) {\n+            return false;\n+        }\n+    }\n+\n+    // Store this header for later processing.\n+    m_redownloaded_headers.push_back(header);\n+    m_redownload_buffer_last_height = next_height;\n+    m_redownload_buffer_last_hash = header.GetHash();\n+\n+    // If we're processing our target block header, which we verified has\n+    // sufficient work, then set a flag for processing all remaining headers.\n+    if (header.GetHash() == m_blockhash_with_sufficient_work) {\n+        m_process_all_remaining_headers = true;\n+    }\n+    return true;\n+}\n+\n+std::vector<CBlockHeader> HeadersSyncState::RemoveHeadersReadyForAcceptance()\n+{\n+    std::vector<CBlockHeader> ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    while (m_redownloaded_headers.size() > REDOWNLOAD_HEADERS_THRESHOLD ||\n+            (m_redownloaded_headers.size() > 0 && m_process_all_remaining_headers)) {\n+        ret.emplace_back(m_redownloaded_headers.front().GetFullHeader(m_redownload_buffer_first_prev_hash));\n+        m_redownloaded_headers.pop_front();\n+        m_redownload_buffer_first_prev_hash = ret.back().GetHash();\n+    }\n+    return ret;\n+}\n+\n+std::optional<CBlockLocator> HeadersSyncState::MakeNextHeadersRequest()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return std::nullopt;\n+\n+    std::vector<uint256> locator;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD && !m_last_header_received.IsNull()) {\n+        // During initial download, we continue from the last header received.\n+        locator.push_back(m_last_header_received.GetHash());\n+    }\n+\n+    if (m_download_state == State::REDOWNLOAD && !m_redownloaded_headers.empty()) {\n+        // During redownload, we will either download from the last received\n+        // header that we stored during the second download phase, or from the\n+        // fork point (m_chain_start).\n+        locator.push_back(m_redownload_buffer_last_hash);\n+    }\n+\n+    locator.insert(locator.end(), m_chain_start_locator.vHave.begin(),\n+            m_chain_start_locator.vHave.end());\n+    return CBlockLocator(locator);",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 317,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "In commit \"Utilize anti-DoS headers download strategy\"\r\n\r\n`return CBlockLocator(std::move(locator))` saves a copy.",
      "created_at": "2022-07-28T14:46:36Z",
      "updated_at": "2022-07-28T14:52:01Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932322801",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932322801"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 317,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932330073",
      "pull_request_review_id": 1054116591,
      "id": 932330073,
      "node_id": "PRRC_kwDOABII5843kjpZ",
      "diff_hunk": "@@ -0,0 +1,246 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <chain.h>\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <arith_uint256.h>\n+#include <net.h> // For NodeId\n+#include <consensus/params.h>\n+#include <util/hasher.h>\n+#include <util/bitdeque.h>\n+\n+#include <vector>\n+#include <deque>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, we can calculate the work on the chain as we\n+ * go (just by checking the nBits value on each header, and validating the\n+ * proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1, store 1 bit (using a salted hash function) for every N headers\n+ * that we see. With a reasonable choice of N, this uses relatively little\n+ * memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer of size H, and only\n+ * accept a batch of N (N < H) headers to memory once we've verified that H/N =\n+ * S commitments have all passed verification. With this parametrization, we\n+ * can achieve a given security target (S) while choosing H and N to minimize\n+ * memory usage in this scheme.\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params);\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** INITIAL_DOWNLOAD means the peer has not yet demonstrated their\n+         * chain has sufficient work */\n+        INITIAL_DOWNLOAD,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Start headers sync (via this download-twice mechanism)\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * initial_headers: first batch of headers to process\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    std::optional<CBlockLocator> StartInitialDownload(const CBlockIndex* chain_start, const\n+            std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+            minimum_required_work, CBlockLocator&& chain_start_locator);\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * headers: headers that were received over the network for processing\n+     * full_headers_message: true if the message was at max capacity,\n+     *                       indicating more headers may be available\n+     * headers_to_process: will be filled in with any headers that the caller\n+     *                     can process and validate now (because these returned\n+     *                     headers are on a chain with sufficient work)\n+     * processing_success: set to false if an error is detected and the sync is\n+     *                     aborted; true otherwise.\n+     */\n+    std::optional<CBlockLocator> ProcessNextHeaders(const std::vector<CBlockHeader>& headers,",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 140,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "In commit \"Utilize anti-DoS headers download strategy\"\r\n\r\nThis function really returns 3 things: an optional block locator, a vector of headers to process, and a bool processing_success. Having those spread over return values and mutable arguments is a bit ugly.\r\n\r\nHow about returning a typedef'd `std::optional<std::pair<std::optional<CBlockLocator>, std::vector<CBlockHeader>>>`, or making a simple custom struct to return the results in?",
      "created_at": "2022-07-28T14:50:56Z",
      "updated_at": "2022-07-28T14:52:01Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932330073",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932330073"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 140,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932560137",
      "pull_request_review_id": 1054584671,
      "id": 932560137,
      "node_id": "PRRC_kwDOABII5843lb0J",
      "diff_hunk": "@@ -44,7 +48,12 @@ BOOST_AUTO_TEST_CASE(get_next_work_lower_limit_actual)\n     pindexLast.nHeight = 68543;\n     pindexLast.nTime = 1279297671;  // Block #68543\n     pindexLast.nBits = 0x1c05a3f4;\n-    BOOST_CHECK_EQUAL(CalculateNextWorkRequired(&pindexLast, nLastRetargetTime, chainParams->GetConsensus()), 0x1c0168fdU);\n+    unsigned int expected_nbits = 0x1c0168fdU;",
      "path": "src/test/pow_tests.cpp",
      "position": 32,
      "original_position": 27,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cbf3069c7d76888a647d68dbb09d77e666dcaf50",
      "in_reply_to_id": null,
      "user": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "can this relation between `pindexLast.nBits` and `expected_nbits` be explicitly computed? Would make the test condition below abundantly clear.",
      "created_at": "2022-07-28T18:38:44Z",
      "updated_at": "2022-07-28T18:50:21Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932560137",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932560137"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 56,
      "original_line": 56,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932562089",
      "pull_request_review_id": 1054584671,
      "id": 932562089,
      "node_id": "PRRC_kwDOABII5843lcSp",
      "diff_hunk": "@@ -32,7 +34,9 @@ BOOST_AUTO_TEST_CASE(get_next_work_pow_limit)\n     pindexLast.nHeight = 2015;\n     pindexLast.nTime = 1233061996;  // Block #2015\n     pindexLast.nBits = 0x1d00ffff;\n-    BOOST_CHECK_EQUAL(CalculateNextWorkRequired(&pindexLast, nLastRetargetTime, chainParams->GetConsensus()), 0x1d00ffffU);\n+    unsigned int expected_nbits = 0x1d00ffffU;",
      "path": "src/test/pow_tests.cpp",
      "position": 21,
      "original_position": 16,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cbf3069c7d76888a647d68dbb09d77e666dcaf50",
      "in_reply_to_id": null,
      "user": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "can this just be the (casted) value of pindexLast.nBits?",
      "created_at": "2022-07-28T18:41:12Z",
      "updated_at": "2022-07-28T18:50:21Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932562089",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932562089"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 42,
      "original_line": 42,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932562835",
      "pull_request_review_id": 1054584671,
      "id": 932562835,
      "node_id": "PRRC_kwDOABII5843lceT",
      "diff_hunk": "@@ -56,7 +65,12 @@ BOOST_AUTO_TEST_CASE(get_next_work_upper_limit_actual)\n     pindexLast.nHeight = 46367;\n     pindexLast.nTime = 1269211443;  // Block #46367\n     pindexLast.nBits = 0x1c387f6f;\n-    BOOST_CHECK_EQUAL(CalculateNextWorkRequired(&pindexLast, nLastRetargetTime, chainParams->GetConsensus()), 0x1d00e1fdU);\n+    unsigned int expected_nbits = 0x1d00e1fdU;",
      "path": "src/test/pow_tests.cpp",
      "position": 46,
      "original_position": 41,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cbf3069c7d76888a647d68dbb09d77e666dcaf50",
      "in_reply_to_id": null,
      "user": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "can this relation between `pindexLast.nBits` and `expected_nbits` be explicitly computed? Would make the test condition below abundantly clear.",
      "created_at": "2022-07-28T18:42:06Z",
      "updated_at": "2022-07-28T18:50:21Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932562835",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932562835"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 73,
      "original_line": 73,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932564292",
      "pull_request_review_id": 1054584671,
      "id": 932564292,
      "node_id": "PRRC_kwDOABII5843lc1E",
      "diff_hunk": "@@ -71,6 +71,54 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n     return bnNew.GetCompact();\n }\n \n+// Check that on difficulty adjustments, the new difficulty does not increase\n+// or decrease beyond the permitted limits.\n+bool PermittedDifficultyTransition(const Consensus::Params& params, int64_t height, uint32_t old_nbits, uint32_t new_nbits)\n+{\n+    if (params.fPowAllowMinDifficultyBlocks) return true;\n+\n+    if (height % params.DifficultyAdjustmentInterval() == 0) {\n+        int64_t smallest_timespan = params.nPowTargetTimespan/4;\n+        int64_t largest_timespan = params.nPowTargetTimespan*4;\n+\n+        const arith_uint256 bnPowLimit = UintToArith256(params.powLimit);",
      "path": "src/pow.cpp",
      "position": null,
      "original_position": 14,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cbf3069c7d76888a647d68dbb09d77e666dcaf50",
      "in_reply_to_id": null,
      "user": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "while we're here, what does `bn` actually refer to? :grimacing: \r\n\r\nwouldn't mind tossing the old naming schemes for this",
      "created_at": "2022-07-28T18:44:03Z",
      "updated_at": "2022-07-28T18:50:21Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932564292",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932564292"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 84,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932566102",
      "pull_request_review_id": 1054584671,
      "id": 932566102,
      "node_id": "PRRC_kwDOABII5843ldRW",
      "diff_hunk": "@@ -71,6 +71,54 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n     return bnNew.GetCompact();\n }\n \n+// Check that on difficulty adjustments, the new difficulty does not increase\n+// or decrease beyond the permitted limits.\n+bool PermittedDifficultyTransition(const Consensus::Params& params, int64_t height, uint32_t old_nbits, uint32_t new_nbits)\n+{\n+    if (params.fPowAllowMinDifficultyBlocks) return true;",
      "path": "src/pow.cpp",
      "position": 8,
      "original_position": 8,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cbf3069c7d76888a647d68dbb09d77e666dcaf50",
      "in_reply_to_id": null,
      "user": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "splitting hairs maybe but should this also check new_bits is at/above powLimit?",
      "created_at": "2022-07-28T18:46:19Z",
      "updated_at": "2022-07-28T18:50:21Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932566102",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932566102"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 78,
      "original_line": 78,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932604102",
      "pull_request_review_id": 1054648121,
      "id": 932604102,
      "node_id": "PRRC_kwDOABII5843lmjG",
      "diff_hunk": "@@ -71,6 +71,54 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n     return bnNew.GetCompact();\n }\n \n+// Check that on difficulty adjustments, the new difficulty does not increase\n+// or decrease beyond the permitted limits.\n+bool PermittedDifficultyTransition(const Consensus::Params& params, int64_t height, uint32_t old_nbits, uint32_t new_nbits)\n+{\n+    if (params.fPowAllowMinDifficultyBlocks) return true;",
      "path": "src/pow.cpp",
      "position": 8,
      "original_position": 8,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cbf3069c7d76888a647d68dbb09d77e666dcaf50",
      "in_reply_to_id": 932566102,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "For the purposes of what is needed for the security analysis of the overall header commitment scheme, the only requirements are that this verifies (a) that the difficulty doesn't change on non-2016-multiple blocks and (b) doesn't go up or down too quickly. Its goal is forcing the attacker to spread out their attempted PoW over many blocks, rather than just one or a few (because creating `N` blocks with each difficulty `D` is much harder than creating one block with difficulty `N*D`, if the hashpower available to the attacker is less than the expected value for an `N*D` difficulty block).\r\n\r\nNo objection to checking whatever can be checked with the provided arguments, but I think the current code just chooses to check exactly what is needed, and document it, rather than verify everything possible.",
      "created_at": "2022-07-28T19:37:05Z",
      "updated_at": "2022-07-28T19:39:47Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932604102",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932604102"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 78,
      "original_line": 78,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932655838",
      "pull_request_review_id": 1054730452,
      "id": 932655838,
      "node_id": "PRRC_kwDOABII5843lzLe",
      "diff_hunk": "@@ -0,0 +1,68 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+import time\n+\n+NODE1_BLOCKS_REQUIRED = 16\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 16 blocks; node2 requires 2048 blocks\n+        self.extra_args = [[\"-minimumchainwork=0x0\"], [\"-minimumchainwork=0x1f\"], [\"-minimumchainwork=0x1000\"]]\n+\n+    def skip_test_if_missing_module(self):\n+        self.skip_if_no_wallet()\n+\n+    def test_chains_sync_when_long_enough(self):\n+        self.log.info(\"Generate blocks on the node with no required chainwork\")\n+        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED-2, sync_fun=self.no_op)\n+\n+        self.log.info(\"Verify nodes 1 and 2 have no new headers in their headers tree\")\n+        time.sleep(2)",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 30,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": 932281150,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Hm, will `assert_debug_log` work properly here if there might be multiple \"[net] Ignoring low-work chain\" lines in our debug.log?  (Perhaps that would work if I generated one block at a time in a loop, rather than invoke `generate` once with multiple blocks?)",
      "created_at": "2022-07-28T20:46:52Z",
      "updated_at": "2022-07-28T20:46:52Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932655838",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932655838"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 27,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 30,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932715853",
      "pull_request_review_id": 1054817686,
      "id": 932715853,
      "node_id": "PRRC_kwDOABII5843mB1N",
      "diff_hunk": "@@ -0,0 +1,246 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <chain.h>\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <arith_uint256.h>\n+#include <net.h> // For NodeId\n+#include <consensus/params.h>\n+#include <util/hasher.h>\n+#include <util/bitdeque.h>\n+\n+#include <vector>\n+#include <deque>",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 18,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": 932182732,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done",
      "created_at": "2022-07-28T22:05:26Z",
      "updated_at": "2022-07-28T22:05:26Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932715853",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932715853"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 8,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 17,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932715978",
      "pull_request_review_id": 1054817858,
      "id": 932715978,
      "node_id": "PRRC_kwDOABII5843mB3K",
      "diff_hunk": "@@ -0,0 +1,68 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+import time\n+\n+NODE1_BLOCKS_REQUIRED = 16\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 16 blocks; node2 requires 2048 blocks\n+        self.extra_args = [[\"-minimumchainwork=0x0\"], [\"-minimumchainwork=0x1f\"], [\"-minimumchainwork=0x1000\"]]\n+\n+    def skip_test_if_missing_module(self):\n+        self.skip_if_no_wallet()\n+\n+    def test_chains_sync_when_long_enough(self):\n+        self.log.info(\"Generate blocks on the node with no required chainwork\")\n+        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED-2, sync_fun=self.no_op)\n+\n+        self.log.info(\"Verify nodes 1 and 2 have no new headers in their headers tree\")\n+        time.sleep(2)\n+        for i in range(1, 2):\n+            chaintips = self.nodes[i].getchaintips()\n+            assert(len(chaintips) == 1)\n+            assert {\n+                'height': 0,\n+                'hash': '0f9188f13cb7b2c71f2a335e3a4fc328bf5beb436012afca590b1a11466e2206',\n+                'branchlen': 0,\n+                'status': 'active',\n+            } in chaintips",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 39,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": 932277428,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Thanks! Fixed.",
      "created_at": "2022-07-28T22:05:41Z",
      "updated_at": "2022-07-28T22:05:41Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932715978",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932715978"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 31,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 61,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932716716",
      "pull_request_review_id": 1054818809,
      "id": 932716716,
      "node_id": "PRRC_kwDOABII5843mCCs",
      "diff_hunk": "@@ -0,0 +1,68 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+import time\n+\n+NODE1_BLOCKS_REQUIRED = 16\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 16 blocks; node2 requires 2048 blocks",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 19,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": 932283353,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Oops, looks like I was off by one a couple of times when I worked on this, and didn't square this away.  Fixed now.",
      "created_at": "2022-07-28T22:07:06Z",
      "updated_at": "2022-07-28T22:07:06Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932716716",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932716716"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 19,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932716792",
      "pull_request_review_id": 1054818916,
      "id": 932716792,
      "node_id": "PRRC_kwDOABII5843mCD4",
      "diff_hunk": "@@ -0,0 +1,68 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+import time\n+\n+NODE1_BLOCKS_REQUIRED = 16\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 16 blocks; node2 requires 2048 blocks\n+        self.extra_args = [[\"-minimumchainwork=0x0\"], [\"-minimumchainwork=0x1f\"], [\"-minimumchainwork=0x1000\"]]\n+\n+    def skip_test_if_missing_module(self):\n+        self.skip_if_no_wallet()\n+\n+    def test_chains_sync_when_long_enough(self):\n+        self.log.info(\"Generate blocks on the node with no required chainwork\")\n+        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED-2, sync_fun=self.no_op)\n+\n+        self.log.info(\"Verify nodes 1 and 2 have no new headers in their headers tree\")\n+        time.sleep(2)\n+        for i in range(1, 2):\n+            chaintips = self.nodes[i].getchaintips()\n+            assert(len(chaintips) == 1)\n+            assert {\n+                'height': 0,\n+                'hash': '0f9188f13cb7b2c71f2a335e3a4fc328bf5beb436012afca590b1a11466e2206',\n+                'branchlen': 0,\n+                'status': 'active',\n+            } in chaintips\n+\n+        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED - self.nodes[0].getblockcount(), sync_fun=self.no_op)\n+        self.log.info(\"Verify that node1 syncs node0's chain\")\n+        time.sleep(2)",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 43,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": 932286853,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed",
      "created_at": "2022-07-28T22:07:15Z",
      "updated_at": "2022-07-28T22:07:15Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932716792",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932716792"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 43,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932717300",
      "pull_request_review_id": 1054819633,
      "id": 932717300,
      "node_id": "PRRC_kwDOABII5843mCL0",
      "diff_hunk": "@@ -71,6 +71,54 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n     return bnNew.GetCompact();\n }\n \n+// Check that on difficulty adjustments, the new difficulty does not increase\n+// or decrease beyond the permitted limits.\n+bool PermittedDifficultyTransition(const Consensus::Params& params, int64_t height, uint32_t old_nbits, uint32_t new_nbits)\n+{\n+    if (params.fPowAllowMinDifficultyBlocks) return true;\n+\n+    if (height % params.DifficultyAdjustmentInterval() == 0) {\n+        int64_t smallest_timespan = params.nPowTargetTimespan/4;\n+        int64_t largest_timespan = params.nPowTargetTimespan*4;\n+\n+        const arith_uint256 bnPowLimit = UintToArith256(params.powLimit);\n+        arith_uint256 observed_new_target;\n+        observed_new_target.SetCompact(new_nbits);\n+\n+        // Calculate the largest difficulty value possible:\n+        arith_uint256 bnNew;\n+        bnNew.SetCompact(old_nbits);\n+        bnNew *= largest_timespan;\n+        bnNew /= params.nPowTargetTimespan;\n+\n+        if (bnNew > bnPowLimit)\n+            bnNew = bnPowLimit;",
      "path": "src/pow.cpp",
      "position": null,
      "original_position": 25,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cbf3069c7d76888a647d68dbb09d77e666dcaf50",
      "in_reply_to_id": 932235084,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed the braces. Should I also fix all the variable names? (Saw @instagibbs' comment along those lines just now, so if you agree then I'll modernize the whole thing.)",
      "created_at": "2022-07-28T22:08:21Z",
      "updated_at": "2022-07-28T22:08:22Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932717300",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932717300"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 95,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932717391",
      "pull_request_review_id": 1054819771,
      "id": 932717391,
      "node_id": "PRRC_kwDOABII5843mCNP",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 7,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": 932238513,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think I fixed all these now.",
      "created_at": "2022-07-28T22:08:33Z",
      "updated_at": "2022-07-28T22:08:34Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932717391",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932717391"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 7,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932717474",
      "pull_request_review_id": 1054819844,
      "id": 932717474,
      "node_id": "PRRC_kwDOABII5843mCOi",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()",
      "path": "src/headerssync.cpp",
      "position": 51,
      "original_position": 24,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": 932252053,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-07-28T22:08:41Z",
      "updated_at": "2022-07-28T22:08:41Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932717474",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932717474"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 51,
      "original_line": 51,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932717538",
      "pull_request_review_id": 1054819939,
      "id": 932717538,
      "node_id": "PRRC_kwDOABII5843mCPi",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 23,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": 932256694,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-07-28T22:08:46Z",
      "updated_at": "2022-07-28T22:08:47Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932717538",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932717538"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 23,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932717675",
      "pull_request_review_id": 1054820110,
      "id": 932717675,
      "node_id": "PRRC_kwDOABII5843mCRr",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    // Overestimate the number of blocks that could possibly exist on this\n+    // chain using 6 blocks/second (fastest blockrate given the MTP rule) times\n+    // the number of seconds from the last allowed block until today. This\n+    // serves as a memory bound on how many commitments we might store from\n+    // this peer, and we can safely give up syncing if the peer exceeds this\n+    // bound, because it's not possible for a consensus-valid chain to be\n+    // longer than this.",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 61,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": 932259718,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Reworked the comment to try to make that clearer, let me know if it looks better now...",
      "created_at": "2022-07-28T22:09:04Z",
      "updated_at": "2022-07-28T22:09:04Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932717675",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932717675"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 61,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932717729",
      "pull_request_review_id": 1054820184,
      "id": 932717729,
      "node_id": "PRRC_kwDOABII5843mCSh",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    // Overestimate the number of blocks that could possibly exist on this\n+    // chain using 6 blocks/second (fastest blockrate given the MTP rule) times\n+    // the number of seconds from the last allowed block until today. This\n+    // serves as a memory bound on how many commitments we might store from\n+    // this peer, and we can safely give up syncing if the peer exceeds this\n+    // bound, because it's not possible for a consensus-valid chain to be\n+    // longer than this.\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+// Process the next batch of headers received from our peer.\n+// Validate and store commitments, and compare total chainwork to our target to\n+// see if we can switch to REDOWNLOAD mode.\n+std::optional<CBlockLocator> HeadersSyncState::ProcessNextHeaders(const std::vector<CBlockHeader>&\n+        headers, bool full_headers_message, std::vector<CBlockHeader>&\n+        headers_to_process, bool& processing_success)\n+{\n+    processing_success = false;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return std::nullopt;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return std::nullopt;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+            // Somehow our peer gave us a header that doesn't connect.\n+            // This might be benign -- perhaps we issued an extra getheaders\n+            // message, such as after a block INV was received.\n+            // Or it could be that our peer is broken or malicious. If broken,\n+            // sending a new getheaders immediately could trigger an infinite\n+            // loop. Just give up for now; if our peer ever gives us an block\n+            // INV later we will fetch headers then, and likely retrigger this\n+            // logic.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return std::nullopt;\n+        }\n+        processing_success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return std::nullopt;\n+            }\n+        }\n+\n+        processing_success = true;\n+        // Return any headers that are ready for acceptance.\n+        headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return std::nullopt;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    }\n+    return std::nullopt;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (m_last_header_received.IsNull()) {",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 168,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": 932271718,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-07-28T22:09:09Z",
      "updated_at": "2022-07-28T22:09:10Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932717729",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932717729"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 168,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932717972",
      "pull_request_review_id": 1054820479,
      "id": 932717972,
      "node_id": "PRRC_kwDOABII5843mCWU",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 47,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": 932272722,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Seems reasonable, I added a variable to do that.  Let me know if that looks good now.",
      "created_at": "2022-07-28T22:09:38Z",
      "updated_at": "2022-07-28T22:09:38Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932717972",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932717972"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 50,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932718040",
      "pull_request_review_id": 1054820569,
      "id": 932718040,
      "node_id": "PRRC_kwDOABII5843mCXY",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    // Overestimate the number of blocks that could possibly exist on this\n+    // chain using 6 blocks/second (fastest blockrate given the MTP rule) times\n+    // the number of seconds from the last allowed block until today. This\n+    // serves as a memory bound on how many commitments we might store from\n+    // this peer, and we can safely give up syncing if the peer exceeds this\n+    // bound, because it's not possible for a consensus-valid chain to be\n+    // longer than this.\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+// Process the next batch of headers received from our peer.\n+// Validate and store commitments, and compare total chainwork to our target to\n+// see if we can switch to REDOWNLOAD mode.\n+std::optional<CBlockLocator> HeadersSyncState::ProcessNextHeaders(const std::vector<CBlockHeader>&\n+        headers, bool full_headers_message, std::vector<CBlockHeader>&\n+        headers_to_process, bool& processing_success)\n+{\n+    processing_success = false;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return std::nullopt;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return std::nullopt;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+            // Somehow our peer gave us a header that doesn't connect.\n+            // This might be benign -- perhaps we issued an extra getheaders\n+            // message, such as after a block INV was received.\n+            // Or it could be that our peer is broken or malicious. If broken,\n+            // sending a new getheaders immediately could trigger an infinite\n+            // loop. Just give up for now; if our peer ever gives us an block\n+            // INV later we will fetch headers then, and likely retrigger this\n+            // logic.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return std::nullopt;\n+        }\n+        processing_success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return std::nullopt;\n+            }\n+        }\n+\n+        processing_success = true;\n+        // Return any headers that are ready for acceptance.\n+        headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return std::nullopt;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    }\n+    return std::nullopt;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (m_last_header_received.IsNull()) {\n+        m_last_header_received = m_chain_start->GetBlockHeader();\n+        m_current_height = m_chain_start->nHeight;\n+    }\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Doesn't connect to what we expect\n+        Finalize();\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 179,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": 932278011,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-07-28T22:09:45Z",
      "updated_at": "2022-07-28T22:09:45Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932718040",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932718040"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 158,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932718129",
      "pull_request_review_id": 1054820676,
      "id": 932718129,
      "node_id": "PRRC_kwDOABII5843mCYx",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    // Overestimate the number of blocks that could possibly exist on this\n+    // chain using 6 blocks/second (fastest blockrate given the MTP rule) times\n+    // the number of seconds from the last allowed block until today. This\n+    // serves as a memory bound on how many commitments we might store from\n+    // this peer, and we can safely give up syncing if the peer exceeds this\n+    // bound, because it's not possible for a consensus-valid chain to be\n+    // longer than this.\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+// Process the next batch of headers received from our peer.\n+// Validate and store commitments, and compare total chainwork to our target to\n+// see if we can switch to REDOWNLOAD mode.\n+std::optional<CBlockLocator> HeadersSyncState::ProcessNextHeaders(const std::vector<CBlockHeader>&\n+        headers, bool full_headers_message, std::vector<CBlockHeader>&\n+        headers_to_process, bool& processing_success)\n+{\n+    processing_success = false;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return std::nullopt;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return std::nullopt;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+            // Somehow our peer gave us a header that doesn't connect.\n+            // This might be benign -- perhaps we issued an extra getheaders\n+            // message, such as after a block INV was received.\n+            // Or it could be that our peer is broken or malicious. If broken,\n+            // sending a new getheaders immediately could trigger an infinite\n+            // loop. Just give up for now; if our peer ever gives us an block\n+            // INV later we will fetch headers then, and likely retrigger this\n+            // logic.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return std::nullopt;\n+        }\n+        processing_success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return std::nullopt;\n+            }\n+        }\n+\n+        processing_success = true;\n+        // Return any headers that are ready for acceptance.\n+        headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return std::nullopt;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    }\n+    return std::nullopt;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (m_last_header_received.IsNull()) {\n+        m_last_header_received = m_chain_start->GetBlockHeader();\n+        m_current_height = m_chain_start->nHeight;\n+    }\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Doesn't connect to what we expect\n+        Finalize();\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr, m_current_height+1)) {\n+            Finalize();\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_blockhash_with_sufficient_work = headers.back().GetHash();\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash.SetNull();\n+        m_download_state = State::REDOWNLOAD;\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current, int64_t current_height)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, current_height,\n+                previous.nBits, current.nBits)) {\n+        return false;\n+    }\n+\n+    if (!CheckProofOfWork(current.GetHash(), current.nBits, m_consensus_params)) return false;\n+\n+    if ((current_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+        // Try to add a commitment.",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 216,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": 932281858,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed.",
      "created_at": "2022-07-28T22:09:54Z",
      "updated_at": "2022-07-28T22:09:54Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932718129",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932718129"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 216,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932718308",
      "pull_request_review_id": 1054820916,
      "id": 932718308,
      "node_id": "PRRC_kwDOABII5843mCbk",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    // Overestimate the number of blocks that could possibly exist on this\n+    // chain using 6 blocks/second (fastest blockrate given the MTP rule) times\n+    // the number of seconds from the last allowed block until today. This\n+    // serves as a memory bound on how many commitments we might store from\n+    // this peer, and we can safely give up syncing if the peer exceeds this\n+    // bound, because it's not possible for a consensus-valid chain to be\n+    // longer than this.\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+// Process the next batch of headers received from our peer.\n+// Validate and store commitments, and compare total chainwork to our target to\n+// see if we can switch to REDOWNLOAD mode.\n+std::optional<CBlockLocator> HeadersSyncState::ProcessNextHeaders(const std::vector<CBlockHeader>&\n+        headers, bool full_headers_message, std::vector<CBlockHeader>&\n+        headers_to_process, bool& processing_success)\n+{\n+    processing_success = false;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return std::nullopt;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return std::nullopt;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+            // Somehow our peer gave us a header that doesn't connect.\n+            // This might be benign -- perhaps we issued an extra getheaders\n+            // message, such as after a block INV was received.\n+            // Or it could be that our peer is broken or malicious. If broken,\n+            // sending a new getheaders immediately could trigger an infinite\n+            // loop. Just give up for now; if our peer ever gives us an block\n+            // INV later we will fetch headers then, and likely retrigger this\n+            // logic.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return std::nullopt;\n+        }\n+        processing_success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return std::nullopt;\n+            }\n+        }\n+\n+        processing_success = true;\n+        // Return any headers that are ready for acceptance.\n+        headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return std::nullopt;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    }\n+    return std::nullopt;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (m_last_header_received.IsNull()) {\n+        m_last_header_received = m_chain_start->GetBlockHeader();\n+        m_current_height = m_chain_start->nHeight;\n+    }\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Doesn't connect to what we expect\n+        Finalize();\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr, m_current_height+1)) {\n+            Finalize();\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_blockhash_with_sufficient_work = headers.back().GetHash();\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash.SetNull();\n+        m_download_state = State::REDOWNLOAD;\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current, int64_t current_height)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, current_height,\n+                previous.nBits, current.nBits)) {\n+        return false;\n+    }\n+\n+    if (!CheckProofOfWork(current.GetHash(), current.nBits, m_consensus_params)) return false;\n+\n+    if ((current_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+        // Try to add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // TODO: disconnect this peer.\n+            LogPrint(BCLog::NET, \"headers chain is too long; giving up sync peer=%d\\n\", m_id);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));\n+    m_last_header_received = current;\n+    ++m_current_height;\n+\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreRedownloadedHeader(const CBlockHeader& header)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Ensure that we're working on a header that connects to the chain we're\n+    // downloading.\n+    if (m_redownloaded_headers.empty()) {\n+        if (header.hashPrevBlock != m_chain_start->GetBlockHash()) {",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 241,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": 932300194,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Yeah that's much better, thanks. Done.",
      "created_at": "2022-07-28T22:10:16Z",
      "updated_at": "2022-07-28T22:10:16Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932718308",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932718308"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 241,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932718382",
      "pull_request_review_id": 1054821015,
      "id": 932718382,
      "node_id": "PRRC_kwDOABII5843mCcu",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    // Overestimate the number of blocks that could possibly exist on this\n+    // chain using 6 blocks/second (fastest blockrate given the MTP rule) times\n+    // the number of seconds from the last allowed block until today. This\n+    // serves as a memory bound on how many commitments we might store from\n+    // this peer, and we can safely give up syncing if the peer exceeds this\n+    // bound, because it's not possible for a consensus-valid chain to be\n+    // longer than this.\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+// Process the next batch of headers received from our peer.\n+// Validate and store commitments, and compare total chainwork to our target to\n+// see if we can switch to REDOWNLOAD mode.\n+std::optional<CBlockLocator> HeadersSyncState::ProcessNextHeaders(const std::vector<CBlockHeader>&\n+        headers, bool full_headers_message, std::vector<CBlockHeader>&\n+        headers_to_process, bool& processing_success)\n+{\n+    processing_success = false;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return std::nullopt;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return std::nullopt;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+            // Somehow our peer gave us a header that doesn't connect.\n+            // This might be benign -- perhaps we issued an extra getheaders\n+            // message, such as after a block INV was received.\n+            // Or it could be that our peer is broken or malicious. If broken,\n+            // sending a new getheaders immediately could trigger an infinite\n+            // loop. Just give up for now; if our peer ever gives us an block\n+            // INV later we will fetch headers then, and likely retrigger this\n+            // logic.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return std::nullopt;\n+        }\n+        processing_success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return std::nullopt;\n+            }\n+        }\n+\n+        processing_success = true;\n+        // Return any headers that are ready for acceptance.\n+        headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return std::nullopt;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    }\n+    return std::nullopt;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (m_last_header_received.IsNull()) {\n+        m_last_header_received = m_chain_start->GetBlockHeader();\n+        m_current_height = m_chain_start->nHeight;\n+    }\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Doesn't connect to what we expect\n+        Finalize();\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr, m_current_height+1)) {\n+            Finalize();\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_blockhash_with_sufficient_work = headers.back().GetHash();\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash.SetNull();\n+        m_download_state = State::REDOWNLOAD;\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current, int64_t current_height)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, current_height,\n+                previous.nBits, current.nBits)) {\n+        return false;\n+    }\n+\n+    if (!CheckProofOfWork(current.GetHash(), current.nBits, m_consensus_params)) return false;\n+\n+    if ((current_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+        // Try to add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // TODO: disconnect this peer.\n+            LogPrint(BCLog::NET, \"headers chain is too long; giving up sync peer=%d\\n\", m_id);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));\n+    m_last_header_received = current;\n+    ++m_current_height;\n+\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreRedownloadedHeader(const CBlockHeader& header)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Ensure that we're working on a header that connects to the chain we're\n+    // downloading.\n+    if (m_redownloaded_headers.empty()) {\n+        if (header.hashPrevBlock != m_chain_start->GetBlockHash()) {\n+            return false;\n+        }\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+    } else if (header.hashPrevBlock != m_redownload_buffer_last_hash) {\n+        return false;\n+    }\n+\n+    int64_t next_height = m_redownload_buffer_last_height + 1;\n+\n+    // If we're at a header for which we previously stored a commitment, verify\n+    // it is correct. Failure will result in aborting download.\n+    if ((next_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+         bool commitment = m_hasher(header.GetHash()) & 1;\n+         if (m_header_commitments.size() == 0) {\n+            // Somehow our peer managed to feed us a different chain and\n+            // we've run out of commitments.\n+            return false;\n+        }\n+        bool expected_commitment = m_header_commitments.front();\n+        m_header_commitments.pop_front();\n+        if (commitment != expected_commitment) {\n+            return false;\n+        }\n+    }\n+\n+    // Store this header for later processing.\n+    m_redownloaded_headers.push_back(header);\n+    m_redownload_buffer_last_height = next_height;\n+    m_redownload_buffer_last_hash = header.GetHash();\n+\n+    // If we're processing our target block header, which we verified has\n+    // sufficient work, then set a flag for processing all remaining headers.\n+    if (header.GetHash() == m_blockhash_with_sufficient_work) {\n+        m_process_all_remaining_headers = true;\n+    }\n+    return true;\n+}\n+\n+std::vector<CBlockHeader> HeadersSyncState::RemoveHeadersReadyForAcceptance()\n+{\n+    std::vector<CBlockHeader> ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    while (m_redownloaded_headers.size() > REDOWNLOAD_HEADERS_THRESHOLD ||\n+            (m_redownloaded_headers.size() > 0 && m_process_all_remaining_headers)) {\n+        ret.emplace_back(m_redownloaded_headers.front().GetFullHeader(m_redownload_buffer_first_prev_hash));\n+        m_redownloaded_headers.pop_front();\n+        m_redownload_buffer_first_prev_hash = ret.back().GetHash();\n+    }\n+    return ret;\n+}\n+\n+std::optional<CBlockLocator> HeadersSyncState::MakeNextHeadersRequest()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return std::nullopt;\n+\n+    std::vector<uint256> locator;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD && !m_last_header_received.IsNull()) {\n+        // During initial download, we continue from the last header received.\n+        locator.push_back(m_last_header_received.GetHash());\n+    }\n+\n+    if (m_download_state == State::REDOWNLOAD && !m_redownloaded_headers.empty()) {\n+        // During redownload, we will either download from the last received\n+        // header that we stored during the second download phase, or from the\n+        // fork point (m_chain_start).\n+        locator.push_back(m_redownload_buffer_last_hash);\n+    }\n+\n+    locator.insert(locator.end(), m_chain_start_locator.vHave.begin(),\n+            m_chain_start_locator.vHave.end());\n+    return CBlockLocator(locator);",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 317,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": 932322801,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-07-28T22:10:24Z",
      "updated_at": "2022-07-28T22:10:25Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932718382",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932718382"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 317,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932718671",
      "pull_request_review_id": 1054821402,
      "id": 932718671,
      "node_id": "PRRC_kwDOABII5843mChP",
      "diff_hunk": "@@ -0,0 +1,246 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <chain.h>\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <arith_uint256.h>\n+#include <net.h> // For NodeId\n+#include <consensus/params.h>\n+#include <util/hasher.h>\n+#include <util/bitdeque.h>\n+\n+#include <vector>\n+#include <deque>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, we can calculate the work on the chain as we\n+ * go (just by checking the nBits value on each header, and validating the\n+ * proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1, store 1 bit (using a salted hash function) for every N headers\n+ * that we see. With a reasonable choice of N, this uses relatively little\n+ * memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer of size H, and only\n+ * accept a batch of N (N < H) headers to memory once we've verified that H/N =\n+ * S commitments have all passed verification. With this parametrization, we\n+ * can achieve a given security target (S) while choosing H and N to minimize\n+ * memory usage in this scheme.\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params);\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** INITIAL_DOWNLOAD means the peer has not yet demonstrated their\n+         * chain has sufficient work */\n+        INITIAL_DOWNLOAD,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Start headers sync (via this download-twice mechanism)\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * initial_headers: first batch of headers to process\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    std::optional<CBlockLocator> StartInitialDownload(const CBlockIndex* chain_start, const\n+            std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+            minimum_required_work, CBlockLocator&& chain_start_locator);\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * headers: headers that were received over the network for processing\n+     * full_headers_message: true if the message was at max capacity,\n+     *                       indicating more headers may be available\n+     * headers_to_process: will be filled in with any headers that the caller\n+     *                     can process and validate now (because these returned\n+     *                     headers are on a chain with sufficient work)\n+     * processing_success: set to false if an error is detected and the sync is\n+     *                     aborted; true otherwise.\n+     */\n+    std::optional<CBlockLocator> ProcessNextHeaders(const std::vector<CBlockHeader>& headers,",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 140,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": 932330073,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "This is way better, thanks.  ",
      "created_at": "2022-07-28T22:10:59Z",
      "updated_at": "2022-07-28T22:10:59Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932718671",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932718671"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 140,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932719744",
      "pull_request_review_id": 1054822937,
      "id": 932719744,
      "node_id": "PRRC_kwDOABII5843mCyA",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 5,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": 932184020,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-07-28T22:13:18Z",
      "updated_at": "2022-07-28T22:13:18Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r932719744",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/932719744"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 1,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 8,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933048600",
      "pull_request_review_id": 1055265006,
      "id": 933048600,
      "node_id": "PRRC_kwDOABII5843nTEY",
      "diff_hunk": "@@ -0,0 +1,67 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+import time\n+\n+NODE1_BLOCKS_REQUIRED = 15\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 15 blocks on top of the genesis block; node2 requires 2047\n+        self.extra_args = [[\"-minimumchainwork=0x0\"], [\"-minimumchainwork=0x1f\"], [\"-minimumchainwork=0x1000\"]]\n+\n+    def skip_test_if_missing_module(self):\n+        self.skip_if_no_wallet()",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 23,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "ec2723b39f4a994938d2d7f14586dcde823fe37e",
      "in_reply_to_id": null,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Not needed since you're not using wallet in this test",
      "created_at": "2022-07-29T09:26:59Z",
      "updated_at": "2022-07-29T09:54:43Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r933048600",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933048600"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 22,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 23,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933060182",
      "pull_request_review_id": 1055265006,
      "id": 933060182,
      "node_id": "PRRC_kwDOABII5843nV5W",
      "diff_hunk": "@@ -0,0 +1,68 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+import time\n+\n+NODE1_BLOCKS_REQUIRED = 16\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 16 blocks; node2 requires 2048 blocks\n+        self.extra_args = [[\"-minimumchainwork=0x0\"], [\"-minimumchainwork=0x1f\"], [\"-minimumchainwork=0x1000\"]]\n+\n+    def skip_test_if_missing_module(self):\n+        self.skip_if_no_wallet()\n+\n+    def test_chains_sync_when_long_enough(self):\n+        self.log.info(\"Generate blocks on the node with no required chainwork\")\n+        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED-2, sync_fun=self.no_op)\n+\n+        self.log.info(\"Verify nodes 1 and 2 have no new headers in their headers tree\")",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 29,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": 932282250,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "+1, it looks like node2 currently doesn't receive any headers because it's not connected to node0?\r\nIIRC the default is that 0-1 and 1-2 are connected. My suggestion would be to override `setup_network` with:\r\n\r\n```\r\nself.setup_nodes()\r\nself.connect_nodes(0, 1)\r\nself.connect_nodes(0, 2)\r\nself.sync_all(self.nodes[0:2])\r\n```",
      "created_at": "2022-07-29T09:41:00Z",
      "updated_at": "2022-07-29T09:54:43Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r933060182",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933060182"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 29,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933068912",
      "pull_request_review_id": 1055265006,
      "id": 933068912,
      "node_id": "PRRC_kwDOABII5843nYBw",
      "diff_hunk": "@@ -0,0 +1,67 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+import time\n+\n+NODE1_BLOCKS_REQUIRED = 15\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 15 blocks on top of the genesis block; node2 requires 2047\n+        self.extra_args = [[\"-minimumchainwork=0x0\"], [\"-minimumchainwork=0x1f\"], [\"-minimumchainwork=0x1000\"]]\n+\n+    def skip_test_if_missing_module(self):\n+        self.skip_if_no_wallet()\n+\n+    def test_chains_sync_when_long_enough(self):\n+        self.log.info(\"Generate blocks on the node with no required chainwork\")\n+        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED-2, sync_fun=self.no_op)\n+\n+        self.log.info(\"Verify nodes 1 and 2 have no new headers in their headers tree\")\n+        time.sleep(2)\n+        for node in self.nodes[1:]:",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 31,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "ec2723b39f4a994938d2d7f14586dcde823fe37e",
      "in_reply_to_id": null,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I agree with removing the `sleep()` since it's not a very reliable way to wait for things to happen in the functional tests (which are often run in parallel). Not sure about relying on `assert_debug_log`.\r\n\r\nI'd recommend `wait_until` the headers are received, for example:\r\n\r\n```suggestion\r\n        self.log.info(\"Generate blocks on the node with no required chainwork\")\r\n        node1_recv_headers = self.nodes[1].getpeerinfo()[0][\"bytesrecv_per_msg\"][\"headers\"]\r\n        node2_recv_headers = self.nodes[2].getpeerinfo()[0][\"bytesrecv_per_msg\"][\"headers\"]\r\n        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED-2, sync_fun=self.no_op)\r\n\r\n        self.log.info(\"Verify nodes 1 and 2 have no new headers in their headers tree\")\r\n        self.wait_until(lambda: self.nodes[1].getpeerinfo()[0][\"bytesrecv_per_msg\"][\"headers\"] > node1_recv_headers)\r\n        self.wait_until(lambda: self.nodes[2].getpeerinfo()[0][\"bytesrecv_per_msg\"][\"headers\"] > node2_recv_headers)\r\n        for node in self.nodes[1:]:\r\n```\r\n(this is using `getpeerinfo()[0]` assuming node1 and node2 have node0 as their first peer)",
      "created_at": "2022-07-29T09:51:32Z",
      "updated_at": "2022-07-29T09:54:43Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r933068912",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933068912"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 26,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 53,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933198524",
      "pull_request_review_id": 1055491612,
      "id": 933198524,
      "node_id": "PRRC_kwDOABII5843n3q8",
      "diff_hunk": "@@ -44,7 +48,12 @@ BOOST_AUTO_TEST_CASE(get_next_work_lower_limit_actual)\n     pindexLast.nHeight = 68543;\n     pindexLast.nTime = 1279297671;  // Block #68543\n     pindexLast.nBits = 0x1c05a3f4;\n-    BOOST_CHECK_EQUAL(CalculateNextWorkRequired(&pindexLast, nLastRetargetTime, chainParams->GetConsensus()), 0x1c0168fdU);\n+    unsigned int expected_nbits = 0x1c0168fdU;",
      "path": "src/test/pow_tests.cpp",
      "position": 32,
      "original_position": 27,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cbf3069c7d76888a647d68dbb09d77e666dcaf50",
      "in_reply_to_id": 932560137,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I'm not sure there's a great way to do this without essentially duplicating the code from `pow.cpp`; is that what you have in mind?  (That might be a reasonable suggestion, I'm just not sure what makes the most sense.)",
      "created_at": "2022-07-29T12:35:00Z",
      "updated_at": "2022-07-29T12:35:20Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r933198524",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933198524"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 56,
      "original_line": 56,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933233479",
      "pull_request_review_id": 1055535674,
      "id": 933233479,
      "node_id": "PRRC_kwDOABII5843oANH",
      "diff_hunk": "@@ -44,7 +48,12 @@ BOOST_AUTO_TEST_CASE(get_next_work_lower_limit_actual)\n     pindexLast.nHeight = 68543;\n     pindexLast.nTime = 1279297671;  // Block #68543\n     pindexLast.nBits = 0x1c05a3f4;\n-    BOOST_CHECK_EQUAL(CalculateNextWorkRequired(&pindexLast, nLastRetargetTime, chainParams->GetConsensus()), 0x1c0168fdU);\n+    unsigned int expected_nbits = 0x1c0168fdU;",
      "path": "src/test/pow_tests.cpp",
      "position": 32,
      "original_position": 27,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cbf3069c7d76888a647d68dbb09d77e666dcaf50",
      "in_reply_to_id": 932560137,
      "user": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Not convinced either way, maybe just comment on the relationship here?",
      "created_at": "2022-07-29T12:59:45Z",
      "updated_at": "2022-07-29T12:59:45Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r933233479",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933233479"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 56,
      "original_line": 56,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933235315",
      "pull_request_review_id": 1055426992,
      "id": 933235315,
      "node_id": "PRRC_kwDOABII5843oApz",
      "diff_hunk": "@@ -0,0 +1,528 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <util/bitdeque.h>\n+\n+#include <random.h>\n+#include <test/fuzz/FuzzedDataProvider.h>\n+#include <test/fuzz/util.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+namespace {\n+\n+constexpr int LEN_BITS = 16;\n+constexpr int RANDDATA_BITS = 20;\n+\n+using bitdeque_type = bitdeque<128>;",
      "path": "src/test/fuzz/bitdeque.cpp",
      "position": 19,
      "original_position": 19,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5072054428fa9229a0e34b9eb4a0eed97639012d",
      "in_reply_to_id": null,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "question in 5072054428fa9229a0e34b9eb4a0eed97639012d: what's the rationale for fuzzing with 128 instead of default blob size?",
      "created_at": "2022-07-29T13:01:32Z",
      "updated_at": "2022-08-01T16:19:09Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r933235315",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933235315"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 19,
      "original_line": 19,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933239940",
      "pull_request_review_id": 1055426992,
      "id": 933239940,
      "node_id": "PRRC_kwDOABII5843oByE",
      "diff_hunk": "@@ -0,0 +1,528 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <util/bitdeque.h>\n+\n+#include <random.h>\n+#include <test/fuzz/FuzzedDataProvider.h>\n+#include <test/fuzz/util.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+namespace {\n+\n+constexpr int LEN_BITS = 16;\n+constexpr int RANDDATA_BITS = 20;\n+\n+using bitdeque_type = bitdeque<128>;\n+\n+//! Deterministic random vector of bools, for begin/end insertions to draw from.\n+std::vector<bool> RANDDATA;\n+\n+void InitRandData()\n+{\n+    FastRandomContext ctx(true);\n+    RANDDATA.clear();\n+    for (size_t i = 0; i < (1U << RANDDATA_BITS) + (1U << LEN_BITS); ++i) {\n+        RANDDATA.push_back(ctx.randbool());\n+    }\n+}\n+\n+} // namespace\n+\n+FUZZ_TARGET_INIT(bitdeque, InitRandData)\n+{\n+    FuzzedDataProvider provider(buffer.data(), buffer.size());\n+    FastRandomContext ctx(true);\n+\n+    size_t maxlen = (1U << provider.ConsumeIntegralInRange<size_t>(0, LEN_BITS)) - 1;\n+    size_t limitlen = 4 * maxlen;\n+\n+    std::deque<bool> deq;\n+    bitdeque_type bitdeq;\n+\n+    const auto& cdeq = deq;\n+    const auto& cbitdeq = bitdeq;\n+\n+    size_t initlen = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+    while (initlen) {\n+        bool val = ctx.randbool();\n+        deq.push_back(val);\n+        bitdeq.push_back(val);\n+        --initlen;\n+    }\n+\n+    while (provider.remaining_bytes()) {\n+        {\n+            assert(deq.size() == bitdeq.size());\n+            auto it = deq.begin();\n+            auto bitit = bitdeq.begin();\n+            auto itend = deq.end();\n+            while (it != itend) {\n+                assert(*it == *bitit);\n+                ++it;\n+                ++bitit;\n+            }\n+        }\n+\n+        CallOneOf(provider,\n+            [&] {\n+                // constructor()\n+                deq = std::deque<bool>{};\n+                bitdeq = bitdeque_type{};\n+            },\n+            [&] {\n+                // assign(count, val)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                bool val = ctx.randbool();\n+                deq.assign(count, val);\n+                bitdeq.assign(count, val);\n+            },\n+            [&] {\n+                // constructor(count, val)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                bool val = ctx.randbool();\n+                deq = std::deque<bool>(count, val);\n+                bitdeq = bitdeque_type(count, val);\n+            },\n+            [&] {\n+                // constructor(count)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                deq = std::deque<bool>(count);\n+                bitdeq = bitdeque_type(count);\n+            },\n+            [&] {\n+                // construct(begin, end)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                auto rand_end = rand_begin + count;\n+                deq = std::deque<bool>(rand_begin, rand_end);\n+                bitdeq = bitdeque_type(rand_begin, rand_end);\n+            },\n+            [&] {\n+                // assign(begin, end)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                auto rand_end = rand_begin + count;\n+                deq.assign(rand_begin, rand_end);\n+                bitdeq.assign(rand_begin, rand_end);\n+            },\n+            [&] {\n+                // construct(initializer_list)\n+                std::initializer_list<bool> ilist{ctx.randbool(), ctx.randbool(), ctx.randbool(), ctx.randbool(), ctx.randbool()};\n+                deq = std::deque<bool>(ilist);\n+                bitdeq = bitdeque_type(ilist);\n+            },\n+            [&] {\n+                // assign(initializer_list)\n+                std::initializer_list<bool> ilist{ctx.randbool(), ctx.randbool(), ctx.randbool()};\n+                deq.assign(ilist);\n+                bitdeq.assign(ilist);\n+            },\n+            [&] {\n+                // operator=(const&)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                bool val = ctx.randbool();\n+                const std::deque<bool> deq2(count, val);\n+                deq = deq2;\n+                const bitdeque_type bitdeq2(count, val);\n+                bitdeq = bitdeq2;\n+            },\n+            [&] {\n+                // operator=(&&)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                bool val = ctx.randbool();\n+                std::deque<bool> deq2(count, val);\n+                deq = std::move(deq2);\n+                bitdeque_type bitdeq2(count, val);\n+                bitdeq = std::move(bitdeq2);\n+            },\n+            [&] {\n+                // deque swap\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                auto rand_end = rand_begin + count;\n+                std::deque<bool> deq2(rand_begin, rand_end);\n+                bitdeque_type bitdeq2(rand_begin, rand_end);\n+                using std::swap;\n+                assert(deq.size() == bitdeq.size());\n+                assert(deq2.size() == bitdeq2.size());\n+                swap(deq, deq2);\n+                swap(bitdeq, bitdeq2);\n+                assert(deq.size() == bitdeq.size());\n+                assert(deq2.size() == bitdeq2.size());\n+            },\n+            [&] {\n+                // deque.swap\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                auto rand_end = rand_begin + count;\n+                std::deque<bool> deq2(rand_begin, rand_end);\n+                bitdeque_type bitdeq2(rand_begin, rand_end);\n+                assert(deq.size() == bitdeq.size());\n+                assert(deq2.size() == bitdeq2.size());\n+                deq.swap(deq2);\n+                bitdeq.swap(bitdeq2);\n+                assert(deq.size() == bitdeq.size());\n+                assert(deq2.size() == bitdeq2.size());\n+            },\n+            [&] {\n+                // operator=(initializer_list)\n+                std::initializer_list<bool> ilist{ctx.randbool(), ctx.randbool(), ctx.randbool()};\n+                deq = ilist;\n+                bitdeq = ilist;\n+            },\n+            [&] {\n+                // iterator arithmetic\n+                auto pos1 = provider.ConsumeIntegralInRange<long>(0, cdeq.size());\n+                auto pos2 = provider.ConsumeIntegralInRange<long>(0, cdeq.size());\n+                auto it = deq.begin() + pos1;\n+                auto bitit = bitdeq.begin() + pos1;\n+                if ((size_t)pos1 != cdeq.size()) assert(*it == *bitit);\n+                assert(it - deq.begin() == pos1);\n+                assert(bitit - bitdeq.begin() == pos1);\n+                if (provider.ConsumeBool()) {\n+                    it += pos2 - pos1;\n+                    bitit += pos2 - pos1;\n+                } else {\n+                    it -= pos1 - pos2;\n+                    bitit -= pos1 - pos2;\n+                }\n+                if ((size_t)pos2 != cdeq.size()) assert(*it == *bitit);\n+                assert(deq.end() - it == bitdeq.end() - bitit);\n+                if (provider.ConsumeBool()) {\n+                    if ((size_t)pos2 != cdeq.size()) {\n+                        ++it;\n+                        ++bitit;\n+                    }\n+                } else {\n+                    if (pos2 != 0) {\n+                        --it;\n+                        --bitit;\n+                    }\n+                }\n+                assert(deq.end() - it == bitdeq.end() - bitit);\n+            },\n+            [&] {\n+                // begin() and end()\n+                assert(deq.end() - deq.begin() == bitdeq.end() - bitdeq.begin());\n+            },\n+            [&] {\n+                // begin() and end() (const)\n+                assert(cdeq.end() - cdeq.begin() == cbitdeq.end() - cbitdeq.begin());\n+            },\n+            [&] {\n+                // rbegin() and rend()\n+                assert(deq.rend() - deq.rbegin() == bitdeq.rend() - bitdeq.rbegin());\n+            },\n+            [&] {\n+                // rbegin() and rend() (const)\n+                assert(cdeq.rend() - cdeq.rbegin() == cbitdeq.rend() - cbitdeq.rbegin());\n+            },\n+            [&] {\n+                // cbegin() and cend()\n+                assert(cdeq.cend() - cdeq.cbegin() == cbitdeq.cend() - cbitdeq.cbegin());\n+            },\n+            [&] {\n+                // crbegin() and crend()\n+                assert(cdeq.crend() - cdeq.crbegin() == cbitdeq.crend() - cbitdeq.crbegin());\n+            },\n+            [&] {\n+                // size\n+                assert(cdeq.size() == cbitdeq.size());\n+            },\n+            [&] {\n+                // empty\n+                assert(cdeq.empty() == cbitdeq.empty());\n+            },\n+            [&] {\n+                // at (in range) and flip\n+                if (!cdeq.empty()) {\n+                    size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 1);\n+                    auto& ref = deq.at(pos);\n+                    auto bitref = bitdeq.at(pos);\n+                    assert(ref == bitref);\n+                    if (ctx.randbool()) {\n+                        ref = !ref;\n+                        bitref.flip();\n+                    }\n+                }\n+            },\n+            [&] {\n+                // at (maybe out of range) and bit assign\n+                size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() + maxlen);\n+                bool newval = ctx.randbool();\n+                bool throw_deq{false}, throw_bitdeq{false};\n+                bool val_deq{false}, val_bitdeq{false};\n+                try {\n+                    auto& ref = deq.at(pos);\n+                    val_deq = ref;\n+                    ref = newval;\n+                } catch (const std::out_of_range&) {\n+                    throw_deq = true;\n+                }\n+                try {\n+                    auto ref = bitdeq.at(pos);\n+                    val_bitdeq = ref;\n+                    ref = newval;\n+                } catch (const std::out_of_range&) {\n+                    throw_bitdeq = true;\n+                }\n+                assert(throw_deq == throw_bitdeq);\n+                assert(throw_bitdeq == pos >= cdeq.size());\n+                if (!throw_deq) assert(val_deq == val_bitdeq);\n+            },\n+            [&] {\n+                // at (maybe out of range) (const)\n+                size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() + maxlen);\n+                bool throw_deq{false}, throw_bitdeq{false};\n+                bool val_deq{false}, val_bitdeq{false};\n+                try {\n+                    auto& ref = cdeq.at(pos);\n+                    val_deq = ref;\n+                } catch (const std::out_of_range&) {\n+                    throw_deq = true;\n+                }\n+                try {\n+                    auto ref = cbitdeq.at(pos);\n+                    val_bitdeq = ref;\n+                } catch (const std::out_of_range&) {\n+                    throw_bitdeq = true;\n+                }\n+                assert(throw_deq == throw_bitdeq);\n+                assert(throw_bitdeq == pos >= cdeq.size());\n+                if (!throw_deq) assert(val_deq == val_bitdeq);\n+            },\n+            [&] {\n+                // operator[]\n+                if (!cdeq.empty()) {\n+                    size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 1);\n+                    assert(deq[pos] == bitdeq[pos]);\n+                    if (ctx.randbool()) {\n+                        deq[pos] = !deq[pos];\n+                        bitdeq[pos].flip();\n+                    }\n+                }\n+            },\n+            [&] {\n+                // operator[] const\n+                if (!cdeq.empty()) {\n+                    size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 1);\n+                    assert(deq[pos] == bitdeq[pos]);\n+                }\n+            },\n+            [&] {\n+                // front()\n+                if (!cdeq.empty()) {\n+                    auto& ref = deq.front();\n+                    auto bitref = bitdeq.front();\n+                    assert(ref == bitref);\n+                    if (ctx.randbool()) {\n+                        ref = !ref;\n+                        bitref = !bitref;\n+                    }\n+                }\n+            },\n+            [&] {\n+                // front() const\n+                if (!cdeq.empty()) {\n+                    auto& ref = cdeq.front();\n+                    auto bitref = cbitdeq.front();\n+                    assert(ref == bitref);\n+                }\n+            },\n+            [&] {\n+                // back() and swap(bool, ref)\n+                if (!cdeq.empty()) {\n+                    auto& ref = deq.back();\n+                    auto bitref = bitdeq.back();\n+                    assert(ref == bitref);\n+                    if (ctx.randbool()) {\n+                        ref = !ref;\n+                        bitref.flip();\n+                    }\n+                }\n+            },\n+            [&] {\n+                // back() const\n+                if (!cdeq.empty()) {\n+                    const auto& cdeq = deq;\n+                    const auto& cbitdeq = bitdeq;\n+                    auto& ref = cdeq.back();\n+                    auto bitref = cbitdeq.back();\n+                    assert(ref == bitref);\n+                }\n+            },\n+            [&] {\n+                // push_back()\n+                if (cdeq.size() < limitlen) {\n+                    bool val = ctx.randbool();\n+                    if (cdeq.empty()) {\n+                        deq.push_back(val);\n+                        bitdeq.push_back(val);\n+                    } else {\n+                        size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 1);\n+                        auto& ref = deq[pos];\n+                        auto bitref = bitdeq[pos];\n+                        assert(ref == bitref);\n+                        deq.push_back(val);\n+                        bitdeq.push_back(val);\n+                        assert(ref == bitref); // references are not invalidated\n+                    }\n+                }\n+            },\n+            [&] {\n+                // push_front()\n+                if (cdeq.size() < limitlen) {\n+                    bool val = ctx.randbool();\n+                    if (cdeq.empty()) {\n+                        deq.push_front(val);\n+                        bitdeq.push_front(val);\n+                    } else {\n+                        size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 1);\n+                        auto& ref = deq[pos];\n+                        auto bitref = bitdeq[pos];\n+                        assert(ref == bitref);\n+                        deq.push_front(val);\n+                        bitdeq.push_front(val);\n+                        assert(ref == bitref); // references are not invalidated\n+                    }\n+                }\n+            },\n+            [&] {\n+                // pop_back()\n+                if (!cdeq.empty()) {\n+                    if (cdeq.size() == 1) {\n+                        deq.pop_back();\n+                        bitdeq.pop_back();\n+                    } else {\n+                        size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 2);\n+                        auto& ref = deq[pos];\n+                        auto bitref = bitdeq[pos];\n+                        assert(ref == bitref);\n+                        deq.pop_back();\n+                        bitdeq.pop_back();\n+                        assert(ref == bitref); // references to other elements are not invalidated\n+                    }\n+                }\n+            },\n+            [&] {\n+                // pop_front()\n+                if (!cdeq.empty()) {\n+                    if (cdeq.size() == 1) {\n+                        deq.pop_front();\n+                        bitdeq.pop_front();\n+                    } else {\n+                        size_t pos = provider.ConsumeIntegralInRange<size_t>(1, cdeq.size() - 1);\n+                        auto& ref = deq[pos];\n+                        auto bitref = bitdeq[pos];\n+                        assert(ref == bitref);\n+                        deq.pop_front();\n+                        bitdeq.pop_front();\n+                        assert(ref == bitref); // references to other elements are not invalidated\n+                    }\n+                }\n+            },\n+            [&] {\n+                // erase (in middle, single)\n+                if (!cdeq.empty()) {\n+                    size_t before = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 1);\n+                    size_t after = cdeq.size() - 1 - before;\n+                    auto it = deq.erase(cdeq.begin() + before);\n+                    auto bitit = bitdeq.erase(cbitdeq.begin() + before);\n+                    assert(it == cdeq.begin() + before && it == cdeq.end() - after);\n+                    assert(bitit == cbitdeq.begin() + before && bitit == cbitdeq.end() - after);\n+                }\n+            },\n+            [&] {\n+                // erase (at front, range)\n+                size_t count = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size());\n+                auto it = deq.erase(cdeq.begin(), cdeq.begin() + count);\n+                auto bitit = bitdeq.erase(cbitdeq.begin(), cbitdeq.begin() + count);\n+                assert(it == deq.begin());\n+                assert(bitit == bitdeq.begin());\n+            },\n+            [&] {\n+                // erase (at back, range)\n+                size_t count = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size());\n+                auto it = deq.erase(cdeq.end() - count, cdeq.end());\n+                auto bitit = bitdeq.erase(cbitdeq.end() - count, cbitdeq.end());\n+                assert(it == deq.end());\n+                assert(bitit == bitdeq.end());\n+            },\n+            [&] {\n+                // erase (in middle, range)\n+                size_t count = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size());\n+                size_t before = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - count);\n+                size_t after = cdeq.size() - count - before;\n+                auto it = deq.erase(cdeq.begin() + before, cdeq.end() - after);\n+                auto bitit = bitdeq.erase(cbitdeq.begin() + before, cbitdeq.end() - after);\n+                assert(it == cdeq.begin() + before && it == cdeq.end() - after);\n+                assert(bitit == cbitdeq.begin() + before && bitit == cbitdeq.end() - after);\n+            },\n+            [&] {\n+                // insert (in middle, single)\n+                if (cdeq.size() < limitlen) {\n+                    size_t before = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size());\n+                    bool val = ctx.randbool();\n+                    auto it = deq.insert(cdeq.begin() + before, val);\n+                    auto bitit = bitdeq.insert(cbitdeq.begin() + before, val);\n+                    assert(it == deq.begin() + before);\n+                    assert(bitit == bitdeq.begin() + before);\n+                }\n+            },\n+            [&] {\n+                // insert (at front, begin/end)\n+                if (cdeq.size() < limitlen) {\n+                    size_t count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                    auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                    auto rand_end = rand_begin + count;\n+                    auto it = deq.insert(cdeq.begin(), rand_begin, rand_end);\n+                    auto bitit = bitdeq.insert(cbitdeq.begin(), rand_begin, rand_end);\n+                    assert(it == cdeq.begin());\n+                    assert(bitit == cbitdeq.begin());\n+                }\n+            },\n+            [&] {\n+                // insert (at back, begin/end)\n+                if (cdeq.size() < limitlen) {\n+                    size_t count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                    auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                    auto rand_end = rand_begin + count;\n+                    auto it = deq.insert(cdeq.end(), rand_begin, rand_end);\n+                    auto bitit = bitdeq.insert(cbitdeq.end(), rand_begin, rand_end);\n+                    assert(it == cdeq.end() - count);\n+                    assert(bitit == cbitdeq.end() - count);\n+                }\n+            },\n+            [&] {\n+                // insert (in middle, range)\n+                if (cdeq.size() < limitlen) {\n+                    size_t count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                    size_t before = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size());\n+                    bool val = ctx.randbool();\n+                    auto it = deq.insert(cdeq.begin() + before, count, val);\n+                    auto bitit = bitdeq.insert(cbitdeq.begin() + before, count, val);\n+                    assert(it == deq.begin() + before);\n+                    assert(bitit == bitdeq.begin() + before);\n+                }\n+            },\n+            [&] {\n+                // insert (in middle, begin/end)\n+                if (cdeq.size() < limitlen) {\n+                    size_t count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                    size_t before = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size());\n+                    auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                    auto rand_end = rand_begin + count;\n+                    auto it = deq.insert(cdeq.begin() + before, rand_begin, rand_end);\n+                    auto bitit = bitdeq.insert(cbitdeq.begin() + before, rand_begin, rand_end);\n+                    assert(it == deq.begin() + before);\n+                    assert(bitit == bitdeq.begin() + before);\n+                }",
      "path": "src/test/fuzz/bitdeque.cpp",
      "position": 537,
      "original_position": 523,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5072054428fa9229a0e34b9eb4a0eed97639012d",
      "in_reply_to_id": null,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "in 5072054428fa9229a0e34b9eb4a0eed97639012d:\r\n\r\nPerhaps these methods could be fuzzed too?\r\n- `clear()`\r\n- `resize()`\r\n- `max_size()`\r\n- `emplace()` (though maybe unnecessary since there's already `insert`)",
      "created_at": "2022-07-29T13:05:35Z",
      "updated_at": "2022-08-01T16:19:09Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r933239940",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933239940"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 537,
      "original_line": 537,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933301267",
      "pull_request_review_id": 1055610302,
      "id": 933301267,
      "node_id": "PRRC_kwDOABII5843oQwT",
      "diff_hunk": "@@ -0,0 +1,67 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+import time\n+\n+NODE1_BLOCKS_REQUIRED = 15\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 15 blocks on top of the genesis block; node2 requires 2047\n+        self.extra_args = [[\"-minimumchainwork=0x0\"], [\"-minimumchainwork=0x1f\"], [\"-minimumchainwork=0x1000\"]]\n+\n+    def skip_test_if_missing_module(self):\n+        self.skip_if_no_wallet()",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 23,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "ec2723b39f4a994938d2d7f14586dcde823fe37e",
      "in_reply_to_id": 933048600,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed.",
      "created_at": "2022-07-29T13:56:58Z",
      "updated_at": "2022-07-29T13:56:59Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r933301267",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933301267"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 22,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 23,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933302625",
      "pull_request_review_id": 1055612143,
      "id": 933302625,
      "node_id": "PRRC_kwDOABII5843oRFh",
      "diff_hunk": "@@ -0,0 +1,67 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+import time\n+\n+NODE1_BLOCKS_REQUIRED = 15\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 15 blocks on top of the genesis block; node2 requires 2047\n+        self.extra_args = [[\"-minimumchainwork=0x0\"], [\"-minimumchainwork=0x1f\"], [\"-minimumchainwork=0x1000\"]]\n+\n+    def skip_test_if_missing_module(self):\n+        self.skip_if_no_wallet()\n+\n+    def test_chains_sync_when_long_enough(self):\n+        self.log.info(\"Generate blocks on the node with no required chainwork\")\n+        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED-2, sync_fun=self.no_op)\n+\n+        self.log.info(\"Verify nodes 1 and 2 have no new headers in their headers tree\")\n+        time.sleep(2)\n+        for node in self.nodes[1:]:",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 31,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "ec2723b39f4a994938d2d7f14586dcde823fe37e",
      "in_reply_to_id": 933068912,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Ah, that's clever, but I think the problem is that the number of headers messages that come in after a call to `generate` is variable, based on the particular blocks that get INV'ed and the responses they generate.  \r\n\r\nI took an approach of changing the logging to include the height of the chain, and then used the `assert_debug_log` approach to check for it.  This gets rid of all the sleeps and I think ought to work?",
      "created_at": "2022-07-29T13:58:16Z",
      "updated_at": "2022-07-29T13:58:16Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r933302625",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933302625"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 26,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 53,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933501386",
      "pull_request_review_id": 1055897986,
      "id": 933501386,
      "node_id": "PRRC_kwDOABII5843pBnK",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 47,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": 932272722,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Maybe it's a bit more natural and concise to have a separate state for this? e,g. have states `UNSTARTED`, `INITAL_DOWNLOAD`, `REDOWNLOAD`, `FINAL`?",
      "created_at": "2022-07-29T17:58:38Z",
      "updated_at": "2022-08-01T21:30:45Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r933501386",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933501386"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 50,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933507679",
      "pull_request_review_id": 1055897986,
      "id": 933507679,
      "node_id": "PRRC_kwDOABII5843pDJf",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+//! Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+    std::vector<uint256>().swap(m_chain_start_locator.vHave);\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Initialize the parameters for this headers download, validate this first\n+ * batch, and request more headers. */\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+    assert(m_sync_started == false);\n+\n+    m_sync_started = true;\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+            // Somehow our peer gave us a header that doesn't connect.\n+            // This might be benign -- perhaps we issued an extra getheaders\n+            // message, such as after a block INV was received.\n+            // Or it could be that our peer is broken or malicious. If broken,\n+            // sending a new getheaders immediately could trigger an infinite\n+            // loop. Just give up for now; if our peer ever gives us an block\n+            // INV later we will fetch headers then, and likely retrigger this\n+            // logic.\n+            Finalize();\n+            return ret;",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 106,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "fe620e1a1fb2ef11d06cf941249ad4e5e4250bbf",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "There are lots of `return ret;` statements in this function now. Perhaps it's cleaner to restructure it so that there is only a single return statement at the end, and all the branches just modify the values in `ret`?\r\n\r\n",
      "created_at": "2022-07-29T18:08:25Z",
      "updated_at": "2022-08-01T21:30:45Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r933507679",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/933507679"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 106,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934021719",
      "pull_request_review_id": 1056600826,
      "id": 934021719,
      "node_id": "PRRC_kwDOABII5843rApX",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    // Overestimate the number of blocks that could possibly exist on this\n+    // chain using 6 blocks/second (fastest blockrate given the MTP rule) times\n+    // the number of seconds from the last allowed block until today. This\n+    // serves as a memory bound on how many commitments we might store from\n+    // this peer, and we can safely give up syncing if the peer exceeds this\n+    // bound, because it's not possible for a consensus-valid chain to be\n+    // longer than this.\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+// Process the next batch of headers received from our peer.\n+// Validate and store commitments, and compare total chainwork to our target to\n+// see if we can switch to REDOWNLOAD mode.\n+std::optional<CBlockLocator> HeadersSyncState::ProcessNextHeaders(const std::vector<CBlockHeader>&\n+        headers, bool full_headers_message, std::vector<CBlockHeader>&\n+        headers_to_process, bool& processing_success)\n+{\n+    processing_success = false;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return std::nullopt;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return std::nullopt;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+            // Somehow our peer gave us a header that doesn't connect.\n+            // This might be benign -- perhaps we issued an extra getheaders\n+            // message, such as after a block INV was received.\n+            // Or it could be that our peer is broken or malicious. If broken,\n+            // sending a new getheaders immediately could trigger an infinite\n+            // loop. Just give up for now; if our peer ever gives us an block\n+            // INV later we will fetch headers then, and likely retrigger this\n+            // logic.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return std::nullopt;\n+        }\n+        processing_success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return std::nullopt;\n+            }\n+        }\n+\n+        processing_success = true;\n+        // Return any headers that are ready for acceptance.\n+        headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return std::nullopt;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    }\n+    return std::nullopt;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (m_last_header_received.IsNull()) {\n+        m_last_header_received = m_chain_start->GetBlockHeader();\n+        m_current_height = m_chain_start->nHeight;\n+    }\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Doesn't connect to what we expect\n+        Finalize();\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr, m_current_height+1)) {\n+            Finalize();\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_blockhash_with_sufficient_work = headers.back().GetHash();\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash.SetNull();\n+        m_download_state = State::REDOWNLOAD;\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current, int64_t current_height)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, current_height,\n+                previous.nBits, current.nBits)) {\n+        return false;\n+    }\n+\n+    if (!CheckProofOfWork(current.GetHash(), current.nBits, m_consensus_params)) return false;\n+\n+    if ((current_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+        // Try to add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // TODO: disconnect this peer.",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 220,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": 932186749,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Now that I think about it, it's possible that if the peer's chain has grown (and the sync takes very long) that you could get to this condition; the right thing to do in that scenario (assuming our peer has an actually more work chain) is to try to sync with this peer again later.  (Obviously this is a pathological case, but I think we should still be able to sync such a chain anyway, eventually.)\r\n\r\nI'll update the comment.",
      "created_at": "2022-07-31T18:14:03Z",
      "updated_at": "2022-07-31T18:14:03Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r934021719",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934021719"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 225,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934021892",
      "pull_request_review_id": 1056600977,
      "id": 934021892,
      "node_id": "PRRC_kwDOABII5843rAsE",
      "diff_hunk": "@@ -0,0 +1,68 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+import time\n+\n+NODE1_BLOCKS_REQUIRED = 16\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 16 blocks; node2 requires 2048 blocks\n+        self.extra_args = [[\"-minimumchainwork=0x0\"], [\"-minimumchainwork=0x1f\"], [\"-minimumchainwork=0x1000\"]]\n+\n+    def skip_test_if_missing_module(self):\n+        self.skip_if_no_wallet()\n+\n+    def test_chains_sync_when_long_enough(self):\n+        self.log.info(\"Generate blocks on the node with no required chainwork\")\n+        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED-2, sync_fun=self.no_op)\n+\n+        self.log.info(\"Verify nodes 1 and 2 have no new headers in their headers tree\")",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 29,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": 932282250,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-07-31T18:15:39Z",
      "updated_at": "2022-07-31T18:15:39Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r934021892",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934021892"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 29,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934656740",
      "pull_request_review_id": 1055426992,
      "id": 934656740,
      "node_id": "PRRC_kwDOABII5843tbrk",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+//! Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 10,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "fe620e1a1fb2ef11d06cf941249ad4e5e4250bbf",
      "in_reply_to_id": null,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "in fe620e1a1fb2ef11d06cf941249ad4e5e4250bbf:\r\n\r\nPerhaps 25 should be named, e.g. `REDOWNLOAD_BUFFER_THRESHOLD`? IIUC it's possible to change this in the future if the optimization script outputs a different rsize?",
      "created_at": "2022-08-01T15:27:25Z",
      "updated_at": "2022-08-01T16:19:09Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r934656740",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934656740"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 10,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934663545",
      "pull_request_review_id": 1055426992,
      "id": 934663545,
      "node_id": "PRRC_kwDOABII5843tdV5",
      "diff_hunk": "@@ -0,0 +1,66 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+NODE1_BLOCKS_REQUIRED = 15\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 15 blocks on top of the genesis block; node2 requires 2047\n+        self.extra_args = [[\"-minimumchainwork=0x0\"], [\"-minimumchainwork=0x1f\"], [\"-minimumchainwork=0x1000\"]]\n+\n+    def setup_network(self):\n+        self.setup_nodes()\n+        self.connect_nodes(0, 1)\n+        self.connect_nodes(0, 2)\n+        self.sync_all(self.nodes[0:2])",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 24,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "e3260960add016aeb43d0be4c5b735418e37c4a7",
      "in_reply_to_id": null,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "in 172453aa980e7e0b9cfa616971090461a2796c09:\r\n\r\nOops! I'm sorry, I gave an incorrect suggestion before :facepalm: this would exclude node2\r\n```suggestion\r\n        self.sync_all()\r\n```",
      "created_at": "2022-08-01T15:34:13Z",
      "updated_at": "2022-08-01T16:19:09Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r934663545",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934663545"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 24,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934671923",
      "pull_request_review_id": 1055426992,
      "id": 934671923,
      "node_id": "PRRC_kwDOABII5843tfYz",
      "diff_hunk": "@@ -3426,6 +3426,25 @@ std::vector<unsigned char> ChainstateManager::GenerateCoinbaseCommitment(CBlock&\n     return commitment;\n }\n \n+bool HasValidProofOfWork(const std::vector<CBlockHeader>& headers, const Consensus::Params& consensusParams)\n+{\n+    bool proof_of_work_valid = true;\n+    for (const CBlockHeader& header : headers) {\n+        proof_of_work_valid = proof_of_work_valid && CheckProofOfWork(header.GetHash(), header.nBits, consensusParams);\n+    }\n+    return proof_of_work_valid;",
      "path": "src/validation.cpp",
      "position": null,
      "original_position": 10,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "fe620e1a1fb2ef11d06cf941249ad4e5e4250bbf",
      "in_reply_to_id": null,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "nit in fe620e1a1fb2ef11d06cf941249ad4e5e4250bbf, this might be clearer:\r\n```suggestion\r\n    return std::all_of(headers.cbegin(), headers.cend(),\r\n                       [&](const auto& header) { return CheckProofOfWork(header.GetHash(), header.nBits, consensusParams);});\r\n```",
      "created_at": "2022-08-01T15:43:14Z",
      "updated_at": "2022-08-01T16:19:09Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r934671923",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934671923"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 3431,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 3435,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934681520",
      "pull_request_review_id": 1055426992,
      "id": 934681520,
      "node_id": "PRRC_kwDOABII5843thuw",
      "diff_hunk": "@@ -20,4 +20,18 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n /** Check whether a block hash satisfies the proof-of-work requirement specified by nBits */\n bool CheckProofOfWork(uint256 hash, unsigned int nBits, const Consensus::Params&);\n \n+/**\n+ * Return false if the proof-of-work requirement specified by new_nbits at a\n+ * given height is not possible, given the proof-of-work on the prior block as\n+ * specified by old_nbits.\n+ *\n+ * This function only checks that the new value is within a factor of 4 of the\n+ * old value for blocks at the difficulty adjustment interval, and otherwise\n+ * requires the values to be the same.\n+ *\n+ * Always returns true on networks where min difficulty blocks are allowed,\n+ * such as regtest/testnet.",
      "path": "src/pow.h",
      "position": 14,
      "original_position": 14,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "130d838df0b02f1115cb54e62002a95823d5efcb",
      "in_reply_to_id": null,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "in 130d838df0b02f1115cb54e62002a95823d5efcb:\r\n\r\nMaybe also mention this always returns true if not on a difficulty adjustment block?",
      "created_at": "2022-08-01T15:53:32Z",
      "updated_at": "2022-08-01T16:19:09Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r934681520",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934681520"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": 32,
      "original_start_line": 32,
      "start_side": "RIGHT",
      "line": 33,
      "original_line": 33,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934686816",
      "pull_request_review_id": 1055426992,
      "id": 934686816,
      "node_id": "PRRC_kwDOABII5843tjBg",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+//! Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 10,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "e3260960add016aeb43d0be4c5b735418e37c4a7",
      "in_reply_to_id": null,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "nit in fe620e1a1fb2ef11d06cf941249ad4e5e4250bbf:\r\n```suggestion\r\n//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\r\nconstexpr size_t HEADER_COMMITMENT_FREQUENCY{571};\r\n//! Pull out headers for acceptance during redownload once our buffer exceeds this number of headers\r\nconstexpr size_t REDOWNLOAD_HEADERS_THRESHOLD{25*HEADER_COMMITMENT_FREQUENCY};\r\n```",
      "created_at": "2022-08-01T15:59:12Z",
      "updated_at": "2022-08-01T16:19:09Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r934686816",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934686816"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 7,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 10,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934916850",
      "pull_request_review_id": 1055897986,
      "id": 934916850,
      "node_id": "PRRC_kwDOABII5843ubLy",
      "diff_hunk": "@@ -71,6 +71,54 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n     return bnNew.GetCompact();\n }\n \n+// Check that on difficulty adjustments, the new difficulty does not increase\n+// or decrease beyond the permitted limits.\n+bool PermittedDifficultyTransition(const Consensus::Params& params, int64_t height, uint32_t old_nbits, uint32_t new_nbits)\n+{\n+    if (params.fPowAllowMinDifficultyBlocks) return true;\n+\n+    if (height % params.DifficultyAdjustmentInterval() == 0) {\n+        int64_t smallest_timespan = params.nPowTargetTimespan/4;\n+        int64_t largest_timespan = params.nPowTargetTimespan*4;\n+\n+        const arith_uint256 bnPowLimit = UintToArith256(params.powLimit);\n+        arith_uint256 observed_new_target;\n+        observed_new_target.SetCompact(new_nbits);\n+\n+        // Calculate the largest difficulty value possible:\n+        arith_uint256 bnNew;\n+        bnNew.SetCompact(old_nbits);\n+        bnNew *= largest_timespan;\n+        bnNew /= params.nPowTargetTimespan;\n+\n+        if (bnNew > bnPowLimit)\n+            bnNew = bnPowLimit;",
      "path": "src/pow.cpp",
      "position": null,
      "original_position": 25,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cbf3069c7d76888a647d68dbb09d77e666dcaf50",
      "in_reply_to_id": 932235084,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I prefer doing so yes.",
      "created_at": "2022-08-01T21:01:22Z",
      "updated_at": "2022-08-01T21:30:45Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r934916850",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934916850"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 95,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934918571",
      "pull_request_review_id": 1055897986,
      "id": 934918571,
      "node_id": "PRRC_kwDOABII5843ubmr",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+//! Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 10,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "fe620e1a1fb2ef11d06cf941249ad4e5e4250bbf",
      "in_reply_to_id": 934656740,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "@glozow `REDOWNLOAD_HEADERS_THRESHOLD` will (in optimal configurations) always be a multiple of `HEADER_COMMITMENT_FREQUENCY`. This could just have been written as just `= 14275;`, but that would make this multiple-of relation less clear.\r\n\r\nIf it's confusing, I would suggest instead doing:\r\n\r\n```c++\r\nconstexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\r\nconstexpr size_t REDOWNLOAD_HEADERS_FACTOR = 25;\r\n```\r\n\r\nand then replace the current instances of `REDOWNLOAD_HEADERS_THRESHOLD` with `HEADER_COMMITMENT_FREQUENCY * REDOWNLOAD_HEADERS_FACTOR`.",
      "created_at": "2022-08-01T21:04:10Z",
      "updated_at": "2022-08-01T21:30:45Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r934918571",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934918571"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 10,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934920798",
      "pull_request_review_id": 1055897986,
      "id": 934920798,
      "node_id": "PRRC_kwDOABII5843ucJe",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    // Overestimate the number of blocks that could possibly exist on this\n+    // chain using 6 blocks/second (fastest blockrate given the MTP rule) times\n+    // the number of seconds from the last allowed block until today. This\n+    // serves as a memory bound on how many commitments we might store from\n+    // this peer, and we can safely give up syncing if the peer exceeds this\n+    // bound, because it's not possible for a consensus-valid chain to be\n+    // longer than this.",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 61,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": 932259718,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Looks good.",
      "created_at": "2022-08-01T21:07:36Z",
      "updated_at": "2022-08-01T21:30:45Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r934920798",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934920798"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 61,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934921949",
      "pull_request_review_id": 1055897986,
      "id": 934921949,
      "node_id": "PRRC_kwDOABII5843ucbd",
      "diff_hunk": "@@ -71,6 +71,54 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n     return bnNew.GetCompact();\n }\n \n+// Check that on difficulty adjustments, the new difficulty does not increase\n+// or decrease beyond the permitted limits.\n+bool PermittedDifficultyTransition(const Consensus::Params& params, int64_t height, uint32_t old_nbits, uint32_t new_nbits)\n+{\n+    if (params.fPowAllowMinDifficultyBlocks) return true;\n+\n+    if (height % params.DifficultyAdjustmentInterval() == 0) {\n+        int64_t smallest_timespan = params.nPowTargetTimespan/4;\n+        int64_t largest_timespan = params.nPowTargetTimespan*4;\n+\n+        const arith_uint256 bnPowLimit = UintToArith256(params.powLimit);",
      "path": "src/pow.cpp",
      "position": null,
      "original_position": 14,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cbf3069c7d76888a647d68dbb09d77e666dcaf50",
      "in_reply_to_id": 932564292,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "@instagibbs In the old Satoshi-era naming convention, it referred to \"bignum\" (there was a `CBigNum` wrapper around the OpenSSL `BIGNUM` type, which was used for various big integer operations, both inside script and for PoW purposes).",
      "created_at": "2022-08-01T21:09:28Z",
      "updated_at": "2022-08-01T21:30:45Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r934921949",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934921949"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 84,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934923131",
      "pull_request_review_id": 1055897986,
      "id": 934923131,
      "node_id": "PRRC_kwDOABII5843uct7",
      "diff_hunk": "@@ -0,0 +1,528 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <util/bitdeque.h>\n+\n+#include <random.h>\n+#include <test/fuzz/FuzzedDataProvider.h>\n+#include <test/fuzz/util.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+namespace {\n+\n+constexpr int LEN_BITS = 16;\n+constexpr int RANDDATA_BITS = 20;\n+\n+using bitdeque_type = bitdeque<128>;",
      "path": "src/test/fuzz/bitdeque.cpp",
      "position": 19,
      "original_position": 19,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5072054428fa9229a0e34b9eb4a0eed97639012d",
      "in_reply_to_id": 933235315,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "@glozow The testing power is significantly less if you need 16384 booleans before another allocation is made (because interactions that involve many separate allocations are much more likely to trigger bugs).",
      "created_at": "2022-08-01T21:11:09Z",
      "updated_at": "2022-08-01T21:30:45Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r934923131",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934923131"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 19,
      "original_line": 19,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934933976",
      "pull_request_review_id": 1055897986,
      "id": 934933976,
      "node_id": "PRRC_kwDOABII5843ufXY",
      "diff_hunk": "@@ -0,0 +1,528 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <util/bitdeque.h>\n+\n+#include <random.h>\n+#include <test/fuzz/FuzzedDataProvider.h>\n+#include <test/fuzz/util.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+namespace {\n+\n+constexpr int LEN_BITS = 16;\n+constexpr int RANDDATA_BITS = 20;\n+\n+using bitdeque_type = bitdeque<128>;\n+\n+//! Deterministic random vector of bools, for begin/end insertions to draw from.\n+std::vector<bool> RANDDATA;\n+\n+void InitRandData()\n+{\n+    FastRandomContext ctx(true);\n+    RANDDATA.clear();\n+    for (size_t i = 0; i < (1U << RANDDATA_BITS) + (1U << LEN_BITS); ++i) {\n+        RANDDATA.push_back(ctx.randbool());\n+    }\n+}\n+\n+} // namespace\n+\n+FUZZ_TARGET_INIT(bitdeque, InitRandData)\n+{\n+    FuzzedDataProvider provider(buffer.data(), buffer.size());\n+    FastRandomContext ctx(true);\n+\n+    size_t maxlen = (1U << provider.ConsumeIntegralInRange<size_t>(0, LEN_BITS)) - 1;\n+    size_t limitlen = 4 * maxlen;\n+\n+    std::deque<bool> deq;\n+    bitdeque_type bitdeq;\n+\n+    const auto& cdeq = deq;\n+    const auto& cbitdeq = bitdeq;\n+\n+    size_t initlen = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+    while (initlen) {\n+        bool val = ctx.randbool();\n+        deq.push_back(val);\n+        bitdeq.push_back(val);\n+        --initlen;\n+    }\n+\n+    while (provider.remaining_bytes()) {\n+        {\n+            assert(deq.size() == bitdeq.size());\n+            auto it = deq.begin();\n+            auto bitit = bitdeq.begin();\n+            auto itend = deq.end();\n+            while (it != itend) {\n+                assert(*it == *bitit);\n+                ++it;\n+                ++bitit;\n+            }\n+        }\n+\n+        CallOneOf(provider,\n+            [&] {\n+                // constructor()\n+                deq = std::deque<bool>{};\n+                bitdeq = bitdeque_type{};\n+            },\n+            [&] {\n+                // assign(count, val)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                bool val = ctx.randbool();\n+                deq.assign(count, val);\n+                bitdeq.assign(count, val);\n+            },\n+            [&] {\n+                // constructor(count, val)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                bool val = ctx.randbool();\n+                deq = std::deque<bool>(count, val);\n+                bitdeq = bitdeque_type(count, val);\n+            },\n+            [&] {\n+                // constructor(count)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                deq = std::deque<bool>(count);\n+                bitdeq = bitdeque_type(count);\n+            },\n+            [&] {\n+                // construct(begin, end)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                auto rand_end = rand_begin + count;\n+                deq = std::deque<bool>(rand_begin, rand_end);\n+                bitdeq = bitdeque_type(rand_begin, rand_end);\n+            },\n+            [&] {\n+                // assign(begin, end)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                auto rand_end = rand_begin + count;\n+                deq.assign(rand_begin, rand_end);\n+                bitdeq.assign(rand_begin, rand_end);\n+            },\n+            [&] {\n+                // construct(initializer_list)\n+                std::initializer_list<bool> ilist{ctx.randbool(), ctx.randbool(), ctx.randbool(), ctx.randbool(), ctx.randbool()};\n+                deq = std::deque<bool>(ilist);\n+                bitdeq = bitdeque_type(ilist);\n+            },\n+            [&] {\n+                // assign(initializer_list)\n+                std::initializer_list<bool> ilist{ctx.randbool(), ctx.randbool(), ctx.randbool()};\n+                deq.assign(ilist);\n+                bitdeq.assign(ilist);\n+            },\n+            [&] {\n+                // operator=(const&)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                bool val = ctx.randbool();\n+                const std::deque<bool> deq2(count, val);\n+                deq = deq2;\n+                const bitdeque_type bitdeq2(count, val);\n+                bitdeq = bitdeq2;\n+            },\n+            [&] {\n+                // operator=(&&)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                bool val = ctx.randbool();\n+                std::deque<bool> deq2(count, val);\n+                deq = std::move(deq2);\n+                bitdeque_type bitdeq2(count, val);\n+                bitdeq = std::move(bitdeq2);\n+            },\n+            [&] {\n+                // deque swap\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                auto rand_end = rand_begin + count;\n+                std::deque<bool> deq2(rand_begin, rand_end);\n+                bitdeque_type bitdeq2(rand_begin, rand_end);\n+                using std::swap;\n+                assert(deq.size() == bitdeq.size());\n+                assert(deq2.size() == bitdeq2.size());\n+                swap(deq, deq2);\n+                swap(bitdeq, bitdeq2);\n+                assert(deq.size() == bitdeq.size());\n+                assert(deq2.size() == bitdeq2.size());\n+            },\n+            [&] {\n+                // deque.swap\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                auto rand_end = rand_begin + count;\n+                std::deque<bool> deq2(rand_begin, rand_end);\n+                bitdeque_type bitdeq2(rand_begin, rand_end);\n+                assert(deq.size() == bitdeq.size());\n+                assert(deq2.size() == bitdeq2.size());\n+                deq.swap(deq2);\n+                bitdeq.swap(bitdeq2);\n+                assert(deq.size() == bitdeq.size());\n+                assert(deq2.size() == bitdeq2.size());\n+            },\n+            [&] {\n+                // operator=(initializer_list)\n+                std::initializer_list<bool> ilist{ctx.randbool(), ctx.randbool(), ctx.randbool()};\n+                deq = ilist;\n+                bitdeq = ilist;\n+            },\n+            [&] {\n+                // iterator arithmetic\n+                auto pos1 = provider.ConsumeIntegralInRange<long>(0, cdeq.size());\n+                auto pos2 = provider.ConsumeIntegralInRange<long>(0, cdeq.size());\n+                auto it = deq.begin() + pos1;\n+                auto bitit = bitdeq.begin() + pos1;\n+                if ((size_t)pos1 != cdeq.size()) assert(*it == *bitit);\n+                assert(it - deq.begin() == pos1);\n+                assert(bitit - bitdeq.begin() == pos1);\n+                if (provider.ConsumeBool()) {\n+                    it += pos2 - pos1;\n+                    bitit += pos2 - pos1;\n+                } else {\n+                    it -= pos1 - pos2;\n+                    bitit -= pos1 - pos2;\n+                }\n+                if ((size_t)pos2 != cdeq.size()) assert(*it == *bitit);\n+                assert(deq.end() - it == bitdeq.end() - bitit);\n+                if (provider.ConsumeBool()) {\n+                    if ((size_t)pos2 != cdeq.size()) {\n+                        ++it;\n+                        ++bitit;\n+                    }\n+                } else {\n+                    if (pos2 != 0) {\n+                        --it;\n+                        --bitit;\n+                    }\n+                }\n+                assert(deq.end() - it == bitdeq.end() - bitit);\n+            },\n+            [&] {\n+                // begin() and end()\n+                assert(deq.end() - deq.begin() == bitdeq.end() - bitdeq.begin());\n+            },\n+            [&] {\n+                // begin() and end() (const)\n+                assert(cdeq.end() - cdeq.begin() == cbitdeq.end() - cbitdeq.begin());\n+            },\n+            [&] {\n+                // rbegin() and rend()\n+                assert(deq.rend() - deq.rbegin() == bitdeq.rend() - bitdeq.rbegin());\n+            },\n+            [&] {\n+                // rbegin() and rend() (const)\n+                assert(cdeq.rend() - cdeq.rbegin() == cbitdeq.rend() - cbitdeq.rbegin());\n+            },\n+            [&] {\n+                // cbegin() and cend()\n+                assert(cdeq.cend() - cdeq.cbegin() == cbitdeq.cend() - cbitdeq.cbegin());\n+            },\n+            [&] {\n+                // crbegin() and crend()\n+                assert(cdeq.crend() - cdeq.crbegin() == cbitdeq.crend() - cbitdeq.crbegin());\n+            },\n+            [&] {\n+                // size\n+                assert(cdeq.size() == cbitdeq.size());\n+            },\n+            [&] {\n+                // empty\n+                assert(cdeq.empty() == cbitdeq.empty());\n+            },\n+            [&] {\n+                // at (in range) and flip\n+                if (!cdeq.empty()) {\n+                    size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 1);\n+                    auto& ref = deq.at(pos);\n+                    auto bitref = bitdeq.at(pos);\n+                    assert(ref == bitref);\n+                    if (ctx.randbool()) {\n+                        ref = !ref;\n+                        bitref.flip();\n+                    }\n+                }\n+            },\n+            [&] {\n+                // at (maybe out of range) and bit assign\n+                size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() + maxlen);\n+                bool newval = ctx.randbool();\n+                bool throw_deq{false}, throw_bitdeq{false};\n+                bool val_deq{false}, val_bitdeq{false};\n+                try {\n+                    auto& ref = deq.at(pos);\n+                    val_deq = ref;\n+                    ref = newval;\n+                } catch (const std::out_of_range&) {\n+                    throw_deq = true;\n+                }\n+                try {\n+                    auto ref = bitdeq.at(pos);\n+                    val_bitdeq = ref;\n+                    ref = newval;\n+                } catch (const std::out_of_range&) {\n+                    throw_bitdeq = true;\n+                }\n+                assert(throw_deq == throw_bitdeq);\n+                assert(throw_bitdeq == pos >= cdeq.size());\n+                if (!throw_deq) assert(val_deq == val_bitdeq);\n+            },\n+            [&] {\n+                // at (maybe out of range) (const)\n+                size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() + maxlen);\n+                bool throw_deq{false}, throw_bitdeq{false};\n+                bool val_deq{false}, val_bitdeq{false};\n+                try {\n+                    auto& ref = cdeq.at(pos);\n+                    val_deq = ref;\n+                } catch (const std::out_of_range&) {\n+                    throw_deq = true;\n+                }\n+                try {\n+                    auto ref = cbitdeq.at(pos);\n+                    val_bitdeq = ref;\n+                } catch (const std::out_of_range&) {\n+                    throw_bitdeq = true;\n+                }\n+                assert(throw_deq == throw_bitdeq);\n+                assert(throw_bitdeq == pos >= cdeq.size());\n+                if (!throw_deq) assert(val_deq == val_bitdeq);\n+            },\n+            [&] {\n+                // operator[]\n+                if (!cdeq.empty()) {\n+                    size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 1);\n+                    assert(deq[pos] == bitdeq[pos]);\n+                    if (ctx.randbool()) {\n+                        deq[pos] = !deq[pos];\n+                        bitdeq[pos].flip();\n+                    }\n+                }\n+            },\n+            [&] {\n+                // operator[] const\n+                if (!cdeq.empty()) {\n+                    size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 1);\n+                    assert(deq[pos] == bitdeq[pos]);\n+                }\n+            },\n+            [&] {\n+                // front()\n+                if (!cdeq.empty()) {\n+                    auto& ref = deq.front();\n+                    auto bitref = bitdeq.front();\n+                    assert(ref == bitref);\n+                    if (ctx.randbool()) {\n+                        ref = !ref;\n+                        bitref = !bitref;\n+                    }\n+                }\n+            },\n+            [&] {\n+                // front() const\n+                if (!cdeq.empty()) {\n+                    auto& ref = cdeq.front();\n+                    auto bitref = cbitdeq.front();\n+                    assert(ref == bitref);\n+                }\n+            },\n+            [&] {\n+                // back() and swap(bool, ref)\n+                if (!cdeq.empty()) {\n+                    auto& ref = deq.back();\n+                    auto bitref = bitdeq.back();\n+                    assert(ref == bitref);\n+                    if (ctx.randbool()) {\n+                        ref = !ref;\n+                        bitref.flip();\n+                    }\n+                }\n+            },\n+            [&] {\n+                // back() const\n+                if (!cdeq.empty()) {\n+                    const auto& cdeq = deq;\n+                    const auto& cbitdeq = bitdeq;\n+                    auto& ref = cdeq.back();\n+                    auto bitref = cbitdeq.back();\n+                    assert(ref == bitref);\n+                }\n+            },\n+            [&] {\n+                // push_back()\n+                if (cdeq.size() < limitlen) {\n+                    bool val = ctx.randbool();\n+                    if (cdeq.empty()) {\n+                        deq.push_back(val);\n+                        bitdeq.push_back(val);\n+                    } else {\n+                        size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 1);\n+                        auto& ref = deq[pos];\n+                        auto bitref = bitdeq[pos];\n+                        assert(ref == bitref);\n+                        deq.push_back(val);\n+                        bitdeq.push_back(val);\n+                        assert(ref == bitref); // references are not invalidated\n+                    }\n+                }\n+            },\n+            [&] {\n+                // push_front()\n+                if (cdeq.size() < limitlen) {\n+                    bool val = ctx.randbool();\n+                    if (cdeq.empty()) {\n+                        deq.push_front(val);\n+                        bitdeq.push_front(val);\n+                    } else {\n+                        size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 1);\n+                        auto& ref = deq[pos];\n+                        auto bitref = bitdeq[pos];\n+                        assert(ref == bitref);\n+                        deq.push_front(val);\n+                        bitdeq.push_front(val);\n+                        assert(ref == bitref); // references are not invalidated\n+                    }\n+                }\n+            },\n+            [&] {\n+                // pop_back()\n+                if (!cdeq.empty()) {\n+                    if (cdeq.size() == 1) {\n+                        deq.pop_back();\n+                        bitdeq.pop_back();\n+                    } else {\n+                        size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 2);\n+                        auto& ref = deq[pos];\n+                        auto bitref = bitdeq[pos];\n+                        assert(ref == bitref);\n+                        deq.pop_back();\n+                        bitdeq.pop_back();\n+                        assert(ref == bitref); // references to other elements are not invalidated\n+                    }\n+                }\n+            },\n+            [&] {\n+                // pop_front()\n+                if (!cdeq.empty()) {\n+                    if (cdeq.size() == 1) {\n+                        deq.pop_front();\n+                        bitdeq.pop_front();\n+                    } else {\n+                        size_t pos = provider.ConsumeIntegralInRange<size_t>(1, cdeq.size() - 1);\n+                        auto& ref = deq[pos];\n+                        auto bitref = bitdeq[pos];\n+                        assert(ref == bitref);\n+                        deq.pop_front();\n+                        bitdeq.pop_front();\n+                        assert(ref == bitref); // references to other elements are not invalidated\n+                    }\n+                }\n+            },\n+            [&] {\n+                // erase (in middle, single)\n+                if (!cdeq.empty()) {\n+                    size_t before = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 1);\n+                    size_t after = cdeq.size() - 1 - before;\n+                    auto it = deq.erase(cdeq.begin() + before);\n+                    auto bitit = bitdeq.erase(cbitdeq.begin() + before);\n+                    assert(it == cdeq.begin() + before && it == cdeq.end() - after);\n+                    assert(bitit == cbitdeq.begin() + before && bitit == cbitdeq.end() - after);\n+                }\n+            },\n+            [&] {\n+                // erase (at front, range)\n+                size_t count = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size());\n+                auto it = deq.erase(cdeq.begin(), cdeq.begin() + count);\n+                auto bitit = bitdeq.erase(cbitdeq.begin(), cbitdeq.begin() + count);\n+                assert(it == deq.begin());\n+                assert(bitit == bitdeq.begin());\n+            },\n+            [&] {\n+                // erase (at back, range)\n+                size_t count = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size());\n+                auto it = deq.erase(cdeq.end() - count, cdeq.end());\n+                auto bitit = bitdeq.erase(cbitdeq.end() - count, cbitdeq.end());\n+                assert(it == deq.end());\n+                assert(bitit == bitdeq.end());\n+            },\n+            [&] {\n+                // erase (in middle, range)\n+                size_t count = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size());\n+                size_t before = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - count);\n+                size_t after = cdeq.size() - count - before;\n+                auto it = deq.erase(cdeq.begin() + before, cdeq.end() - after);\n+                auto bitit = bitdeq.erase(cbitdeq.begin() + before, cbitdeq.end() - after);\n+                assert(it == cdeq.begin() + before && it == cdeq.end() - after);\n+                assert(bitit == cbitdeq.begin() + before && bitit == cbitdeq.end() - after);\n+            },\n+            [&] {\n+                // insert (in middle, single)\n+                if (cdeq.size() < limitlen) {\n+                    size_t before = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size());\n+                    bool val = ctx.randbool();\n+                    auto it = deq.insert(cdeq.begin() + before, val);\n+                    auto bitit = bitdeq.insert(cbitdeq.begin() + before, val);\n+                    assert(it == deq.begin() + before);\n+                    assert(bitit == bitdeq.begin() + before);\n+                }\n+            },\n+            [&] {\n+                // insert (at front, begin/end)\n+                if (cdeq.size() < limitlen) {\n+                    size_t count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                    auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                    auto rand_end = rand_begin + count;\n+                    auto it = deq.insert(cdeq.begin(), rand_begin, rand_end);\n+                    auto bitit = bitdeq.insert(cbitdeq.begin(), rand_begin, rand_end);\n+                    assert(it == cdeq.begin());\n+                    assert(bitit == cbitdeq.begin());\n+                }\n+            },\n+            [&] {\n+                // insert (at back, begin/end)\n+                if (cdeq.size() < limitlen) {\n+                    size_t count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                    auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                    auto rand_end = rand_begin + count;\n+                    auto it = deq.insert(cdeq.end(), rand_begin, rand_end);\n+                    auto bitit = bitdeq.insert(cbitdeq.end(), rand_begin, rand_end);\n+                    assert(it == cdeq.end() - count);\n+                    assert(bitit == cbitdeq.end() - count);\n+                }\n+            },\n+            [&] {\n+                // insert (in middle, range)\n+                if (cdeq.size() < limitlen) {\n+                    size_t count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                    size_t before = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size());\n+                    bool val = ctx.randbool();\n+                    auto it = deq.insert(cdeq.begin() + before, count, val);\n+                    auto bitit = bitdeq.insert(cbitdeq.begin() + before, count, val);\n+                    assert(it == deq.begin() + before);\n+                    assert(bitit == bitdeq.begin() + before);\n+                }\n+            },\n+            [&] {\n+                // insert (in middle, begin/end)\n+                if (cdeq.size() < limitlen) {\n+                    size_t count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                    size_t before = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size());\n+                    auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                    auto rand_end = rand_begin + count;\n+                    auto it = deq.insert(cdeq.begin() + before, rand_begin, rand_end);\n+                    auto bitit = bitdeq.insert(cbitdeq.begin() + before, rand_begin, rand_end);\n+                    assert(it == deq.begin() + before);\n+                    assert(bitit == bitdeq.begin() + before);\n+                }",
      "path": "src/test/fuzz/bitdeque.cpp",
      "position": 537,
      "original_position": 523,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5072054428fa9229a0e34b9eb4a0eed97639012d",
      "in_reply_to_id": 933239940,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "@glozow @sdaftuar Added a commit that adds these to the fuzzer in https://github.com/sipa/bitcoin/commits/pr25717",
      "created_at": "2022-08-01T21:28:42Z",
      "updated_at": "2022-08-01T21:30:45Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r934933976",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934933976"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 537,
      "original_line": 537,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934934877",
      "pull_request_review_id": 1055897986,
      "id": 934934877,
      "node_id": "PRRC_kwDOABII5843ufld",
      "diff_hunk": "@@ -20,4 +20,18 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n /** Check whether a block hash satisfies the proof-of-work requirement specified by nBits */\n bool CheckProofOfWork(uint256 hash, unsigned int nBits, const Consensus::Params&);\n \n+/**\n+ * Return false if the proof-of-work requirement specified by new_nbits at a\n+ * given height is not possible, given the proof-of-work on the prior block as\n+ * specified by old_nbits.\n+ *\n+ * This function only checks that the new value is within a factor of 4 of the\n+ * old value for blocks at the difficulty adjustment interval, and otherwise\n+ * requires the values to be the same.\n+ *\n+ * Always returns true on networks where min difficulty blocks are allowed,\n+ * such as regtest/testnet.",
      "path": "src/pow.h",
      "position": 14,
      "original_position": 14,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "130d838df0b02f1115cb54e62002a95823d5efcb",
      "in_reply_to_id": 934681520,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "@glozow No, on non-difficulty-adjustment blocks it returns whether the difficulty changed.",
      "created_at": "2022-08-01T21:30:21Z",
      "updated_at": "2022-08-01T21:30:45Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r934934877",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/934934877"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": 32,
      "original_start_line": 32,
      "start_side": "RIGHT",
      "line": 33,
      "original_line": 33,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/935289113",
      "pull_request_review_id": 1058361971,
      "id": 935289113,
      "node_id": "PRRC_kwDOABII5843v2EZ",
      "diff_hunk": "@@ -0,0 +1,528 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <util/bitdeque.h>\n+\n+#include <random.h>\n+#include <test/fuzz/FuzzedDataProvider.h>\n+#include <test/fuzz/util.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+namespace {\n+\n+constexpr int LEN_BITS = 16;\n+constexpr int RANDDATA_BITS = 20;\n+\n+using bitdeque_type = bitdeque<128>;",
      "path": "src/test/fuzz/bitdeque.cpp",
      "position": 19,
      "original_position": 19,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5072054428fa9229a0e34b9eb4a0eed97639012d",
      "in_reply_to_id": 933235315,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Ah, that makes sense to me - thanks!",
      "created_at": "2022-08-02T08:50:33Z",
      "updated_at": "2022-08-02T08:50:33Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r935289113",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/935289113"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 19,
      "original_line": 19,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/935926734",
      "pull_request_review_id": 1059273476,
      "id": 935926734,
      "node_id": "PRRC_kwDOABII5843yRvO",
      "diff_hunk": "@@ -32,7 +34,9 @@ BOOST_AUTO_TEST_CASE(get_next_work_pow_limit)\n     pindexLast.nHeight = 2015;\n     pindexLast.nTime = 1233061996;  // Block #2015\n     pindexLast.nBits = 0x1d00ffff;\n-    BOOST_CHECK_EQUAL(CalculateNextWorkRequired(&pindexLast, nLastRetargetTime, chainParams->GetConsensus()), 0x1d00ffffU);\n+    unsigned int expected_nbits = 0x1d00ffffU;",
      "path": "src/test/pow_tests.cpp",
      "position": 21,
      "original_position": 16,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cbf3069c7d76888a647d68dbb09d77e666dcaf50",
      "in_reply_to_id": 932562089,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Well, it could be, but the test would still be correct if `pindexLast.nBits` were modified to be within a factor of 4 from the max difficulty target.\r\n\r\nSo it seems to me that having `expected_nbts = 0x1d00ffffU` is the more important line as it captures exactly what the test case is trying to exercise here (and `pindexLast.nBits` could be set from that, or not).",
      "created_at": "2022-08-02T18:54:58Z",
      "updated_at": "2022-08-02T18:54:58Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r935926734",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/935926734"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 42,
      "original_line": 42,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936040537",
      "pull_request_review_id": 1059447041,
      "id": 936040537,
      "node_id": "PRRC_kwDOABII5843ythZ",
      "diff_hunk": "@@ -0,0 +1,68 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+import time\n+\n+NODE1_BLOCKS_REQUIRED = 16\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 16 blocks; node2 requires 2048 blocks\n+        self.extra_args = [[\"-minimumchainwork=0x0\"], [\"-minimumchainwork=0x1f\"], [\"-minimumchainwork=0x1000\"]]\n+\n+    def skip_test_if_missing_module(self):\n+        self.skip_if_no_wallet()\n+\n+    def test_chains_sync_when_long_enough(self):\n+        self.log.info(\"Generate blocks on the node with no required chainwork\")\n+        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED-2, sync_fun=self.no_op)\n+\n+        self.log.info(\"Verify nodes 1 and 2 have no new headers in their headers tree\")\n+        time.sleep(2)",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 30,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": 932281150,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Marking this as closed, I believe this is addressed now with the improved logging and the use of `assert_debug_log`.",
      "created_at": "2022-08-02T21:26:09Z",
      "updated_at": "2022-08-02T21:26:09Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936040537",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936040537"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 27,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 30,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936040744",
      "pull_request_review_id": 1059447322,
      "id": 936040744,
      "node_id": "PRRC_kwDOABII5843ytko",
      "diff_hunk": "@@ -0,0 +1,68 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+import time\n+\n+NODE1_BLOCKS_REQUIRED = 16\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 16 blocks; node2 requires 2048 blocks\n+        self.extra_args = [[\"-minimumchainwork=0x0\"], [\"-minimumchainwork=0x1f\"], [\"-minimumchainwork=0x1000\"]]\n+\n+    def skip_test_if_missing_module(self):\n+        self.skip_if_no_wallet()\n+\n+    def test_chains_sync_when_long_enough(self):\n+        self.log.info(\"Generate blocks on the node with no required chainwork\")\n+        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED-2, sync_fun=self.no_op)\n+\n+        self.log.info(\"Verify nodes 1 and 2 have no new headers in their headers tree\")\n+        time.sleep(2)\n+        for i in range(1, 2):\n+            chaintips = self.nodes[i].getchaintips()\n+            assert(len(chaintips) == 1)\n+            assert {\n+                'height': 0,\n+                'hash': '0f9188f13cb7b2c71f2a335e3a4fc328bf5beb436012afca590b1a11466e2206',\n+                'branchlen': 0,\n+                'status': 'active',\n+            } in chaintips\n+\n+        self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED - self.nodes[0].getblockcount(), sync_fun=self.no_op)\n+        self.log.info(\"Verify that node1 syncs node0's chain\")\n+        time.sleep(2)\n+        self.sync_blocks(self.nodes[0:2])\n+\n+        self.log.info(\"Verify that node2 still has no new headers\")\n+        time.sleep(2)",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 47,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "7bed25cbd3776189d47f3771ab3abc180ff34077",
      "in_reply_to_id": 932288256,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Also fixed now.",
      "created_at": "2022-08-02T21:26:25Z",
      "updated_at": "2022-08-02T21:26:26Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936040744",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936040744"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 46,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936040926",
      "pull_request_review_id": 1059447541,
      "id": 936040926,
      "node_id": "PRRC_kwDOABII5843ytne",
      "diff_hunk": "@@ -71,6 +71,54 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n     return bnNew.GetCompact();\n }\n \n+// Check that on difficulty adjustments, the new difficulty does not increase\n+// or decrease beyond the permitted limits.\n+bool PermittedDifficultyTransition(const Consensus::Params& params, int64_t height, uint32_t old_nbits, uint32_t new_nbits)\n+{\n+    if (params.fPowAllowMinDifficultyBlocks) return true;\n+\n+    if (height % params.DifficultyAdjustmentInterval() == 0) {\n+        int64_t smallest_timespan = params.nPowTargetTimespan/4;\n+        int64_t largest_timespan = params.nPowTargetTimespan*4;\n+\n+        const arith_uint256 bnPowLimit = UintToArith256(params.powLimit);\n+        arith_uint256 observed_new_target;\n+        observed_new_target.SetCompact(new_nbits);\n+\n+        // Calculate the largest difficulty value possible:\n+        arith_uint256 bnNew;\n+        bnNew.SetCompact(old_nbits);\n+        bnNew *= largest_timespan;\n+        bnNew /= params.nPowTargetTimespan;\n+\n+        if (bnNew > bnPowLimit)\n+            bnNew = bnPowLimit;",
      "path": "src/pow.cpp",
      "position": null,
      "original_position": 25,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cbf3069c7d76888a647d68dbb09d77e666dcaf50",
      "in_reply_to_id": 932235084,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-02T21:26:39Z",
      "updated_at": "2022-08-02T21:26:39Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936040926",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936040926"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 95,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936041037",
      "pull_request_review_id": 1059447703,
      "id": 936041037,
      "node_id": "PRRC_kwDOABII5843ytpN",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 47,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": 932272722,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-02T21:26:50Z",
      "updated_at": "2022-08-02T21:26:50Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936041037",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936041037"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 50,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936041725",
      "pull_request_review_id": 1059448614,
      "id": 936041725,
      "node_id": "PRRC_kwDOABII5843ytz9",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <util/check.h>\n+#include <pow.h>\n+#include <timedata.h>\n+\n+// Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+// Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+// Free any memory in use, and mark this object as no longer usable.\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+// Initialize the parameters for this headers download, validate this first\n+// batch, and request more headers.\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    // Overestimate the number of blocks that could possibly exist on this\n+    // chain using 6 blocks/second (fastest blockrate given the MTP rule) times\n+    // the number of seconds from the last allowed block until today. This\n+    // serves as a memory bound on how many commitments we might store from\n+    // this peer, and we can safely give up syncing if the peer exceeds this\n+    // bound, because it's not possible for a consensus-valid chain to be\n+    // longer than this.\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+// Process the next batch of headers received from our peer.\n+// Validate and store commitments, and compare total chainwork to our target to\n+// see if we can switch to REDOWNLOAD mode.\n+std::optional<CBlockLocator> HeadersSyncState::ProcessNextHeaders(const std::vector<CBlockHeader>&\n+        headers, bool full_headers_message, std::vector<CBlockHeader>&\n+        headers_to_process, bool& processing_success)\n+{\n+    processing_success = false;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return std::nullopt;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return std::nullopt;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+            // Somehow our peer gave us a header that doesn't connect.\n+            // This might be benign -- perhaps we issued an extra getheaders\n+            // message, such as after a block INV was received.\n+            // Or it could be that our peer is broken or malicious. If broken,\n+            // sending a new getheaders immediately could trigger an infinite\n+            // loop. Just give up for now; if our peer ever gives us an block\n+            // INV later we will fetch headers then, and likely retrigger this\n+            // logic.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return std::nullopt;\n+        }\n+        processing_success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return std::nullopt;\n+            }\n+        }\n+\n+        processing_success = true;\n+        // Return any headers that are ready for acceptance.\n+        headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return std::nullopt;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            return MakeNextHeadersRequest();\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return std::nullopt;\n+        }\n+    }\n+    return std::nullopt;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (m_last_header_received.IsNull()) {\n+        m_last_header_received = m_chain_start->GetBlockHeader();\n+        m_current_height = m_chain_start->nHeight;\n+    }\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Doesn't connect to what we expect\n+        Finalize();\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr, m_current_height+1)) {\n+            Finalize();\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_blockhash_with_sufficient_work = headers.back().GetHash();\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash.SetNull();\n+        m_download_state = State::REDOWNLOAD;\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current, int64_t current_height)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, current_height,\n+                previous.nBits, current.nBits)) {\n+        return false;\n+    }\n+\n+    if (!CheckProofOfWork(current.GetHash(), current.nBits, m_consensus_params)) return false;\n+\n+    if ((current_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+        // Try to add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // TODO: disconnect this peer.\n+            LogPrint(BCLog::NET, \"headers chain is too long; giving up sync peer=%d\\n\", m_id);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));\n+    m_last_header_received = current;\n+    ++m_current_height;\n+\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreRedownloadedHeader(const CBlockHeader& header)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Ensure that we're working on a header that connects to the chain we're\n+    // downloading.\n+    if (m_redownloaded_headers.empty()) {\n+        if (header.hashPrevBlock != m_chain_start->GetBlockHash()) {\n+            return false;\n+        }\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+    } else if (header.hashPrevBlock != m_redownload_buffer_last_hash) {\n+        return false;\n+    }\n+\n+    int64_t next_height = m_redownload_buffer_last_height + 1;\n+\n+    // If we're at a header for which we previously stored a commitment, verify\n+    // it is correct. Failure will result in aborting download.\n+    if ((next_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+         bool commitment = m_hasher(header.GetHash()) & 1;\n+         if (m_header_commitments.size() == 0) {\n+            // Somehow our peer managed to feed us a different chain and\n+            // we've run out of commitments.\n+            return false;\n+        }\n+        bool expected_commitment = m_header_commitments.front();\n+        m_header_commitments.pop_front();\n+        if (commitment != expected_commitment) {\n+            return false;\n+        }\n+    }\n+\n+    // Store this header for later processing.\n+    m_redownloaded_headers.push_back(header);\n+    m_redownload_buffer_last_height = next_height;\n+    m_redownload_buffer_last_hash = header.GetHash();\n+\n+    // If we're processing our target block header, which we verified has",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 272,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "de72d0867dbcbab6e2f68a73c48e99497c5efd7a",
      "in_reply_to_id": 932308150,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I added a commit that tries to improve this.  Left it unsquashed for now so that it's easier to review; I don't believe there are any material DoS risks introduced by this change but just want to make sure we get enough eyes on this logic.",
      "created_at": "2022-08-02T21:27:51Z",
      "updated_at": "2022-08-02T21:27:51Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936041725",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936041725"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 272,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936042096",
      "pull_request_review_id": 1059449206,
      "id": 936042096,
      "node_id": "PRRC_kwDOABII5843yt5w",
      "diff_hunk": "@@ -44,7 +48,12 @@ BOOST_AUTO_TEST_CASE(get_next_work_lower_limit_actual)\n     pindexLast.nHeight = 68543;\n     pindexLast.nTime = 1279297671;  // Block #68543\n     pindexLast.nBits = 0x1c05a3f4;\n-    BOOST_CHECK_EQUAL(CalculateNextWorkRequired(&pindexLast, nLastRetargetTime, chainParams->GetConsensus()), 0x1c0168fdU);\n+    unsigned int expected_nbits = 0x1c0168fdU;",
      "path": "src/test/pow_tests.cpp",
      "position": 32,
      "original_position": 27,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cbf3069c7d76888a647d68dbb09d77e666dcaf50",
      "in_reply_to_id": 932560137,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Added a comment, let me know if this is what you had in mind (or please suggest some other language?)",
      "created_at": "2022-08-02T21:28:30Z",
      "updated_at": "2022-08-02T21:28:30Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936042096",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936042096"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 56,
      "original_line": 56,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936042242",
      "pull_request_review_id": 1059449397,
      "id": 936042242,
      "node_id": "PRRC_kwDOABII5843yt8C",
      "diff_hunk": "@@ -71,6 +71,54 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n     return bnNew.GetCompact();\n }\n \n+// Check that on difficulty adjustments, the new difficulty does not increase\n+// or decrease beyond the permitted limits.\n+bool PermittedDifficultyTransition(const Consensus::Params& params, int64_t height, uint32_t old_nbits, uint32_t new_nbits)\n+{\n+    if (params.fPowAllowMinDifficultyBlocks) return true;\n+\n+    if (height % params.DifficultyAdjustmentInterval() == 0) {\n+        int64_t smallest_timespan = params.nPowTargetTimespan/4;\n+        int64_t largest_timespan = params.nPowTargetTimespan*4;\n+\n+        const arith_uint256 bnPowLimit = UintToArith256(params.powLimit);",
      "path": "src/pow.cpp",
      "position": null,
      "original_position": 14,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cbf3069c7d76888a647d68dbb09d77e666dcaf50",
      "in_reply_to_id": 932564292,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-02T21:28:43Z",
      "updated_at": "2022-08-02T21:28:43Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936042242",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936042242"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 84,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936043151",
      "pull_request_review_id": 1059450674,
      "id": 936043151,
      "node_id": "PRRC_kwDOABII5843yuKP",
      "diff_hunk": "@@ -0,0 +1,528 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <util/bitdeque.h>\n+\n+#include <random.h>\n+#include <test/fuzz/FuzzedDataProvider.h>\n+#include <test/fuzz/util.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+namespace {\n+\n+constexpr int LEN_BITS = 16;\n+constexpr int RANDDATA_BITS = 20;\n+\n+using bitdeque_type = bitdeque<128>;\n+\n+//! Deterministic random vector of bools, for begin/end insertions to draw from.\n+std::vector<bool> RANDDATA;\n+\n+void InitRandData()\n+{\n+    FastRandomContext ctx(true);\n+    RANDDATA.clear();\n+    for (size_t i = 0; i < (1U << RANDDATA_BITS) + (1U << LEN_BITS); ++i) {\n+        RANDDATA.push_back(ctx.randbool());\n+    }\n+}\n+\n+} // namespace\n+\n+FUZZ_TARGET_INIT(bitdeque, InitRandData)\n+{\n+    FuzzedDataProvider provider(buffer.data(), buffer.size());\n+    FastRandomContext ctx(true);\n+\n+    size_t maxlen = (1U << provider.ConsumeIntegralInRange<size_t>(0, LEN_BITS)) - 1;\n+    size_t limitlen = 4 * maxlen;\n+\n+    std::deque<bool> deq;\n+    bitdeque_type bitdeq;\n+\n+    const auto& cdeq = deq;\n+    const auto& cbitdeq = bitdeq;\n+\n+    size_t initlen = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+    while (initlen) {\n+        bool val = ctx.randbool();\n+        deq.push_back(val);\n+        bitdeq.push_back(val);\n+        --initlen;\n+    }\n+\n+    while (provider.remaining_bytes()) {\n+        {\n+            assert(deq.size() == bitdeq.size());\n+            auto it = deq.begin();\n+            auto bitit = bitdeq.begin();\n+            auto itend = deq.end();\n+            while (it != itend) {\n+                assert(*it == *bitit);\n+                ++it;\n+                ++bitit;\n+            }\n+        }\n+\n+        CallOneOf(provider,\n+            [&] {\n+                // constructor()\n+                deq = std::deque<bool>{};\n+                bitdeq = bitdeque_type{};\n+            },\n+            [&] {\n+                // assign(count, val)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                bool val = ctx.randbool();\n+                deq.assign(count, val);\n+                bitdeq.assign(count, val);\n+            },\n+            [&] {\n+                // constructor(count, val)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                bool val = ctx.randbool();\n+                deq = std::deque<bool>(count, val);\n+                bitdeq = bitdeque_type(count, val);\n+            },\n+            [&] {\n+                // constructor(count)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                deq = std::deque<bool>(count);\n+                bitdeq = bitdeque_type(count);\n+            },\n+            [&] {\n+                // construct(begin, end)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                auto rand_end = rand_begin + count;\n+                deq = std::deque<bool>(rand_begin, rand_end);\n+                bitdeq = bitdeque_type(rand_begin, rand_end);\n+            },\n+            [&] {\n+                // assign(begin, end)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                auto rand_end = rand_begin + count;\n+                deq.assign(rand_begin, rand_end);\n+                bitdeq.assign(rand_begin, rand_end);\n+            },\n+            [&] {\n+                // construct(initializer_list)\n+                std::initializer_list<bool> ilist{ctx.randbool(), ctx.randbool(), ctx.randbool(), ctx.randbool(), ctx.randbool()};\n+                deq = std::deque<bool>(ilist);\n+                bitdeq = bitdeque_type(ilist);\n+            },\n+            [&] {\n+                // assign(initializer_list)\n+                std::initializer_list<bool> ilist{ctx.randbool(), ctx.randbool(), ctx.randbool()};\n+                deq.assign(ilist);\n+                bitdeq.assign(ilist);\n+            },\n+            [&] {\n+                // operator=(const&)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                bool val = ctx.randbool();\n+                const std::deque<bool> deq2(count, val);\n+                deq = deq2;\n+                const bitdeque_type bitdeq2(count, val);\n+                bitdeq = bitdeq2;\n+            },\n+            [&] {\n+                // operator=(&&)\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                bool val = ctx.randbool();\n+                std::deque<bool> deq2(count, val);\n+                deq = std::move(deq2);\n+                bitdeque_type bitdeq2(count, val);\n+                bitdeq = std::move(bitdeq2);\n+            },\n+            [&] {\n+                // deque swap\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                auto rand_end = rand_begin + count;\n+                std::deque<bool> deq2(rand_begin, rand_end);\n+                bitdeque_type bitdeq2(rand_begin, rand_end);\n+                using std::swap;\n+                assert(deq.size() == bitdeq.size());\n+                assert(deq2.size() == bitdeq2.size());\n+                swap(deq, deq2);\n+                swap(bitdeq, bitdeq2);\n+                assert(deq.size() == bitdeq.size());\n+                assert(deq2.size() == bitdeq2.size());\n+            },\n+            [&] {\n+                // deque.swap\n+                auto count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                auto rand_end = rand_begin + count;\n+                std::deque<bool> deq2(rand_begin, rand_end);\n+                bitdeque_type bitdeq2(rand_begin, rand_end);\n+                assert(deq.size() == bitdeq.size());\n+                assert(deq2.size() == bitdeq2.size());\n+                deq.swap(deq2);\n+                bitdeq.swap(bitdeq2);\n+                assert(deq.size() == bitdeq.size());\n+                assert(deq2.size() == bitdeq2.size());\n+            },\n+            [&] {\n+                // operator=(initializer_list)\n+                std::initializer_list<bool> ilist{ctx.randbool(), ctx.randbool(), ctx.randbool()};\n+                deq = ilist;\n+                bitdeq = ilist;\n+            },\n+            [&] {\n+                // iterator arithmetic\n+                auto pos1 = provider.ConsumeIntegralInRange<long>(0, cdeq.size());\n+                auto pos2 = provider.ConsumeIntegralInRange<long>(0, cdeq.size());\n+                auto it = deq.begin() + pos1;\n+                auto bitit = bitdeq.begin() + pos1;\n+                if ((size_t)pos1 != cdeq.size()) assert(*it == *bitit);\n+                assert(it - deq.begin() == pos1);\n+                assert(bitit - bitdeq.begin() == pos1);\n+                if (provider.ConsumeBool()) {\n+                    it += pos2 - pos1;\n+                    bitit += pos2 - pos1;\n+                } else {\n+                    it -= pos1 - pos2;\n+                    bitit -= pos1 - pos2;\n+                }\n+                if ((size_t)pos2 != cdeq.size()) assert(*it == *bitit);\n+                assert(deq.end() - it == bitdeq.end() - bitit);\n+                if (provider.ConsumeBool()) {\n+                    if ((size_t)pos2 != cdeq.size()) {\n+                        ++it;\n+                        ++bitit;\n+                    }\n+                } else {\n+                    if (pos2 != 0) {\n+                        --it;\n+                        --bitit;\n+                    }\n+                }\n+                assert(deq.end() - it == bitdeq.end() - bitit);\n+            },\n+            [&] {\n+                // begin() and end()\n+                assert(deq.end() - deq.begin() == bitdeq.end() - bitdeq.begin());\n+            },\n+            [&] {\n+                // begin() and end() (const)\n+                assert(cdeq.end() - cdeq.begin() == cbitdeq.end() - cbitdeq.begin());\n+            },\n+            [&] {\n+                // rbegin() and rend()\n+                assert(deq.rend() - deq.rbegin() == bitdeq.rend() - bitdeq.rbegin());\n+            },\n+            [&] {\n+                // rbegin() and rend() (const)\n+                assert(cdeq.rend() - cdeq.rbegin() == cbitdeq.rend() - cbitdeq.rbegin());\n+            },\n+            [&] {\n+                // cbegin() and cend()\n+                assert(cdeq.cend() - cdeq.cbegin() == cbitdeq.cend() - cbitdeq.cbegin());\n+            },\n+            [&] {\n+                // crbegin() and crend()\n+                assert(cdeq.crend() - cdeq.crbegin() == cbitdeq.crend() - cbitdeq.crbegin());\n+            },\n+            [&] {\n+                // size\n+                assert(cdeq.size() == cbitdeq.size());\n+            },\n+            [&] {\n+                // empty\n+                assert(cdeq.empty() == cbitdeq.empty());\n+            },\n+            [&] {\n+                // at (in range) and flip\n+                if (!cdeq.empty()) {\n+                    size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 1);\n+                    auto& ref = deq.at(pos);\n+                    auto bitref = bitdeq.at(pos);\n+                    assert(ref == bitref);\n+                    if (ctx.randbool()) {\n+                        ref = !ref;\n+                        bitref.flip();\n+                    }\n+                }\n+            },\n+            [&] {\n+                // at (maybe out of range) and bit assign\n+                size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() + maxlen);\n+                bool newval = ctx.randbool();\n+                bool throw_deq{false}, throw_bitdeq{false};\n+                bool val_deq{false}, val_bitdeq{false};\n+                try {\n+                    auto& ref = deq.at(pos);\n+                    val_deq = ref;\n+                    ref = newval;\n+                } catch (const std::out_of_range&) {\n+                    throw_deq = true;\n+                }\n+                try {\n+                    auto ref = bitdeq.at(pos);\n+                    val_bitdeq = ref;\n+                    ref = newval;\n+                } catch (const std::out_of_range&) {\n+                    throw_bitdeq = true;\n+                }\n+                assert(throw_deq == throw_bitdeq);\n+                assert(throw_bitdeq == pos >= cdeq.size());\n+                if (!throw_deq) assert(val_deq == val_bitdeq);\n+            },\n+            [&] {\n+                // at (maybe out of range) (const)\n+                size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() + maxlen);\n+                bool throw_deq{false}, throw_bitdeq{false};\n+                bool val_deq{false}, val_bitdeq{false};\n+                try {\n+                    auto& ref = cdeq.at(pos);\n+                    val_deq = ref;\n+                } catch (const std::out_of_range&) {\n+                    throw_deq = true;\n+                }\n+                try {\n+                    auto ref = cbitdeq.at(pos);\n+                    val_bitdeq = ref;\n+                } catch (const std::out_of_range&) {\n+                    throw_bitdeq = true;\n+                }\n+                assert(throw_deq == throw_bitdeq);\n+                assert(throw_bitdeq == pos >= cdeq.size());\n+                if (!throw_deq) assert(val_deq == val_bitdeq);\n+            },\n+            [&] {\n+                // operator[]\n+                if (!cdeq.empty()) {\n+                    size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 1);\n+                    assert(deq[pos] == bitdeq[pos]);\n+                    if (ctx.randbool()) {\n+                        deq[pos] = !deq[pos];\n+                        bitdeq[pos].flip();\n+                    }\n+                }\n+            },\n+            [&] {\n+                // operator[] const\n+                if (!cdeq.empty()) {\n+                    size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 1);\n+                    assert(deq[pos] == bitdeq[pos]);\n+                }\n+            },\n+            [&] {\n+                // front()\n+                if (!cdeq.empty()) {\n+                    auto& ref = deq.front();\n+                    auto bitref = bitdeq.front();\n+                    assert(ref == bitref);\n+                    if (ctx.randbool()) {\n+                        ref = !ref;\n+                        bitref = !bitref;\n+                    }\n+                }\n+            },\n+            [&] {\n+                // front() const\n+                if (!cdeq.empty()) {\n+                    auto& ref = cdeq.front();\n+                    auto bitref = cbitdeq.front();\n+                    assert(ref == bitref);\n+                }\n+            },\n+            [&] {\n+                // back() and swap(bool, ref)\n+                if (!cdeq.empty()) {\n+                    auto& ref = deq.back();\n+                    auto bitref = bitdeq.back();\n+                    assert(ref == bitref);\n+                    if (ctx.randbool()) {\n+                        ref = !ref;\n+                        bitref.flip();\n+                    }\n+                }\n+            },\n+            [&] {\n+                // back() const\n+                if (!cdeq.empty()) {\n+                    const auto& cdeq = deq;\n+                    const auto& cbitdeq = bitdeq;\n+                    auto& ref = cdeq.back();\n+                    auto bitref = cbitdeq.back();\n+                    assert(ref == bitref);\n+                }\n+            },\n+            [&] {\n+                // push_back()\n+                if (cdeq.size() < limitlen) {\n+                    bool val = ctx.randbool();\n+                    if (cdeq.empty()) {\n+                        deq.push_back(val);\n+                        bitdeq.push_back(val);\n+                    } else {\n+                        size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 1);\n+                        auto& ref = deq[pos];\n+                        auto bitref = bitdeq[pos];\n+                        assert(ref == bitref);\n+                        deq.push_back(val);\n+                        bitdeq.push_back(val);\n+                        assert(ref == bitref); // references are not invalidated\n+                    }\n+                }\n+            },\n+            [&] {\n+                // push_front()\n+                if (cdeq.size() < limitlen) {\n+                    bool val = ctx.randbool();\n+                    if (cdeq.empty()) {\n+                        deq.push_front(val);\n+                        bitdeq.push_front(val);\n+                    } else {\n+                        size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 1);\n+                        auto& ref = deq[pos];\n+                        auto bitref = bitdeq[pos];\n+                        assert(ref == bitref);\n+                        deq.push_front(val);\n+                        bitdeq.push_front(val);\n+                        assert(ref == bitref); // references are not invalidated\n+                    }\n+                }\n+            },\n+            [&] {\n+                // pop_back()\n+                if (!cdeq.empty()) {\n+                    if (cdeq.size() == 1) {\n+                        deq.pop_back();\n+                        bitdeq.pop_back();\n+                    } else {\n+                        size_t pos = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 2);\n+                        auto& ref = deq[pos];\n+                        auto bitref = bitdeq[pos];\n+                        assert(ref == bitref);\n+                        deq.pop_back();\n+                        bitdeq.pop_back();\n+                        assert(ref == bitref); // references to other elements are not invalidated\n+                    }\n+                }\n+            },\n+            [&] {\n+                // pop_front()\n+                if (!cdeq.empty()) {\n+                    if (cdeq.size() == 1) {\n+                        deq.pop_front();\n+                        bitdeq.pop_front();\n+                    } else {\n+                        size_t pos = provider.ConsumeIntegralInRange<size_t>(1, cdeq.size() - 1);\n+                        auto& ref = deq[pos];\n+                        auto bitref = bitdeq[pos];\n+                        assert(ref == bitref);\n+                        deq.pop_front();\n+                        bitdeq.pop_front();\n+                        assert(ref == bitref); // references to other elements are not invalidated\n+                    }\n+                }\n+            },\n+            [&] {\n+                // erase (in middle, single)\n+                if (!cdeq.empty()) {\n+                    size_t before = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - 1);\n+                    size_t after = cdeq.size() - 1 - before;\n+                    auto it = deq.erase(cdeq.begin() + before);\n+                    auto bitit = bitdeq.erase(cbitdeq.begin() + before);\n+                    assert(it == cdeq.begin() + before && it == cdeq.end() - after);\n+                    assert(bitit == cbitdeq.begin() + before && bitit == cbitdeq.end() - after);\n+                }\n+            },\n+            [&] {\n+                // erase (at front, range)\n+                size_t count = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size());\n+                auto it = deq.erase(cdeq.begin(), cdeq.begin() + count);\n+                auto bitit = bitdeq.erase(cbitdeq.begin(), cbitdeq.begin() + count);\n+                assert(it == deq.begin());\n+                assert(bitit == bitdeq.begin());\n+            },\n+            [&] {\n+                // erase (at back, range)\n+                size_t count = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size());\n+                auto it = deq.erase(cdeq.end() - count, cdeq.end());\n+                auto bitit = bitdeq.erase(cbitdeq.end() - count, cbitdeq.end());\n+                assert(it == deq.end());\n+                assert(bitit == bitdeq.end());\n+            },\n+            [&] {\n+                // erase (in middle, range)\n+                size_t count = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size());\n+                size_t before = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size() - count);\n+                size_t after = cdeq.size() - count - before;\n+                auto it = deq.erase(cdeq.begin() + before, cdeq.end() - after);\n+                auto bitit = bitdeq.erase(cbitdeq.begin() + before, cbitdeq.end() - after);\n+                assert(it == cdeq.begin() + before && it == cdeq.end() - after);\n+                assert(bitit == cbitdeq.begin() + before && bitit == cbitdeq.end() - after);\n+            },\n+            [&] {\n+                // insert (in middle, single)\n+                if (cdeq.size() < limitlen) {\n+                    size_t before = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size());\n+                    bool val = ctx.randbool();\n+                    auto it = deq.insert(cdeq.begin() + before, val);\n+                    auto bitit = bitdeq.insert(cbitdeq.begin() + before, val);\n+                    assert(it == deq.begin() + before);\n+                    assert(bitit == bitdeq.begin() + before);\n+                }\n+            },\n+            [&] {\n+                // insert (at front, begin/end)\n+                if (cdeq.size() < limitlen) {\n+                    size_t count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                    auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                    auto rand_end = rand_begin + count;\n+                    auto it = deq.insert(cdeq.begin(), rand_begin, rand_end);\n+                    auto bitit = bitdeq.insert(cbitdeq.begin(), rand_begin, rand_end);\n+                    assert(it == cdeq.begin());\n+                    assert(bitit == cbitdeq.begin());\n+                }\n+            },\n+            [&] {\n+                // insert (at back, begin/end)\n+                if (cdeq.size() < limitlen) {\n+                    size_t count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                    auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                    auto rand_end = rand_begin + count;\n+                    auto it = deq.insert(cdeq.end(), rand_begin, rand_end);\n+                    auto bitit = bitdeq.insert(cbitdeq.end(), rand_begin, rand_end);\n+                    assert(it == cdeq.end() - count);\n+                    assert(bitit == cbitdeq.end() - count);\n+                }\n+            },\n+            [&] {\n+                // insert (in middle, range)\n+                if (cdeq.size() < limitlen) {\n+                    size_t count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                    size_t before = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size());\n+                    bool val = ctx.randbool();\n+                    auto it = deq.insert(cdeq.begin() + before, count, val);\n+                    auto bitit = bitdeq.insert(cbitdeq.begin() + before, count, val);\n+                    assert(it == deq.begin() + before);\n+                    assert(bitit == bitdeq.begin() + before);\n+                }\n+            },\n+            [&] {\n+                // insert (in middle, begin/end)\n+                if (cdeq.size() < limitlen) {\n+                    size_t count = provider.ConsumeIntegralInRange<size_t>(0, maxlen);\n+                    size_t before = provider.ConsumeIntegralInRange<size_t>(0, cdeq.size());\n+                    auto rand_begin = RANDDATA.begin() + ctx.randbits(RANDDATA_BITS);\n+                    auto rand_end = rand_begin + count;\n+                    auto it = deq.insert(cdeq.begin() + before, rand_begin, rand_end);\n+                    auto bitit = bitdeq.insert(cbitdeq.begin() + before, rand_begin, rand_end);\n+                    assert(it == deq.begin() + before);\n+                    assert(bitit == bitdeq.begin() + before);\n+                }",
      "path": "src/test/fuzz/bitdeque.cpp",
      "position": 537,
      "original_position": 523,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5072054428fa9229a0e34b9eb4a0eed97639012d",
      "in_reply_to_id": 933239940,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Thanks, cherry-picked here and squashed.",
      "created_at": "2022-08-02T21:30:11Z",
      "updated_at": "2022-08-02T21:30:11Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936043151",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936043151"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 537,
      "original_line": 537,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936043281",
      "pull_request_review_id": 1059450835,
      "id": 936043281,
      "node_id": "PRRC_kwDOABII5843yuMR",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+//! Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 10,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "fe620e1a1fb2ef11d06cf941249ad4e5e4250bbf",
      "in_reply_to_id": 934656740,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Thanks, done.",
      "created_at": "2022-08-02T21:30:22Z",
      "updated_at": "2022-08-02T21:30:22Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936043281",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936043281"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 10,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936043450",
      "pull_request_review_id": 1059451087,
      "id": 936043450,
      "node_id": "PRRC_kwDOABII5843yuO6",
      "diff_hunk": "@@ -0,0 +1,66 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+NODE1_BLOCKS_REQUIRED = 15\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 15 blocks on top of the genesis block; node2 requires 2047\n+        self.extra_args = [[\"-minimumchainwork=0x0\"], [\"-minimumchainwork=0x1f\"], [\"-minimumchainwork=0x1000\"]]\n+\n+    def setup_network(self):\n+        self.setup_nodes()\n+        self.connect_nodes(0, 1)\n+        self.connect_nodes(0, 2)\n+        self.sync_all(self.nodes[0:2])",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 24,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "e3260960add016aeb43d0be4c5b735418e37c4a7",
      "in_reply_to_id": 934663545,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Oops I should have caught that too.  Fixed!",
      "created_at": "2022-08-02T21:30:39Z",
      "updated_at": "2022-08-02T21:30:39Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936043450",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936043450"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 24,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936044329",
      "pull_request_review_id": 1059452250,
      "id": 936044329,
      "node_id": "PRRC_kwDOABII5843yucp",
      "diff_hunk": "@@ -71,6 +71,54 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n     return bnNew.GetCompact();\n }\n \n+// Check that on difficulty adjustments, the new difficulty does not increase\n+// or decrease beyond the permitted limits.\n+bool PermittedDifficultyTransition(const Consensus::Params& params, int64_t height, uint32_t old_nbits, uint32_t new_nbits)\n+{\n+    if (params.fPowAllowMinDifficultyBlocks) return true;",
      "path": "src/pow.cpp",
      "position": 8,
      "original_position": 8,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cbf3069c7d76888a647d68dbb09d77e666dcaf50",
      "in_reply_to_id": 932566102,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think I prefer to just not check anything for chains where proof-of-work isn't an anti-DoS mechanism for us; seems like it avoids a false sense of security (and it doesn't accomplish anything anyway, as an attacker would just avoid failing such a check without expending any real resources).",
      "created_at": "2022-08-02T21:31:58Z",
      "updated_at": "2022-08-02T21:32:25Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936044329",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936044329"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 78,
      "original_line": 78,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936048510",
      "pull_request_review_id": 1059457865,
      "id": 936048510,
      "node_id": "PRRC_kwDOABII5843yvd-",
      "diff_hunk": "@@ -3426,6 +3426,25 @@ std::vector<unsigned char> ChainstateManager::GenerateCoinbaseCommitment(CBlock&\n     return commitment;\n }\n \n+bool HasValidProofOfWork(const std::vector<CBlockHeader>& headers, const Consensus::Params& consensusParams)\n+{\n+    bool proof_of_work_valid = true;\n+    for (const CBlockHeader& header : headers) {\n+        proof_of_work_valid = proof_of_work_valid && CheckProofOfWork(header.GetHash(), header.nBits, consensusParams);\n+    }\n+    return proof_of_work_valid;",
      "path": "src/validation.cpp",
      "position": null,
      "original_position": 10,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "fe620e1a1fb2ef11d06cf941249ad4e5e4250bbf",
      "in_reply_to_id": 934671923,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done, thanks.",
      "created_at": "2022-08-02T21:38:39Z",
      "updated_at": "2022-08-02T21:38:39Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936048510",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936048510"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 3431,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 3435,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936049872",
      "pull_request_review_id": 1059459507,
      "id": 936049872,
      "node_id": "PRRC_kwDOABII5843yvzQ",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+//! Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 10,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "e3260960add016aeb43d0be4c5b735418e37c4a7",
      "in_reply_to_id": 934686816,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-02T21:40:39Z",
      "updated_at": "2022-08-02T21:40:40Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936049872",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936049872"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 7,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 10,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936071181",
      "pull_request_review_id": 1059486854,
      "id": 936071181,
      "node_id": "PRRC_kwDOABII5843y1AN",
      "diff_hunk": "@@ -71,6 +71,57 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n     return bnNew.GetCompact();\n }\n \n+// Check that on difficulty adjustments, the new difficulty does not increase\n+// or decrease beyond the permitted limits.\n+bool PermittedDifficultyTransition(const Consensus::Params& params, int64_t height, uint32_t old_nbits, uint32_t new_nbits)\n+{\n+    if (params.fPowAllowMinDifficultyBlocks) return true;\n+\n+    if (height % params.DifficultyAdjustmentInterval() == 0) {\n+        int64_t smallest_timespan = params.nPowTargetTimespan/4;\n+        int64_t largest_timespan = params.nPowTargetTimespan*4;\n+\n+        const arith_uint256 pow_limit = UintToArith256(params.powLimit);\n+        arith_uint256 observed_new_target;\n+        observed_new_target.SetCompact(new_nbits);",
      "path": "src/pow.cpp",
      "position": 16,
      "original_position": 16,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "68c05de96a5b8d0b9a7122d964fe1c9704e2135d",
      "in_reply_to_id": null,
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "It might save a few operations to first check if the new target is larger or smaller than the old one, and conditional on that run only one of the two checks.",
      "created_at": "2022-08-02T22:17:26Z",
      "updated_at": "2022-08-03T05:03:18Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936071181",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936071181"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 86,
      "original_line": 86,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936085122",
      "pull_request_review_id": 1059486854,
      "id": 936085122,
      "node_id": "PRRC_kwDOABII5843y4aC",
      "diff_hunk": "@@ -5076,6 +5072,24 @@ bool PeerManagerImpl::SendMessages(CNode* pto)\n \n     MaybeSendAddr(*pto, *peer, current_time);\n \n+    // Delay sending SENDHEADERS (BIP 130) until we're done with an\n+    // initial-headers-sync with this peer. Receiving headers announcements for",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 29,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "c64d02a21dee37372822d2f85149606f9b1a6875",
      "in_reply_to_id": null,
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Would we actually receive headers messages during initial-headers-sync without this commit? As I wrote [here](https://github.com/bitcoin/bitcoin/pull/25720#discussion_r935756421), I got the impression that other peers revert to Inv when they found a new block during headers sync and won't send us unrequested headers messages anyway, so I wonder if delaying `SENDHEADERS` actually has an effect.",
      "created_at": "2022-08-02T22:45:27Z",
      "updated_at": "2022-08-03T05:11:38Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936085122",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936085122"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 5074,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936216176",
      "pull_request_review_id": 1059486854,
      "id": 936216176,
      "node_id": "PRRC_kwDOABII5843zYZw",
      "diff_hunk": "@@ -2483,25 +2683,31 @@ void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n     }\n \n     // At this point, the headers connect to something in our block index.\n-    if (!CheckHeadersAreContinuous(headers)) {\n-        Misbehaving(peer, 20, \"non-continuous headers sequence\");\n+    // Do anti-DoS checks to determine if we should process or store for later\n+    // processing.\n+    if (!already_validated_work && TryLowWorkHeadersSync(peer, pfrom,",
      "path": "src/net_processing.cpp",
      "position": 489,
      "original_position": 270,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6335a03b9e98394e28f8feab3fbade502ac1bbc2",
      "in_reply_to_id": null,
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think it would be good to have some unconditional logging during the first phase, because the node is currently quiet for possibly minutes after startup, which might lead users to think the node is stuck. Currently, the \"Synchronizing blockheaders, height: (...) (~(...)%)\" messages only start in the redownload phase. We should probably not log too much to prevent possible disk-filling attacks caused by a low-work header chain, but at least announcing the start of each phase would be nice.",
      "created_at": "2022-08-03T04:07:17Z",
      "updated_at": "2022-08-03T05:03:18Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936216176",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936216176"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2826,
      "original_line": 2826,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936232102",
      "pull_request_review_id": 1059486854,
      "id": 936232102,
      "node_id": "PRRC_kwDOABII5843zcSm",
      "diff_hunk": "@@ -0,0 +1,256 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, we can calculate the work on the chain as we\n+ * go (just by checking the nBits value on each header, and validating the\n+ * proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1, store 1 bit (using a salted hash function) for every N headers\n+ * that we see. With a reasonable choice of N, this uses relatively little\n+ * memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer of size H, and only\n+ * accept a batch of N (N < H) headers to memory once we've verified that H/N =\n+ * S commitments have all passed verification. With this parametrization, we",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 95,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6335a03b9e98394e28f8feab3fbade502ac1bbc2",
      "in_reply_to_id": null,
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I can't really relate this comment to the implementation. From what I understand we accept all received headers in phase 2 to memory (`m_redownloaded_headers`), but only accept them to the block index once there are `REDOWNLOAD_HEADERS_CHECK_BITS` correct commitments on top of them - with the batch size N of headers that are being accepted to the block index being just an (unimportant) consequence of the batch size in which we happen to receive headers from peers (2000 once the lookahead buffer is filled)?",
      "created_at": "2022-08-03T04:49:25Z",
      "updated_at": "2022-08-03T05:03:18Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936232102",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936232102"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 95,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936626595",
      "pull_request_review_id": 1060249068,
      "id": 936626595,
      "node_id": "PRRC_kwDOABII584308mj",
      "diff_hunk": "@@ -5076,6 +5072,24 @@ bool PeerManagerImpl::SendMessages(CNode* pto)\n \n     MaybeSendAddr(*pto, *peer, current_time);\n \n+    // Delay sending SENDHEADERS (BIP 130) until we're done with an\n+    // initial-headers-sync with this peer. Receiving headers announcements for",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 29,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "c64d02a21dee37372822d2f85149606f9b1a6875",
      "in_reply_to_id": 936085122,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I believe it is possible, for 2 main reasons: (1) BIP 130 has no explicit rules around when headers may be used to announce a block, so in theory any software implementing BIP 130 might be free to send a header to us regardless of whether we've completed header sync; (2) Bitcoin Core implementations would send us announcements via header at the end of our phase 1 download (if we reach their tip at the end of that phase, which is certainly possible).  So I think delaying `sendheaders` helps eliminate the common cases of how we might get a spurious headers message during our sync.\r\n",
      "created_at": "2022-08-03T12:53:44Z",
      "updated_at": "2022-08-03T12:53:44Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936626595",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936626595"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 5074,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936878007",
      "pull_request_review_id": 1060631981,
      "id": 936878007,
      "node_id": "PRRC_kwDOABII584315-3",
      "diff_hunk": "@@ -0,0 +1,256 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, we can calculate the work on the chain as we\n+ * go (just by checking the nBits value on each header, and validating the\n+ * proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1, store 1 bit (using a salted hash function) for every N headers\n+ * that we see. With a reasonable choice of N, this uses relatively little\n+ * memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer of size H, and only\n+ * accept a batch of N (N < H) headers to memory once we've verified that H/N =\n+ * S commitments have all passed verification. With this parametrization, we",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 95,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6335a03b9e98394e28f8feab3fbade502ac1bbc2",
      "in_reply_to_id": 936232102,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I updated the comment to hopefully make more sense and better match how we're thinking about this; let me know if it reads better now.",
      "created_at": "2022-08-03T16:17:55Z",
      "updated_at": "2022-08-03T16:17:55Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936878007",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936878007"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 95,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936882362",
      "pull_request_review_id": 1060639280,
      "id": 936882362,
      "node_id": "PRRC_kwDOABII584317C6",
      "diff_hunk": "@@ -2483,25 +2683,31 @@ void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n     }\n \n     // At this point, the headers connect to something in our block index.\n-    if (!CheckHeadersAreContinuous(headers)) {\n-        Misbehaving(peer, 20, \"non-continuous headers sequence\");\n+    // Do anti-DoS checks to determine if we should process or store for later\n+    // processing.\n+    if (!already_validated_work && TryLowWorkHeadersSync(peer, pfrom,",
      "path": "src/net_processing.cpp",
      "position": 489,
      "original_position": 270,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6335a03b9e98394e28f8feab3fbade502ac1bbc2",
      "in_reply_to_id": 936216176,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "So if I add unconditional logging on starting a sync, then in the absence of a rate limiter I think it'd be easy for someone to fill up our disk by starting and restarting syncs with us.  Perhaps I can log at most once per peer connection when we start a low-work headers sync, what do you think?",
      "created_at": "2022-08-03T16:19:21Z",
      "updated_at": "2022-08-03T16:19:22Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936882362",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936882362"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2826,
      "original_line": 2826,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936902553",
      "pull_request_review_id": 1060675384,
      "id": 936902553,
      "node_id": "PRRC_kwDOABII58431_-Z",
      "diff_hunk": "@@ -2483,25 +2683,31 @@ void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n     }\n \n     // At this point, the headers connect to something in our block index.\n-    if (!CheckHeadersAreContinuous(headers)) {\n-        Misbehaving(peer, 20, \"non-continuous headers sequence\");\n+    // Do anti-DoS checks to determine if we should process or store for later\n+    // processing.\n+    if (!already_validated_work && TryLowWorkHeadersSync(peer, pfrom,",
      "path": "src/net_processing.cpp",
      "position": 489,
      "original_position": 270,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6335a03b9e98394e28f8feab3fbade502ac1bbc2",
      "in_reply_to_id": 936216176,
      "user": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "maybe a per connection per X seconds, which would give consistent feedback that's ratelimited?",
      "created_at": "2022-08-03T16:27:38Z",
      "updated_at": "2022-08-03T16:27:38Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936902553",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936902553"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2826,
      "original_line": 2826,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936906842",
      "pull_request_review_id": 1060680877,
      "id": 936906842,
      "node_id": "PRRC_kwDOABII58432BBa",
      "diff_hunk": "@@ -2483,25 +2683,31 @@ void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n     }\n \n     // At this point, the headers connect to something in our block index.\n-    if (!CheckHeadersAreContinuous(headers)) {\n-        Misbehaving(peer, 20, \"non-continuous headers sequence\");\n+    // Do anti-DoS checks to determine if we should process or store for later\n+    // processing.\n+    if (!already_validated_work && TryLowWorkHeadersSync(peer, pfrom,",
      "path": "src/net_processing.cpp",
      "position": 489,
      "original_position": 270,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6335a03b9e98394e28f8feab3fbade502ac1bbc2",
      "in_reply_to_id": 936216176,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Ideally, I'd say, we produce header sync progress updates based on the longest currently-ongoing header sync across all peers. That sounds like a pain to implement, though.",
      "created_at": "2022-08-03T16:30:31Z",
      "updated_at": "2022-08-03T16:30:31Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936906842",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936906842"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2826,
      "original_line": 2826,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936915939",
      "pull_request_review_id": 1060692987,
      "id": 936915939,
      "node_id": "PRRC_kwDOABII58432DPj",
      "diff_hunk": "@@ -2483,25 +2683,31 @@ void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n     }\n \n     // At this point, the headers connect to something in our block index.\n-    if (!CheckHeadersAreContinuous(headers)) {\n-        Misbehaving(peer, 20, \"non-continuous headers sequence\");\n+    // Do anti-DoS checks to determine if we should process or store for later\n+    // processing.\n+    if (!already_validated_work && TryLowWorkHeadersSync(peer, pfrom,",
      "path": "src/net_processing.cpp",
      "position": 489,
      "original_position": 270,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6335a03b9e98394e28f8feab3fbade502ac1bbc2",
      "in_reply_to_id": 936216176,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "An additional complication in logging is that the first phase takes place entirely in net_processing, in per-peer datastructures. There is no interaction with validation, which currently emits the header sync progress updates, until the second phase.",
      "created_at": "2022-08-03T16:40:09Z",
      "updated_at": "2022-08-03T16:40:09Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936915939",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936915939"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2826,
      "original_line": 2826,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936919365",
      "pull_request_review_id": 1060697684,
      "id": 936919365,
      "node_id": "PRRC_kwDOABII58432EFF",
      "diff_hunk": "@@ -2483,25 +2683,31 @@ void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n     }\n \n     // At this point, the headers connect to something in our block index.\n-    if (!CheckHeadersAreContinuous(headers)) {\n-        Misbehaving(peer, 20, \"non-continuous headers sequence\");\n+    // Do anti-DoS checks to determine if we should process or store for later\n+    // processing.\n+    if (!already_validated_work && TryLowWorkHeadersSync(peer, pfrom,",
      "path": "src/net_processing.cpp",
      "position": 489,
      "original_position": 270,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6335a03b9e98394e28f8feab3fbade502ac1bbc2",
      "in_reply_to_id": 936216176,
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Maybe once per connection, and excluding inbound peers would be safe? Alternatively, we could log something once on startup if our best chain is below `nMinimumChainWork`. I'm thinking of the typical case of a new node without any headers starting up.",
      "created_at": "2022-08-03T16:43:57Z",
      "updated_at": "2022-08-03T16:43:57Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936919365",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/936919365"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2826,
      "original_line": 2826,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/937145579",
      "pull_request_review_id": 1061022786,
      "id": 937145579,
      "node_id": "PRRC_kwDOABII584327Tr",
      "diff_hunk": "@@ -0,0 +1,256 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, we can calculate the work on the chain as we\n+ * go (just by checking the nBits value on each header, and validating the\n+ * proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1, store 1 bit (using a salted hash function) for every N headers\n+ * that we see. With a reasonable choice of N, this uses relatively little\n+ * memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer of size H, and only\n+ * accept a batch of N (N < H) headers to memory once we've verified that H/N =\n+ * S commitments have all passed verification. With this parametrization, we",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 95,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6335a03b9e98394e28f8feab3fbade502ac1bbc2",
      "in_reply_to_id": 936232102,
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Thanks, it reads well now!",
      "created_at": "2022-08-03T21:16:31Z",
      "updated_at": "2022-08-03T21:16:31Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r937145579",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/937145579"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 95,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/938148745",
      "pull_request_review_id": 1062449560,
      "id": 938148745,
      "node_id": "PRRC_kwDOABII58436wOJ",
      "diff_hunk": "@@ -71,6 +71,57 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n     return bnNew.GetCompact();\n }\n \n+// Check that on difficulty adjustments, the new difficulty does not increase\n+// or decrease beyond the permitted limits.\n+bool PermittedDifficultyTransition(const Consensus::Params& params, int64_t height, uint32_t old_nbits, uint32_t new_nbits)",
      "path": "src/pow.cpp",
      "position": 6,
      "original_position": 6,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "68c05de96a5b8d0b9a7122d964fe1c9704e2135d",
      "in_reply_to_id": null,
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Would it be possible to also add `PermittedDifficultyTransition()` coverage to the fuzz test `src/test/fuzz/pow.cpp`? I'm thinking of something like combining it with the new target from `GetNextWorkRequired()`, and then asserting that the resulting transition passes `PermittedDifficultyTransition()`.",
      "created_at": "2022-08-04T18:50:39Z",
      "updated_at": "2022-08-05T16:02:35Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r938148745",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/938148745"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 76,
      "original_line": 76,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/938907402",
      "pull_request_review_id": 1063495599,
      "id": 938907402,
      "node_id": "PRRC_kwDOABII58439pcK",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY{571};\n+\n+//! Only feed headers to validation once this many commitment bits on top have been checked.\n+//! During redownloading, a per-peer buffer with space for\n+//! HEADER_COMMITMENT_FREQUENCY * REDOWNLOAD_HEADERS_CHECK_BITS headers will be used.\n+constexpr size_t REDOWNLOAD_HEADERS_CHECK_BITS{25};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+    std::vector<uint256>().swap(m_chain_start_locator.vHave);\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Initialize the parameters for this headers download, validate this first\n+ * batch, and request more headers. */\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::UNSTARTED);\n+    m_download_state = State::INITIAL_DOWNLOAD;\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 69,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d: note to other reviewers: when using this code to sync from the genesis block in the year 2100, `m_max_commitments` would be about 32 million. That's less than 4 MB per peer, assuming `bitdeque` has low overhead.\r\n\r\nOf course by that time the baked in value of `nMinimumChainWork` might be too low.",
      "created_at": "2022-08-05T15:01:30Z",
      "updated_at": "2022-08-05T17:33:52Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r938907402",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/938907402"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 41,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/938950210",
      "pull_request_review_id": 1063495599,
      "id": 938950210,
      "node_id": "PRRC_kwDOABII58439z5C",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY{571};",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 8,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d  nit: it's a period or interval, not a frequency (which would be 1/571)",
      "created_at": "2022-08-05T15:52:07Z",
      "updated_at": "2022-08-05T16:16:58Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r938950210",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/938950210"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 8,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/938963599",
      "pull_request_review_id": 1063495599,
      "id": 938963599,
      "node_id": "PRRC_kwDOABII584393KP",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY{571};\n+\n+//! Only feed headers to validation once this many commitment bits on top have been checked.\n+//! During redownloading, a per-peer buffer with space for\n+//! HEADER_COMMITMENT_FREQUENCY * REDOWNLOAD_HEADERS_CHECK_BITS headers will be used.\n+constexpr size_t REDOWNLOAD_HEADERS_CHECK_BITS{25};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+    std::vector<uint256>().swap(m_chain_start_locator.vHave);\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Initialize the parameters for this headers download, validate this first\n+ * batch, and request more headers. */\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::UNSTARTED);\n+    m_download_state = State::INITIAL_DOWNLOAD;\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return ret;\n+        }\n+        ret.success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            ret.locator = MakeNextHeadersRequest();\n+            return ret;\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            LogPrint(BCLog::NET, \"Ignoring low-work chain (height=%u) from peer=%d\\n\", m_current_height, m_id);",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 110,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I'm still a bit unclear on the sequence of calls here. Why are there two checks for non-full messages with low-work? ",
      "created_at": "2022-08-05T16:08:54Z",
      "updated_at": "2022-08-05T16:16:58Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r938963599",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/938963599"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 110,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/938966167",
      "pull_request_review_id": 1063495599,
      "id": 938966167,
      "node_id": "PRRC_kwDOABII584393yX",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY{571};\n+\n+//! Only feed headers to validation once this many commitment bits on top have been checked.\n+//! During redownloading, a per-peer buffer with space for\n+//! HEADER_COMMITMENT_FREQUENCY * REDOWNLOAD_HEADERS_CHECK_BITS headers will be used.\n+constexpr size_t REDOWNLOAD_HEADERS_CHECK_BITS{25};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+    std::vector<uint256>().swap(m_chain_start_locator.vHave);\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Initialize the parameters for this headers download, validate this first\n+ * batch, and request more headers. */\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::UNSTARTED);\n+    m_download_state = State::INITIAL_DOWNLOAD;\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return ret;\n+        }\n+        ret.success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            ret.locator = MakeNextHeadersRequest();\n+            return ret;\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            LogPrint(BCLog::NET, \"Ignoring low-work chain (height=%u) from peer=%d\\n\", m_current_height, m_id);\n+            Finalize();\n+            return ret;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return ret;\n+            }\n+        }\n+\n+        ret.success = true;\n+        // Return any headers that are ready for acceptance.\n+        ret.headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return ret;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            ret.locator = MakeNextHeadersRequest();\n+            return ret;\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return ret;\n+        }\n+    }\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps we issued an extra getheaders\n+        // message, such as after a block INV was received.\n+        // Or it could be that our peer is broken or malicious. If broken,\n+        // sending a new getheaders immediately could trigger an infinite\n+        // loop. Just give up for now; if our peer ever gives us an block\n+        // INV later we will fetch headers then, and likely retrigger this\n+        // logic.\n+        Finalize();\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr, m_current_height+1)) {\n+            Finalize();\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_blockhash_with_sufficient_work = headers.back().GetHash();\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_download_state = State::REDOWNLOAD;",
      "path": "src/headerssync.cpp",
      "position": 171,
      "original_position": 192,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d: it would be good to have a log message when the state switches to `REDOWNLOAD`.",
      "created_at": "2022-08-05T16:12:09Z",
      "updated_at": "2022-08-05T16:16:58Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r938966167",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/938966167"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 171,
      "original_line": 171,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/938967065",
      "pull_request_review_id": 1063495599,
      "id": 938967065,
      "node_id": "PRRC_kwDOABII584394AZ",
      "diff_hunk": "@@ -2309,6 +2379,100 @@ bool PeerManagerImpl::CheckHeadersAreContinuous(const std::vector<CBlockHeader>&\n     return true;\n }\n \n+bool PeerManagerImpl::IsContinuationOfLowWorkHeadersSync(Peer& peer, CNode& pfrom, const std::vector<CBlockHeader>& headers, std::vector<CBlockHeader>& previously_downloaded_headers)\n+{\n+    if (peer.m_headers_sync) {\n+        auto result = peer.m_headers_sync->ProcessNextHeaders(headers, headers.size() == MAX_HEADERS_RESULTS);\n+        if (result.locator) {\n+            // If we get back a locator, it should not be empty\n+            Assume(!result.locator->vHave.empty());\n+            if (!result.locator->vHave.empty()) {\n+                // It should be impossible for the getheaders request to fail,\n+                // because we should have cleared the last getheaders timestamp\n+                // when processing the headers that triggered this call. But\n+                // it may be possible to bypass this via compactblock\n+                // processing, so check the result before logging just to be\n+                // safe.\n+                bool sent_getheaders = MaybeSendGetHeaders(pfrom, *result.locator, peer);\n+                if (sent_getheaders) {\n+                    LogPrint(BCLog::NET, \"more getheaders (from %s) to peer=%d\\n\",\n+                            result.locator->vHave.front().ToString(), pfrom.GetId());\n+                }\n+            }\n+        }\n+\n+        if (peer.m_headers_sync->GetState() == HeadersSyncState::State::FINAL) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }\n+\n+        previously_downloaded_headers.swap(result.headers_to_process);\n+\n+        if (previously_downloaded_headers.empty() && result.success) {\n+            // If nothing else was returned and processing was successful, then\n+            // we're all done.\n+            return true;\n+        }\n+    }\n+    // Either we didn't have a sync in progress, or something went wrong\n+    // processing these headers, or we are returning headers to the caller to\n+    // process.\n+    return false;\n+}\n+\n+bool PeerManagerImpl::TryLowWorkHeadersSync(Peer& peer, CNode& pfrom, const CBlockIndex* chain_start_header, const std::vector<CBlockHeader>& headers)\n+{\n+    // Calculate the total work on this chain.\n+    arith_uint256 total_work = chain_start_header->nChainWork + CalculateHeadersWork(headers);\n+\n+    // Our dynamic anti-DoS threshold (minimum work required on a headers chain\n+    // before we'll store it)\n+    arith_uint256 minimum_chain_work = GetAntiDoSWorkThreshold();\n+\n+    // Avoid DoS via low-difficulty-headers by only processing if the headers\n+    // are part of a chain with sufficient work.\n+    if (total_work < minimum_chain_work) {\n+        // Only try to sync with this peer if their headers message was full;\n+        // otherwise they don't have more headers after this so no point in\n+        // trying to sync their too-little-work chain.\n+        if (headers.size() == MAX_HEADERS_RESULTS) {\n+            // Note: we could advance to the last header in this set that is\n+            // known to us, rather than starting at the first header (which we\n+            // may already have). But we actually might have all the headers in\n+            // this set already, because headers sync can be imprecise when a\n+            // peer has to serve us a long chain (due to imprecision in the way\n+            // locators are calculated). So philosophically, if we wanted to\n+            // optimize this correctly, we could consider requesting more\n+            // headers until we find the peer's first new header on this chain,\n+            // and start the sync from there. In practice this is unlikely to\n+            // matter much outside of initial sync, which we generally only do\n+            // once, so for now we'll just start the sync with the first header\n+            // in the set and not worry about this issue.\n+            peer.m_headers_sync.reset(new HeadersSyncState(peer.m_id, m_chainparams.GetConsensus()));\n+            if (std::optional<CBlockLocator> locator =\n+                    peer.m_headers_sync->StartInitialDownload(chain_start_header,\n+                        headers, minimum_chain_work,\n+                        m_chainman.ActiveChain().GetLocator(chain_start_header)))\n+            {\n+                Assume(!locator->vHave.empty());\n+                if (!locator->vHave.empty() && MaybeSendGetHeaders(pfrom, *locator, peer)) {\n+                    LogPrint(BCLog::NET, \"more getheaders (from %s) to end to peer=%d\\n\",",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 184,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d : can you add the height to this message? That and a message when the state changes, makes it easier to follow what's happening in the `-debug=net` log.",
      "created_at": "2022-08-05T16:13:24Z",
      "updated_at": "2022-08-05T16:16:58Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r938967065",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/938967065"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 2458,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/939062349",
      "pull_request_review_id": 1063715336,
      "id": 939062349,
      "node_id": "PRRC_kwDOABII5843-PRN",
      "diff_hunk": "@@ -2309,6 +2379,100 @@ bool PeerManagerImpl::CheckHeadersAreContinuous(const std::vector<CBlockHeader>&\n     return true;\n }\n \n+bool PeerManagerImpl::IsContinuationOfLowWorkHeadersSync(Peer& peer, CNode& pfrom, const std::vector<CBlockHeader>& headers, std::vector<CBlockHeader>& previously_downloaded_headers)\n+{\n+    if (peer.m_headers_sync) {\n+        auto result = peer.m_headers_sync->ProcessNextHeaders(headers, headers.size() == MAX_HEADERS_RESULTS);\n+        if (result.locator) {\n+            // If we get back a locator, it should not be empty\n+            Assume(!result.locator->vHave.empty());\n+            if (!result.locator->vHave.empty()) {\n+                // It should be impossible for the getheaders request to fail,\n+                // because we should have cleared the last getheaders timestamp\n+                // when processing the headers that triggered this call. But\n+                // it may be possible to bypass this via compactblock\n+                // processing, so check the result before logging just to be\n+                // safe.\n+                bool sent_getheaders = MaybeSendGetHeaders(pfrom, *result.locator, peer);\n+                if (sent_getheaders) {\n+                    LogPrint(BCLog::NET, \"more getheaders (from %s) to peer=%d\\n\",\n+                            result.locator->vHave.front().ToString(), pfrom.GetId());\n+                }\n+            }\n+        }\n+\n+        if (peer.m_headers_sync->GetState() == HeadersSyncState::State::FINAL) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }\n+\n+        previously_downloaded_headers.swap(result.headers_to_process);\n+\n+        if (previously_downloaded_headers.empty() && result.success) {\n+            // If nothing else was returned and processing was successful, then\n+            // we're all done.\n+            return true;\n+        }\n+    }\n+    // Either we didn't have a sync in progress, or something went wrong\n+    // processing these headers, or we are returning headers to the caller to\n+    // process.\n+    return false;\n+}\n+\n+bool PeerManagerImpl::TryLowWorkHeadersSync(Peer& peer, CNode& pfrom, const CBlockIndex* chain_start_header, const std::vector<CBlockHeader>& headers)\n+{\n+    // Calculate the total work on this chain.\n+    arith_uint256 total_work = chain_start_header->nChainWork + CalculateHeadersWork(headers);\n+\n+    // Our dynamic anti-DoS threshold (minimum work required on a headers chain\n+    // before we'll store it)\n+    arith_uint256 minimum_chain_work = GetAntiDoSWorkThreshold();\n+\n+    // Avoid DoS via low-difficulty-headers by only processing if the headers\n+    // are part of a chain with sufficient work.\n+    if (total_work < minimum_chain_work) {\n+        // Only try to sync with this peer if their headers message was full;\n+        // otherwise they don't have more headers after this so no point in\n+        // trying to sync their too-little-work chain.\n+        if (headers.size() == MAX_HEADERS_RESULTS) {\n+            // Note: we could advance to the last header in this set that is\n+            // known to us, rather than starting at the first header (which we\n+            // may already have). But we actually might have all the headers in\n+            // this set already, because headers sync can be imprecise when a\n+            // peer has to serve us a long chain (due to imprecision in the way\n+            // locators are calculated). So philosophically, if we wanted to\n+            // optimize this correctly, we could consider requesting more\n+            // headers until we find the peer's first new header on this chain,\n+            // and start the sync from there. In practice this is unlikely to\n+            // matter much outside of initial sync, which we generally only do\n+            // once, so for now we'll just start the sync with the first header\n+            // in the set and not worry about this issue.\n+            peer.m_headers_sync.reset(new HeadersSyncState(peer.m_id, m_chainparams.GetConsensus()));\n+            if (std::optional<CBlockLocator> locator =\n+                    peer.m_headers_sync->StartInitialDownload(chain_start_header,\n+                        headers, minimum_chain_work,\n+                        m_chainman.ActiveChain().GetLocator(chain_start_header)))\n+            {\n+                Assume(!locator->vHave.empty());\n+                if (!locator->vHave.empty() && MaybeSendGetHeaders(pfrom, *locator, peer)) {\n+                    LogPrint(BCLog::NET, \"more getheaders (from %s) to end to peer=%d\\n\",",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 184,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 938967065,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "But see also this discussion on logging: https://github.com/bitcoin/bitcoin/pull/25717#discussion_r936216176",
      "created_at": "2022-08-05T18:19:49Z",
      "updated_at": "2022-08-05T18:19:50Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r939062349",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/939062349"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 2458,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/939436270",
      "pull_request_review_id": 1064220561,
      "id": 939436270,
      "node_id": "PRRC_kwDOABII5843_qju",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY{571};\n+\n+//! Only feed headers to validation once this many commitment bits on top have been checked.\n+//! During redownloading, a per-peer buffer with space for\n+//! HEADER_COMMITMENT_FREQUENCY * REDOWNLOAD_HEADERS_CHECK_BITS headers will be used.\n+constexpr size_t REDOWNLOAD_HEADERS_CHECK_BITS{25};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+    std::vector<uint256>().swap(m_chain_start_locator.vHave);\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Initialize the parameters for this headers download, validate this first\n+ * batch, and request more headers. */\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::UNSTARTED);\n+    m_download_state = State::INITIAL_DOWNLOAD;\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return ret;\n+        }\n+        ret.success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            ret.locator = MakeNextHeadersRequest();\n+            return ret;\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            LogPrint(BCLog::NET, \"Ignoring low-work chain (height=%u) from peer=%d\\n\", m_current_height, m_id);",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 110,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 938963599,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Just to make sure I understand your question -- I think you're wondering why this log message appears here as well as in `net_processing.cpp` at line 2473?\r\n\r\nThe issue is that in `net_processing` we won't bother even starting a headers sync using the logic in `headerssync.cpp` if we receive a headers message that (a) isn't full and (b) which connects to something we know in our block index and (c) has too little work, because we know in this case that the headers don't lead to a high work chain (if it did, the headers message would have been full, indicating the peer has more to give us).\r\n\r\nHowever, in the case that we start a headers sync using this module, then `net_processing` will be receiving headers that don't connect to the block index, and those messages can only be properly processed by this logic which keeps track of where we expect the headers to connect. So in this logic we also are checking whether the sync has to end because our peer has no more headers to give us (ie the headers message we processed from them was not full, and the chain has too little work).",
      "created_at": "2022-08-05T22:58:22Z",
      "updated_at": "2022-08-05T22:58:22Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r939436270",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/939436270"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 110,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/939507811",
      "pull_request_review_id": 1064290130,
      "id": 939507811,
      "node_id": "PRRC_kwDOABII5843_8Bj",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY{571};\n+\n+//! Only feed headers to validation once this many commitment bits on top have been checked.\n+//! During redownloading, a per-peer buffer with space for\n+//! HEADER_COMMITMENT_FREQUENCY * REDOWNLOAD_HEADERS_CHECK_BITS headers will be used.\n+constexpr size_t REDOWNLOAD_HEADERS_CHECK_BITS{25};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+    std::vector<uint256>().swap(m_chain_start_locator.vHave);\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Initialize the parameters for this headers download, validate this first\n+ * batch, and request more headers. */\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::UNSTARTED);\n+    m_download_state = State::INITIAL_DOWNLOAD;\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return ret;\n+        }\n+        ret.success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            ret.locator = MakeNextHeadersRequest();\n+            return ret;\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            LogPrint(BCLog::NET, \"Ignoring low-work chain (height=%u) from peer=%d\\n\", m_current_height, m_id);",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 110,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 938963599,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "That was indeed my question.\r\n\r\nThe other confusing bit is the comment \"and definitely doesn't have enough work\". When looking at this function in isolation, it's not obvious why this would be the case, since we don't even check the work. It's also not obvious if I look at where this is called from.",
      "created_at": "2022-08-06T09:28:55Z",
      "updated_at": "2022-08-06T18:24:42Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r939507811",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/939507811"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 110,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/939523672",
      "pull_request_review_id": 1064303425,
      "id": 939523672,
      "node_id": "PRRC_kwDOABII5843__5Y",
      "diff_hunk": "@@ -62,7 +63,7 @@ def run_test(self):\n \n         self.log.info(\"Feed all fork headers (succeeds without checkpoint)\")\n         # On node 0 it succeeds because checkpoints are disabled\n-        self.restart_node(0, extra_args=['-nocheckpoints'])\n+        self.restart_node(0, extra_args=['-nocheckpoints', \"-minimumchainwork=0x0\"])",
      "path": "test/functional/p2p_dos_header_tree.py",
      "position": 13,
      "original_position": 13,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d: maybe add additional tests where you set `minimumchainwork` to the chainwork at the original checkpoint (`0x0000000000000000000000000000000000000000000000000000022302230223`).\r\n\r\n```python\r\n\r\n        self.log.info(\"Feed all fork headers (fails due to minimum chain work)\")\r\n        # Disable checkpoint, but require its chainwork as the minimum.\r\n        self.restart_node(0, extra_args=['-nocheckpoints', \"-minimumchainwork=0x0000000000000000000000000000000000000000000000000000022302230223\"])\r\n        peer_minimumchainwork = self.nodes[0].add_p2p_connection(P2PInterface())\r\n        peer_minimumchainwork.send_and_ping(msg_headers(self.headers_fork))\r\n        with self.nodes[0].assert_debug_log(['Ignoring low-work chain']):\r\n            peer_minimumchainwork.send_message(msg_headers(self.headers_fork))\r\n            # We don't disconnect\r\n```\r\n\r\nNote that if you also set the minimum chainwork for node 0 for the entire test, it will stall at \"Feed all fork headers (fails due to checkpoint)\", because we no longer disconnect.\r\n\r\nNot sure if the above covers more ground than the new test file introduced in 7be81ce6257b1940d633b1311432c09fbbe09a0b, but it might still be useful to illustrate the behavior change. Then again, in the real world our minimum chainwork is much higher than that of the most recent checkpoints, so this example may be more confusing than illustrating? ",
      "created_at": "2022-08-06T12:32:20Z",
      "updated_at": "2022-08-06T12:55:26Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r939523672",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/939523672"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 66,
      "original_line": 66,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/939526940",
      "pull_request_review_id": 1064306189,
      "id": 939526940,
      "node_id": "PRRC_kwDOABII5844AAsc",
      "diff_hunk": "@@ -2457,18 +2621,54 @@ void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n                                             const std::vector<CBlockHeader>& headers,\n                                             bool via_compact_block)\n {\n-    const CNetMsgMaker msgMaker(pfrom.GetCommonVersion());",
      "path": "src/net_processing.cpp",
      "position": 398,
      "original_position": 209,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d: I was a bit surprised this never triggered an unused variable warning, but that's apparently possible with non-trivial types: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=55203",
      "created_at": "2022-08-06T13:09:16Z",
      "updated_at": "2022-08-06T13:26:29Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r939526940",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/939526940"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2467,
      "original_line": 2467,
      "side": "LEFT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/939557140",
      "pull_request_review_id": 1064330845,
      "id": 939557140,
      "node_id": "PRRC_kwDOABII5844AIEU",
      "diff_hunk": "@@ -2256,6 +2297,35 @@ void PeerManagerImpl::SendBlockTransactions(CNode& pfrom, Peer& peer, const CBlo\n     m_connman.PushMessage(&pfrom, msgMaker.Make(NetMsgType::BLOCKTXN, resp));\n }\n \n+bool PeerManagerImpl::CheckHeadersPoW(const std::vector<CBlockHeader>& headers, const Consensus::Params& consensusParams, Peer& peer)\n+{\n+    // Do these headers have proof-of-work matching what's claimed?\n+    if (!HasValidProofOfWork(headers, consensusParams)) {\n+        Misbehaving(peer, 100, \"header with invalid proof of work\");\n+        return false;\n+    }\n+\n+    // Are these headers connected to each other?\n+    if (!CheckHeadersAreContinuous(headers)) {\n+        Misbehaving(peer, 20, \"non-continuous headers sequence\");\n+        return false;\n+    }\n+    return true;\n+}\n+\n+arith_uint256 PeerManagerImpl::GetAntiDoSWorkThreshold()\n+{\n+    arith_uint256 near_chaintip_work = 0;\n+    LOCK(cs_main);\n+    if (m_chainman.ActiveChain().Tip() != nullptr) {\n+        const CBlockIndex *tip = m_chainman.ActiveChain().Tip();\n+        // Use a 144 block buffer, so that we'll accept headers that fork from\n+        // near our tip.",
      "path": "src/net_processing.cpp",
      "position": 225,
      "original_position": 95,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d: I assume we're interested in those, because although their total work is lower, there's a chance a miner builds on top of it. In that case, if we receive only that new block (header), we already have the parent? Otherwise we'd need a peer to send multiple headers to learn about this new block.",
      "created_at": "2022-08-06T18:04:21Z",
      "updated_at": "2022-08-06T18:05:05Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r939557140",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/939557140"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2386,
      "original_line": 2386,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/939559072",
      "pull_request_review_id": 1064332485,
      "id": 939559072,
      "node_id": "PRRC_kwDOABII5844AIig",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY{571};\n+\n+//! Only feed headers to validation once this many commitment bits on top have been checked.\n+//! During redownloading, a per-peer buffer with space for\n+//! HEADER_COMMITMENT_FREQUENCY * REDOWNLOAD_HEADERS_CHECK_BITS headers will be used.\n+constexpr size_t REDOWNLOAD_HEADERS_CHECK_BITS{25};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+    std::vector<uint256>().swap(m_chain_start_locator.vHave);\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Initialize the parameters for this headers download, validate this first\n+ * batch, and request more headers. */\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::UNSTARTED);\n+    m_download_state = State::INITIAL_DOWNLOAD;\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return ret;\n+        }\n+        ret.success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            ret.locator = MakeNextHeadersRequest();\n+            return ret;\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            LogPrint(BCLog::NET, \"Ignoring low-work chain (height=%u) from peer=%d\\n\", m_current_height, m_id);",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 110,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 938963599,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Oh, `ValidateAndStoreHeadersCommitments()` has the side-effect of changing `m_download_state` to `REDOWNLOAD` once minimum difficulty is reached.",
      "created_at": "2022-08-06T18:26:44Z",
      "updated_at": "2022-08-06T18:26:56Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r939559072",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/939559072"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 110,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/939559233",
      "pull_request_review_id": 1064332620,
      "id": 939559233,
      "node_id": "PRRC_kwDOABII5844AIlB",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY{571};\n+\n+//! Only feed headers to validation once this many commitment bits on top have been checked.\n+//! During redownloading, a per-peer buffer with space for\n+//! HEADER_COMMITMENT_FREQUENCY * REDOWNLOAD_HEADERS_CHECK_BITS headers will be used.\n+constexpr size_t REDOWNLOAD_HEADERS_CHECK_BITS{25};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+    std::vector<uint256>().swap(m_chain_start_locator.vHave);\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Initialize the parameters for this headers download, validate this first\n+ * batch, and request more headers. */\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::UNSTARTED);\n+    m_download_state = State::INITIAL_DOWNLOAD;\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 94,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d: maybe add a comment \"m_download_state is updated to REDOWNLOAD if headers reach the minimum difficulty.\"",
      "created_at": "2022-08-06T18:28:36Z",
      "updated_at": "2022-08-06T18:28:36Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r939559233",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/939559233"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 80,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940092129",
      "pull_request_review_id": 1064970764,
      "id": 940092129,
      "node_id": "PRRC_kwDOABII5844CKrh",
      "diff_hunk": "@@ -0,0 +1,332 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY{571};\n+\n+//! Only feed headers to validation once this many commitment bits on top have been checked.\n+//! During redownloading, a per-peer buffer with space for\n+//! HEADER_COMMITMENT_FREQUENCY * REDOWNLOAD_HEADERS_CHECK_BITS headers will be used.\n+constexpr size_t REDOWNLOAD_HEADERS_CHECK_BITS{25};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+    std::vector<uint256>().swap(m_chain_start_locator.vHave);\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Initialize the parameters for this headers download, validate this first\n+ * batch, and request more headers. */\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::UNSTARTED);\n+    m_download_state = State::INITIAL_DOWNLOAD;\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return ret;\n+        }\n+        ret.success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            ret.locator = MakeNextHeadersRequest();\n+            return ret;\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            LogPrint(BCLog::NET, \"Ignoring low-work chain (height=%u) from peer=%d\\n\", m_current_height, m_id);\n+            Finalize();\n+            return ret;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return ret;\n+            }\n+        }\n+\n+        ret.success = true;\n+        // Return any headers that are ready for acceptance.\n+        ret.headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return ret;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            ret.locator = MakeNextHeadersRequest();\n+            return ret;\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return ret;\n+        }\n+    }\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps we issued an extra getheaders\n+        // message, such as after a block INV was received.\n+        // Or it could be that our peer is broken or malicious. If broken,\n+        // sending a new getheaders immediately could trigger an infinite\n+        // loop. Just give up for now; if our peer ever gives us an block\n+        // INV later we will fetch headers then, and likely retrigger this\n+        // logic.\n+        Finalize();\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr, m_current_height+1)) {\n+            Finalize();\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current, int64_t current_height)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, current_height,\n+                previous.nBits, current.nBits)) {\n+        return false;\n+    }\n+\n+    if (!CheckProofOfWork(current.GetHash(), current.nBits, m_consensus_params)) return false;\n+\n+    if ((current_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+        // Add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // It's possible the chain grew since we started the sync; so\n+            // potentially we could succeed in syncing the peer's chain if we\n+            // try again later.\n+            LogPrint(BCLog::NET, \"headers chain is too long; giving up sync peer=%d\\n\", m_id);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));\n+    m_last_header_received = current;\n+    ++m_current_height;\n+\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreRedownloadedHeader(const CBlockHeader& header)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Ensure that we're working on a header that connects to the chain we're\n+    // downloading.\n+    if (header.hashPrevBlock != m_redownload_buffer_last_hash) {\n+        return false;\n+    }\n+\n+    int64_t next_height = m_redownload_buffer_last_height + 1;\n+\n+    // Check that the difficulty adjustments are within our tolerance:\n+    uint32_t previous_nBits{0};\n+    if (!m_redownloaded_headers.empty()) {\n+        previous_nBits = m_redownloaded_headers.back().nBits;\n+    } else {\n+        previous_nBits = m_chain_start->nBits;\n+    }\n+\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous_nBits, header.nBits)) {\n+        return false;\n+    }\n+\n+    // Track work on the redownloaded chain\n+    m_redownload_chain_work += GetBlockProof(CBlockIndex(header));\n+\n+    if (m_redownload_chain_work >= m_minimum_required_work) {\n+        m_process_all_remaining_headers = true;\n+    }\n+\n+    // If we're at a header for which we previously stored a commitment, verify\n+    // it is correct. Failure will result in aborting download.\n+    // Also, don't check commitments once we've gotten to our target blockhash;\n+    // it's possible our peer has extended its chain between our first sync and\n+    // our second, and we don't want to return failure after we've seen our\n+    // target blockhash just because we ran out of commitments.\n+    if (!m_process_all_remaining_headers && (next_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+         bool commitment = m_hasher(header.GetHash()) & 1;\n+         if (m_header_commitments.size() == 0) {\n+            // Somehow our peer managed to feed us a different chain and\n+            // we've run out of commitments.\n+            return false;\n+        }\n+        bool expected_commitment = m_header_commitments.front();\n+        m_header_commitments.pop_front();\n+        if (commitment != expected_commitment) {\n+            return false;\n+        }\n+    }\n+\n+    // Store this header for later processing.\n+    m_redownloaded_headers.push_back(header);\n+    m_redownload_buffer_last_height = next_height;\n+    m_redownload_buffer_last_hash = header.GetHash();\n+\n+    return true;\n+}\n+\n+std::vector<CBlockHeader> HeadersSyncState::RemoveHeadersReadyForAcceptance()",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 294,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "caa2419e65e3b9d7183f2246d66eb7ce6b7ce1f7",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Maybe rename to `PopHeadersReadyForAcceptance()`",
      "created_at": "2022-08-08T10:40:53Z",
      "updated_at": "2022-08-08T11:40:08Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r940092129",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940092129"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 285,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940093835",
      "pull_request_review_id": 1064970764,
      "id": 940093835,
      "node_id": "PRRC_kwDOABII5844CLGL",
      "diff_hunk": "@@ -0,0 +1,255 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, we can calculate the work on the chain as we\n+ * go (just by checking the nBits value on each header, and validating the\n+ * proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1, store 1 bit (using a salted hash function) for every N headers\n+ * that we see. With a reasonable choice of N, this uses relatively little\n+ * memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params);\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        UNSTARTED,\n+        /** INITIAL_DOWNLOAD means the peer has not yet demonstrated their\n+         * chain has sufficient work */\n+        INITIAL_DOWNLOAD,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Start headers sync (via this download-twice mechanism)\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * initial_headers: first batch of headers to process (assumes the caller\n+     *                  has already verified the headers connect)\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    std::optional<CBlockLocator> StartInitialDownload(const CBlockIndex* chain_start, const\n+            std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+            minimum_required_work, CBlockLocator&& chain_start_locator);\n+\n+    struct ProcessingResult {\n+        std::optional<CBlockLocator> locator{std::nullopt};\n+        std::vector<CBlockHeader> headers_to_process;\n+        bool success{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * headers: headers that were received over the network for processing.\n+     *          Assumes the caller has already verified the headers connect.\n+     * full_headers_message: true if the message was at max capacity,\n+     *                       indicating more headers may be available\n+     * ProcessingResult.headers_to_process: will be filled in with any headers that the caller\n+     *                       can process and validate now (because these returned\n+     *                       headers are on a chain with sufficient work)\n+     * ProcessingResult.success: set to false if an error is detected and the sync is\n+     *                       aborted; true otherwise.\n+     * ProcessingResult.locator: if present, the next locator to send in a\n+     *                       getheaders message\n+     */\n+    ProcessingResult ProcessNextHeaders(const std::vector<CBlockHeader>&",
      "path": "src/headerssync.h",
      "position": 168,
      "original_position": 152,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "caa2419e65e3b9d7183f2246d66eb7ce6b7ce1f7",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Passing headers in and out of this function is a bit hard to follow. I think we need a more descriptive term than \"process\" and \"to process\". You could rename the input `headers` to `received_headers` and the return value `headers_to_process` with `pow_checked_headers` or `filtered_headers`.\r\n\r\nBut perhaps it's even more clear if you decouple the feeding of headers into `HeadersSyncState` from the step of extracting filtered headers. Though I suppose these do have to happen in lock step.",
      "created_at": "2022-08-08T10:43:00Z",
      "updated_at": "2022-08-08T11:40:08Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r940093835",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940093835"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 168,
      "original_line": 168,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940138850",
      "pull_request_review_id": 1064970764,
      "id": 940138850,
      "node_id": "PRRC_kwDOABII5844CWFi",
      "diff_hunk": "@@ -5076,6 +5072,24 @@ bool PeerManagerImpl::SendMessages(CNode* pto)\n \n     MaybeSendAddr(*pto, *peer, current_time);\n \n+    // Delay sending SENDHEADERS (BIP 130) until we're done with an\n+    // initial-headers-sync with this peer. Receiving headers announcements for\n+    // new blocks while trying to sync their headers chain is problematic,\n+    // because of the state tracking done.\n+    if (!peer->m_sent_sendheaders && pto->GetCommonVersion() >= SENDHEADERS_VERSION) {",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 32,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "2444ce2f8cb1330ba1dc88b7e5407a3efcfd9aeb",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "2444ce2f8cb1330ba1dc88b7e5407a3efcfd9aeb The above `MaybeSendPing` and `MaybeSendAddr`  suggests we should put this in a helper function `MaybeSendSendHeaders()`",
      "created_at": "2022-08-08T11:38:37Z",
      "updated_at": "2022-08-08T11:40:08Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r940138850",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940138850"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 5077,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940153281",
      "pull_request_review_id": 1065056740,
      "id": 940153281,
      "node_id": "PRRC_kwDOABII5844CZnB",
      "diff_hunk": "@@ -62,7 +63,7 @@ def run_test(self):\n \n         self.log.info(\"Feed all fork headers (succeeds without checkpoint)\")\n         # On node 0 it succeeds because checkpoints are disabled\n-        self.restart_node(0, extra_args=['-nocheckpoints'])\n+        self.restart_node(0, extra_args=['-nocheckpoints', \"-minimumchainwork=0x0\"])",
      "path": "test/functional/p2p_dos_header_tree.py",
      "position": 13,
      "original_position": 13,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 939523672,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Do we need to rethink our headers sync timeout and/or disconnect logic? What happens when the first peer we try to do a headers sync with sends a low work chain? Since we don't disconnect, is there another mechanism that causes us to try with another peer, or do we wait for a timeout or new block announcement? (trying to figure this out from just reading `net_processing.cpp` is daunting...)",
      "created_at": "2022-08-08T11:56:40Z",
      "updated_at": "2022-08-08T11:57:19Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r940153281",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940153281"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 66,
      "original_line": 66,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940174167",
      "pull_request_review_id": 1065087090,
      "id": 940174167,
      "node_id": "PRRC_kwDOABII5844CetX",
      "diff_hunk": "@@ -62,7 +63,7 @@ def run_test(self):\n \n         self.log.info(\"Feed all fork headers (succeeds without checkpoint)\")\n         # On node 0 it succeeds because checkpoints are disabled\n-        self.restart_node(0, extra_args=['-nocheckpoints'])\n+        self.restart_node(0, extra_args=['-nocheckpoints', \"-minimumchainwork=0x0\"])",
      "path": "test/functional/p2p_dos_header_tree.py",
      "position": 13,
      "original_position": 13,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 939523672,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "@sjors Have you seen #25720?",
      "created_at": "2022-08-08T12:21:35Z",
      "updated_at": "2022-08-08T12:21:35Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r940174167",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940174167"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 66,
      "original_line": 66,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940186104",
      "pull_request_review_id": 1065104164,
      "id": 940186104,
      "node_id": "PRRC_kwDOABII5844Chn4",
      "diff_hunk": "@@ -62,7 +63,7 @@ def run_test(self):\n \n         self.log.info(\"Feed all fork headers (succeeds without checkpoint)\")\n         # On node 0 it succeeds because checkpoints are disabled\n-        self.restart_node(0, extra_args=['-nocheckpoints'])\n+        self.restart_node(0, extra_args=['-nocheckpoints', \"-minimumchainwork=0x0\"])",
      "path": "test/functional/p2p_dos_header_tree.py",
      "position": 13,
      "original_position": 13,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 939523672,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I have, but during initial sync it might be a while until the first INV with a block arrives.",
      "created_at": "2022-08-08T12:34:43Z",
      "updated_at": "2022-08-08T12:34:58Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r940186104",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940186104"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 66,
      "original_line": 66,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940190817",
      "pull_request_review_id": 1065111074,
      "id": 940190817,
      "node_id": "PRRC_kwDOABII5844Cixh",
      "diff_hunk": "@@ -62,7 +63,7 @@ def run_test(self):\n \n         self.log.info(\"Feed all fork headers (succeeds without checkpoint)\")\n         # On node 0 it succeeds because checkpoints are disabled\n-        self.restart_node(0, extra_args=['-nocheckpoints'])\n+        self.restart_node(0, extra_args=['-nocheckpoints', \"-minimumchainwork=0x0\"])",
      "path": "test/functional/p2p_dos_header_tree.py",
      "position": 13,
      "original_position": 13,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 939523672,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "This comment goes into more detail: https://github.com/bitcoin/bitcoin/pull/25720/files#r936707629\r\n\r\nBut still, if the first peer gives us low PoW headers - and we don't disconnect from it as we do know when it doesn't contain a checkpoint - that would cause us to do nothing for quite a while. At least IIUC.",
      "created_at": "2022-08-08T12:39:55Z",
      "updated_at": "2022-08-08T12:39:55Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r940190817",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940190817"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 66,
      "original_line": 66,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940194138",
      "pull_request_review_id": 1065115633,
      "id": 940194138,
      "node_id": "PRRC_kwDOABII5844Cjla",
      "diff_hunk": "@@ -62,7 +63,7 @@ def run_test(self):\n \n         self.log.info(\"Feed all fork headers (succeeds without checkpoint)\")\n         # On node 0 it succeeds because checkpoints are disabled\n-        self.restart_node(0, extra_args=['-nocheckpoints'])\n+        self.restart_node(0, extra_args=['-nocheckpoints', \"-minimumchainwork=0x0\"])",
      "path": "test/functional/p2p_dos_header_tree.py",
      "position": 13,
      "original_position": 13,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 939523672,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Oh wait, we don't have to disconnect, as long as `nSyncStarted` is set to 0 we'll try another peer (but we don't do that in this PR).",
      "created_at": "2022-08-08T12:43:25Z",
      "updated_at": "2022-08-08T12:44:11Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r940194138",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940194138"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 66,
      "original_line": 66,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940255846",
      "pull_request_review_id": 1065205311,
      "id": 940255846,
      "node_id": "PRRC_kwDOABII5844Cypm",
      "diff_hunk": "@@ -62,7 +63,7 @@ def run_test(self):\n \n         self.log.info(\"Feed all fork headers (succeeds without checkpoint)\")\n         # On node 0 it succeeds because checkpoints are disabled\n-        self.restart_node(0, extra_args=['-nocheckpoints'])\n+        self.restart_node(0, extra_args=['-nocheckpoints', \"-minimumchainwork=0x0\"])",
      "path": "test/functional/p2p_dos_header_tree.py",
      "position": 13,
      "original_position": 13,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 939523672,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "@Sjors Being in IBD doesn't stop other peers from announcing new blocks to us, IIRC.",
      "created_at": "2022-08-08T13:41:30Z",
      "updated_at": "2022-08-08T13:41:31Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r940255846",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940255846"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 66,
      "original_line": 66,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940261035",
      "pull_request_review_id": 1065212668,
      "id": 940261035,
      "node_id": "PRRC_kwDOABII5844Cz6r",
      "diff_hunk": "@@ -62,7 +63,7 @@ def run_test(self):\n \n         self.log.info(\"Feed all fork headers (succeeds without checkpoint)\")\n         # On node 0 it succeeds because checkpoints are disabled\n-        self.restart_node(0, extra_args=['-nocheckpoints'])\n+        self.restart_node(0, extra_args=['-nocheckpoints', \"-minimumchainwork=0x0\"])",
      "path": "test/functional/p2p_dos_header_tree.py",
      "position": 13,
      "original_position": 13,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 939523672,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "By \"new blocks\" do you mean blocks they know we don't have yet, or blocks they just received? Because the latter is not very frequent.",
      "created_at": "2022-08-08T13:46:07Z",
      "updated_at": "2022-08-08T13:46:07Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r940261035",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940261035"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 66,
      "original_line": 66,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940292962",
      "pull_request_review_id": 1065258690,
      "id": 940292962,
      "node_id": "PRRC_kwDOABII5844C7ti",
      "diff_hunk": "@@ -62,7 +63,7 @@ def run_test(self):\n \n         self.log.info(\"Feed all fork headers (succeeds without checkpoint)\")\n         # On node 0 it succeeds because checkpoints are disabled\n-        self.restart_node(0, extra_args=['-nocheckpoints'])\n+        self.restart_node(0, extra_args=['-nocheckpoints', \"-minimumchainwork=0x0\"])",
      "path": "test/functional/p2p_dos_header_tree.py",
      "position": 13,
      "original_position": 13,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 939523672,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Actually new blocks, once every 10 minutes. That's the current protection in practice against peers stalling headers sync forever by being very slow - once a new block is found on the network, we effectively start headers sync with all peers that announce it to us (unintentionally). #25720 changes it to just one new peer every block.",
      "created_at": "2022-08-08T14:14:40Z",
      "updated_at": "2022-08-08T14:14:40Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r940292962",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940292962"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 66,
      "original_line": 66,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940417640",
      "pull_request_review_id": 1065436351,
      "id": 940417640,
      "node_id": "PRRC_kwDOABII5844DaJo",
      "diff_hunk": "@@ -62,7 +63,7 @@ def run_test(self):\n \n         self.log.info(\"Feed all fork headers (succeeds without checkpoint)\")\n         # On node 0 it succeeds because checkpoints are disabled\n-        self.restart_node(0, extra_args=['-nocheckpoints'])\n+        self.restart_node(0, extra_args=['-nocheckpoints', \"-minimumchainwork=0x0\"])",
      "path": "test/functional/p2p_dos_header_tree.py",
      "position": 13,
      "original_position": 13,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 939523672,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "So currently peers can stall us by sending valid headers slowly (ensuring the first checkpoint is not reached within ~10 minutes). After this PR they can stall us regardless of the how fast the headers are sent. But just sending headers slowly is simple enough to implement, so I suppose this PR does not make that problem worse.",
      "created_at": "2022-08-08T16:07:24Z",
      "updated_at": "2022-08-08T16:07:41Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r940417640",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940417640"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 66,
      "original_line": 66,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940588509",
      "pull_request_review_id": 1065684036,
      "id": 940588509,
      "node_id": "PRRC_kwDOABII5844ED3d",
      "diff_hunk": "@@ -62,7 +63,7 @@ def run_test(self):\n \n         self.log.info(\"Feed all fork headers (succeeds without checkpoint)\")\n         # On node 0 it succeeds because checkpoints are disabled\n-        self.restart_node(0, extra_args=['-nocheckpoints'])\n+        self.restart_node(0, extra_args=['-nocheckpoints', \"-minimumchainwork=0x0\"])",
      "path": "test/functional/p2p_dos_header_tree.py",
      "position": 13,
      "original_position": 13,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 939523672,
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "> So currently peers can stall us by sending valid headers slowly (ensuring the first checkpoint is not reached within ~10 minutes). After this PR they can stall us regardless of the how fast the headers are sent. But just sending headers slowly is simple enough to implement, so I suppose this PR does not make that problem worse.\r\n\r\nThis is not my understanding. Peers that we initially choose for headers sync currently have a time window of ~20 minutes (15min `HEADERS_DOWNLOAD_TIMEOUT_BASE` + a component based on the expected number of headers). If we don't have an up-to-date best header by then (no matter if we even received that from them or another peer), then they get disconnected. This logic is unaffected by this PR, although peers now have to serve the headers chain twice within these 20 minutes, so they need to be twice as fast.\r\nMaybe it could make sense to add some criteria based on actual progress to this - if a peer has failed to serve us a single header for 10 minutes, does it really make sense to give them another 10 minutes?\r\n\r\nThe fact that we may also pick additional peers to sync headers from whenever a new block is found (10 minutes on average, but obviously very fluctuating) is independent from that, and also independent from the behavior of a potentially stalling peer. It could sometimes happen a few seconds after the headerssync start if a lucky block is mined then.\r\n",
      "created_at": "2022-08-08T19:26:10Z",
      "updated_at": "2022-08-08T19:26:11Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r940588509",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940588509"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 66,
      "original_line": 66,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940613684",
      "pull_request_review_id": 1065719833,
      "id": 940613684,
      "node_id": "PRRC_kwDOABII5844EKA0",
      "diff_hunk": "@@ -62,7 +63,7 @@ def run_test(self):\n \n         self.log.info(\"Feed all fork headers (succeeds without checkpoint)\")\n         # On node 0 it succeeds because checkpoints are disabled\n-        self.restart_node(0, extra_args=['-nocheckpoints'])\n+        self.restart_node(0, extra_args=['-nocheckpoints', \"-minimumchainwork=0x0\"])",
      "path": "test/functional/p2p_dos_header_tree.py",
      "position": 13,
      "original_position": 13,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 939523672,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "That's still different from the case where a peer feeds us a set of headers that doesn't include a checkpoint. In that case we immediately disconnect and move on the next peer. That behavior goes away with this PR (though it's probably not a big deal).",
      "created_at": "2022-08-08T19:57:59Z",
      "updated_at": "2022-08-08T19:57:59Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r940613684",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/940613684"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 66,
      "original_line": 66,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943534438",
      "pull_request_review_id": 1069790485,
      "id": 943534438,
      "node_id": "PRRC_kwDOABII5844PTFm",
      "diff_hunk": "@@ -62,7 +63,7 @@ def run_test(self):\n \n         self.log.info(\"Feed all fork headers (succeeds without checkpoint)\")\n         # On node 0 it succeeds because checkpoints are disabled\n-        self.restart_node(0, extra_args=['-nocheckpoints'])\n+        self.restart_node(0, extra_args=['-nocheckpoints', \"-minimumchainwork=0x0\"])",
      "path": "test/functional/p2p_dos_header_tree.py",
      "position": 13,
      "original_position": 13,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 939523672,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "> Do we need to rethink our headers sync timeout and/or disconnect logic? What happens when the first peer we try to do a headers sync with sends a low work chain? Since we don't disconnect, is there another mechanism that causes us to try with another peer, or do we wait for a timeout or new block announcement? (trying to figure this out from just reading net_processing.cpp is daunting...)\r\n\r\n> So currently peers can stall us by sending valid headers slowly (ensuring the first checkpoint is not reached within ~10 minutes). After this PR they can stall us regardless of the how fast the headers are sent. But just sending headers slowly is simple enough to implement, so I suppose this PR does not make that problem worse.\r\n\r\nNote that ever since #5927, checkpoints only affect block/block header validation AFTER we have downloaded a headers chain that includes the checkpointed hashes.  So when a new node is starting up for the first time, if its initial headers sync peer serves a bogus chain, it would still be validated and accepted (even if it includes blocks at heights that have different block hashes checkpointed at those heights), because the main chain with the checkpointed block hashes wouldn't have been downloaded yet.\r\n\r\nBut that distinction is not so important, because for the purposes of (1) determining when the initial headers sync peer is bad/broken, or (2) determining whether a new node starting up can be flooded with low difficulty headers, an adversary has other options to cause us the same problems.  In the case of issue (1), as @mzumsande explained, for bandwidth reasons we give our initial headers sync peer roughly 20 minutes before we'll disconnect them if we haven't yet seen a chain that looks close to caught up.  So a peer looking to stall us can just give us _no headers at all_ (rather than give us a bogus headers chain) and can still stall us for the same amount of time, under that check.  In the case of issue (2), even if block hashes were being checked at the checkpointed heights prior to downloading a chain with those hashes, an adversary looking to waste our memory could just give us a lots of chains that start at genesis and build up to the first checkpoint, and then restart from genesis again with a new chain.\r\n\r\nI believe this PR completely solves issue (2).\r\n\r\nRegarding issue (1): for a very long time, our protection against a stalling peer has been that we will initiate headers sync (via a `getheaders` message) with any peer sending us a block INV.  This is overkill and made somewhat more bandwidth-wasteful after this PR, so #25720 tries to strike a better balance between robustness in the face of stalling peers, and bandwidth usage.  However I believe exactly how we strike that balance is orthogonal to the behavior introduced here, so it makes more sense to discuss strategies for when we start to sync headers with a peer in that PR, rather than here.",
      "created_at": "2022-08-11T14:11:36Z",
      "updated_at": "2022-08-11T14:11:36Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r943534438",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943534438"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 66,
      "original_line": 66,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943541373",
      "pull_request_review_id": 1069800421,
      "id": 943541373,
      "node_id": "PRRC_kwDOABII5844PUx9",
      "diff_hunk": "@@ -2256,6 +2297,35 @@ void PeerManagerImpl::SendBlockTransactions(CNode& pfrom, Peer& peer, const CBlo\n     m_connman.PushMessage(&pfrom, msgMaker.Make(NetMsgType::BLOCKTXN, resp));\n }\n \n+bool PeerManagerImpl::CheckHeadersPoW(const std::vector<CBlockHeader>& headers, const Consensus::Params& consensusParams, Peer& peer)\n+{\n+    // Do these headers have proof-of-work matching what's claimed?\n+    if (!HasValidProofOfWork(headers, consensusParams)) {\n+        Misbehaving(peer, 100, \"header with invalid proof of work\");\n+        return false;\n+    }\n+\n+    // Are these headers connected to each other?\n+    if (!CheckHeadersAreContinuous(headers)) {\n+        Misbehaving(peer, 20, \"non-continuous headers sequence\");\n+        return false;\n+    }\n+    return true;\n+}\n+\n+arith_uint256 PeerManagerImpl::GetAntiDoSWorkThreshold()\n+{\n+    arith_uint256 near_chaintip_work = 0;\n+    LOCK(cs_main);\n+    if (m_chainman.ActiveChain().Tip() != nullptr) {\n+        const CBlockIndex *tip = m_chainman.ActiveChain().Tip();\n+        // Use a 144 block buffer, so that we'll accept headers that fork from\n+        // near our tip.",
      "path": "src/net_processing.cpp",
      "position": 225,
      "original_position": 95,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 939557140,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Yeah, so when I started writing this code I think I had no buffer at all, which turns out to cause problems when there's a small reorg (requiring extra round-trips for blocks to successfully relay).  I think this also caused zillions of our functional tests to fail.\r\n\r\nUpon further thought, I reasoned that for anti-DoS purposes, it's enough to have any kind of work threshold that grows with the tip, rather than is pegged at the tip's work exactly.  So I picked something big enough that we should never have a problem with headers not relaying if they're at all close to the tip, while small enough that I figured an attacker would not be able to keep up for very long unless they had a huge amount of hashpower.  The exact value here is pretty arbitrary though.",
      "created_at": "2022-08-11T14:17:30Z",
      "updated_at": "2022-08-11T14:17:40Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r943541373",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943541373"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2386,
      "original_line": 2386,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943637178",
      "pull_request_review_id": 1069938105,
      "id": 943637178,
      "node_id": "PRRC_kwDOABII5844PsK6",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY = 571;\n+//! Pull out headers for acceptance during redownload once our buffer exceeds this amount\n+constexpr size_t REDOWNLOAD_HEADERS_THRESHOLD = 25*HEADER_COMMITMENT_FREQUENCY;\n+// Ensure that the redownload buffer is a multiple of the header commitment\n+// frequency, so that we aren't wasting memory.\n+static_assert(REDOWNLOAD_HEADERS_THRESHOLD % HEADER_COMMITMENT_FREQUENCY == 0);\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+    std::vector<uint256>().swap(m_chain_start_locator.vHave);\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Initialize the parameters for this headers download, validate this first\n+ * batch, and request more headers. */\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::INITIAL_DOWNLOAD);\n+    assert(m_sync_started == false);\n+\n+    m_sync_started = true;\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+            // Somehow our peer gave us a header that doesn't connect.\n+            // This might be benign -- perhaps we issued an extra getheaders\n+            // message, such as after a block INV was received.\n+            // Or it could be that our peer is broken or malicious. If broken,\n+            // sending a new getheaders immediately could trigger an infinite\n+            // loop. Just give up for now; if our peer ever gives us an block\n+            // INV later we will fetch headers then, and likely retrigger this\n+            // logic.\n+            Finalize();\n+            return ret;",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 106,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "fe620e1a1fb2ef11d06cf941249ad4e5e4250bbf",
      "in_reply_to_id": 933507679,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Incorporated your fix! :)",
      "created_at": "2022-08-11T15:41:09Z",
      "updated_at": "2022-08-11T15:41:09Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r943637178",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943637178"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 106,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943657918",
      "pull_request_review_id": 1069967466,
      "id": 943657918,
      "node_id": "PRRC_kwDOABII5844PxO-",
      "diff_hunk": "@@ -71,6 +71,57 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n     return bnNew.GetCompact();\n }\n \n+// Check that on difficulty adjustments, the new difficulty does not increase\n+// or decrease beyond the permitted limits.\n+bool PermittedDifficultyTransition(const Consensus::Params& params, int64_t height, uint32_t old_nbits, uint32_t new_nbits)\n+{\n+    if (params.fPowAllowMinDifficultyBlocks) return true;\n+\n+    if (height % params.DifficultyAdjustmentInterval() == 0) {\n+        int64_t smallest_timespan = params.nPowTargetTimespan/4;\n+        int64_t largest_timespan = params.nPowTargetTimespan*4;\n+\n+        const arith_uint256 pow_limit = UintToArith256(params.powLimit);\n+        arith_uint256 observed_new_target;\n+        observed_new_target.SetCompact(new_nbits);",
      "path": "src/pow.cpp",
      "position": 16,
      "original_position": 16,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "68c05de96a5b8d0b9a7122d964fe1c9704e2135d",
      "in_reply_to_id": 936071181,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I made an attempt at this suggestion to see if the code would look any simpler, but it just seems to make it slightly more complex to think about (and not any shorter, at least the way I wrote it, which is mimicking the code in `CalculateNextWorkRequired()`).\r\n\r\nAt any rate, I also think the performance effect should be very minor, since we only do this once every 2000 headers (and nothing here should be all that slow to begin with, I think?).\r\n\r\nI'll leave this as-is, and if there's some reason to make further improvements here, perhaps someone can pick it up in a followup PR.",
      "created_at": "2022-08-11T16:01:19Z",
      "updated_at": "2022-08-11T16:01:19Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r943657918",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943657918"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 86,
      "original_line": 86,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943754325",
      "pull_request_review_id": 1070103410,
      "id": 943754325,
      "node_id": "PRRC_kwDOABII5844QIxV",
      "diff_hunk": "@@ -2309,6 +2379,100 @@ bool PeerManagerImpl::CheckHeadersAreContinuous(const std::vector<CBlockHeader>&\n     return true;\n }\n \n+bool PeerManagerImpl::IsContinuationOfLowWorkHeadersSync(Peer& peer, CNode& pfrom, const std::vector<CBlockHeader>& headers, std::vector<CBlockHeader>& previously_downloaded_headers)\n+{\n+    if (peer.m_headers_sync) {\n+        auto result = peer.m_headers_sync->ProcessNextHeaders(headers, headers.size() == MAX_HEADERS_RESULTS);\n+        if (result.locator) {\n+            // If we get back a locator, it should not be empty\n+            Assume(!result.locator->vHave.empty());\n+            if (!result.locator->vHave.empty()) {\n+                // It should be impossible for the getheaders request to fail,\n+                // because we should have cleared the last getheaders timestamp\n+                // when processing the headers that triggered this call. But\n+                // it may be possible to bypass this via compactblock\n+                // processing, so check the result before logging just to be\n+                // safe.\n+                bool sent_getheaders = MaybeSendGetHeaders(pfrom, *result.locator, peer);\n+                if (sent_getheaders) {\n+                    LogPrint(BCLog::NET, \"more getheaders (from %s) to peer=%d\\n\",\n+                            result.locator->vHave.front().ToString(), pfrom.GetId());\n+                }\n+            }\n+        }\n+\n+        if (peer.m_headers_sync->GetState() == HeadersSyncState::State::FINAL) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }\n+\n+        previously_downloaded_headers.swap(result.headers_to_process);\n+\n+        if (previously_downloaded_headers.empty() && result.success) {\n+            // If nothing else was returned and processing was successful, then\n+            // we're all done.\n+            return true;\n+        }\n+    }\n+    // Either we didn't have a sync in progress, or something went wrong\n+    // processing these headers, or we are returning headers to the caller to\n+    // process.\n+    return false;\n+}\n+\n+bool PeerManagerImpl::TryLowWorkHeadersSync(Peer& peer, CNode& pfrom, const CBlockIndex* chain_start_header, const std::vector<CBlockHeader>& headers)\n+{\n+    // Calculate the total work on this chain.\n+    arith_uint256 total_work = chain_start_header->nChainWork + CalculateHeadersWork(headers);\n+\n+    // Our dynamic anti-DoS threshold (minimum work required on a headers chain\n+    // before we'll store it)\n+    arith_uint256 minimum_chain_work = GetAntiDoSWorkThreshold();\n+\n+    // Avoid DoS via low-difficulty-headers by only processing if the headers\n+    // are part of a chain with sufficient work.\n+    if (total_work < minimum_chain_work) {\n+        // Only try to sync with this peer if their headers message was full;\n+        // otherwise they don't have more headers after this so no point in\n+        // trying to sync their too-little-work chain.\n+        if (headers.size() == MAX_HEADERS_RESULTS) {\n+            // Note: we could advance to the last header in this set that is\n+            // known to us, rather than starting at the first header (which we\n+            // may already have). But we actually might have all the headers in\n+            // this set already, because headers sync can be imprecise when a\n+            // peer has to serve us a long chain (due to imprecision in the way\n+            // locators are calculated). So philosophically, if we wanted to\n+            // optimize this correctly, we could consider requesting more\n+            // headers until we find the peer's first new header on this chain,\n+            // and start the sync from there. In practice this is unlikely to\n+            // matter much outside of initial sync, which we generally only do\n+            // once, so for now we'll just start the sync with the first header\n+            // in the set and not worry about this issue.\n+            peer.m_headers_sync.reset(new HeadersSyncState(peer.m_id, m_chainparams.GetConsensus()));\n+            if (std::optional<CBlockLocator> locator =\n+                    peer.m_headers_sync->StartInitialDownload(chain_start_header,\n+                        headers, minimum_chain_work,\n+                        m_chainman.ActiveChain().GetLocator(chain_start_header)))\n+            {\n+                Assume(!locator->vHave.empty());\n+                if (!locator->vHave.empty() && MaybeSendGetHeaders(pfrom, *locator, peer)) {\n+                    LogPrint(BCLog::NET, \"more getheaders (from %s) to end to peer=%d\\n\",",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 184,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 938967065,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "The new log stuff is good enough.",
      "created_at": "2022-08-11T17:42:56Z",
      "updated_at": "2022-08-11T17:42:56Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r943754325",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943754325"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 2458,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943868526",
      "pull_request_review_id": 1070267279,
      "id": 943868526,
      "node_id": "PRRC_kwDOABII5844Qkpu",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY{571};\n+\n+//! Only feed headers to validation once this many commitment bits on top have been checked.\n+//! During redownloading, a per-peer buffer with space for\n+//! HEADER_COMMITMENT_FREQUENCY * REDOWNLOAD_HEADERS_CHECK_BITS headers will be used.\n+constexpr size_t REDOWNLOAD_HEADERS_CHECK_BITS{25};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+    std::vector<uint256>().swap(m_chain_start_locator.vHave);\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Initialize the parameters for this headers download, validate this first\n+ * batch, and request more headers. */\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::UNSTARTED);\n+    m_download_state = State::INITIAL_DOWNLOAD;\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return ret;\n+        }\n+        ret.success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            ret.locator = MakeNextHeadersRequest();\n+            return ret;\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            LogPrint(BCLog::NET, \"Ignoring low-work chain (height=%u) from peer=%d\\n\", m_current_height, m_id);\n+            Finalize();\n+            return ret;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return ret;\n+            }\n+        }\n+\n+        ret.success = true;\n+        // Return any headers that are ready for acceptance.\n+        ret.headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return ret;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            ret.locator = MakeNextHeadersRequest();\n+            return ret;\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return ret;\n+        }\n+    }\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps we issued an extra getheaders\n+        // message, such as after a block INV was received.\n+        // Or it could be that our peer is broken or malicious. If broken,\n+        // sending a new getheaders immediately could trigger an infinite\n+        // loop. Just give up for now; if our peer ever gives us an block\n+        // INV later we will fetch headers then, and likely retrigger this\n+        // logic.\n+        Finalize();\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr, m_current_height+1)) {\n+            Finalize();\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_blockhash_with_sufficient_work = headers.back().GetHash();\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_download_state = State::REDOWNLOAD;",
      "path": "src/headerssync.cpp",
      "position": 171,
      "original_position": 192,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 938966167,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-11T19:46:36Z",
      "updated_at": "2022-08-11T19:46:36Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r943868526",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943868526"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 171,
      "original_line": 171,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943881524",
      "pull_request_review_id": 1070286831,
      "id": 943881524,
      "node_id": "PRRC_kwDOABII5844Qn00",
      "diff_hunk": "@@ -0,0 +1,255 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, we can calculate the work on the chain as we\n+ * go (just by checking the nBits value on each header, and validating the\n+ * proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1, store 1 bit (using a salted hash function) for every N headers\n+ * that we see. With a reasonable choice of N, this uses relatively little\n+ * memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params);\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        UNSTARTED,\n+        /** INITIAL_DOWNLOAD means the peer has not yet demonstrated their\n+         * chain has sufficient work */\n+        INITIAL_DOWNLOAD,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Start headers sync (via this download-twice mechanism)\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * initial_headers: first batch of headers to process (assumes the caller\n+     *                  has already verified the headers connect)\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    std::optional<CBlockLocator> StartInitialDownload(const CBlockIndex* chain_start, const\n+            std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+            minimum_required_work, CBlockLocator&& chain_start_locator);\n+\n+    struct ProcessingResult {\n+        std::optional<CBlockLocator> locator{std::nullopt};\n+        std::vector<CBlockHeader> headers_to_process;\n+        bool success{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * headers: headers that were received over the network for processing.\n+     *          Assumes the caller has already verified the headers connect.\n+     * full_headers_message: true if the message was at max capacity,\n+     *                       indicating more headers may be available\n+     * ProcessingResult.headers_to_process: will be filled in with any headers that the caller\n+     *                       can process and validate now (because these returned\n+     *                       headers are on a chain with sufficient work)\n+     * ProcessingResult.success: set to false if an error is detected and the sync is\n+     *                       aborted; true otherwise.\n+     * ProcessingResult.locator: if present, the next locator to send in a\n+     *                       getheaders message\n+     */\n+    ProcessingResult ProcessNextHeaders(const std::vector<CBlockHeader>&",
      "path": "src/headerssync.h",
      "position": 168,
      "original_position": 152,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "caa2419e65e3b9d7183f2246d66eb7ce6b7ce1f7",
      "in_reply_to_id": 940093835,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I'm doing the rename you suggest, but yes pulling headers out of `HeadersSyncState` needs to happen in lockstep with processing, so I think rather than add another API call to extract them, it makes the most sense to just return them.",
      "created_at": "2022-08-11T20:01:12Z",
      "updated_at": "2022-08-11T20:01:12Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r943881524",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943881524"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 168,
      "original_line": 168,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943922645",
      "pull_request_review_id": 1070341778,
      "id": 943922645,
      "node_id": "PRRC_kwDOABII5844Qx3V",
      "diff_hunk": "@@ -0,0 +1,332 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY{571};\n+\n+//! Only feed headers to validation once this many commitment bits on top have been checked.\n+//! During redownloading, a per-peer buffer with space for\n+//! HEADER_COMMITMENT_FREQUENCY * REDOWNLOAD_HEADERS_CHECK_BITS headers will be used.\n+constexpr size_t REDOWNLOAD_HEADERS_CHECK_BITS{25};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+    std::vector<uint256>().swap(m_chain_start_locator.vHave);\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Initialize the parameters for this headers download, validate this first\n+ * batch, and request more headers. */\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::UNSTARTED);\n+    m_download_state = State::INITIAL_DOWNLOAD;\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.\n+        if (!ValidateAndStoreHeadersCommitments(headers)) {\n+            // The headers didn't pass validation; give up on the sync.\n+            return ret;\n+        }\n+        ret.success = true;\n+        if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+            // A full headers message means the peer may have more to give us;\n+            // also if we just switched to REDOWNLOAD then we need to re-request\n+            // headers from the beginning.\n+            ret.locator = MakeNextHeadersRequest();\n+            return ret;\n+        } else {\n+            // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+            // message, then the peer's chain has ended and definitely doesn't\n+            // have enough work, so we can stop our sync.\n+            LogPrint(BCLog::NET, \"Ignoring low-work chain (height=%u) from peer=%d\\n\", m_current_height, m_id);\n+            Finalize();\n+            return ret;\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        for (const auto& hdr : headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                Finalize();\n+                return ret;\n+            }\n+        }\n+\n+        ret.success = true;\n+        // Return any headers that are ready for acceptance.\n+        ret.headers_to_process = RemoveHeadersReadyForAcceptance();\n+\n+        // If we hit our target blockhash, then all remaining headers will be\n+        // returned and we can clear any leftover internal state.\n+        if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+            Finalize();\n+            return ret;\n+        }\n+\n+        // If the headers message is full, we need to request more.\n+        if (full_headers_message) {\n+            ret.locator = MakeNextHeadersRequest();\n+            return ret;\n+        } else {\n+            // For some reason our peer gave us a high-work chain, but is now\n+            // declining to serve us that full chain again. Give up.\n+            // Note that there's no more processing to be done with these\n+            // headers, so we can still return success.\n+            Finalize();\n+            return ret;\n+        }\n+    }\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps we issued an extra getheaders\n+        // message, such as after a block INV was received.\n+        // Or it could be that our peer is broken or malicious. If broken,\n+        // sending a new getheaders immediately could trigger an infinite\n+        // loop. Just give up for now; if our peer ever gives us an block\n+        // INV later we will fetch headers then, and likely retrigger this\n+        // logic.\n+        Finalize();\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr, m_current_height+1)) {\n+            Finalize();\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current, int64_t current_height)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, current_height,\n+                previous.nBits, current.nBits)) {\n+        return false;\n+    }\n+\n+    if (!CheckProofOfWork(current.GetHash(), current.nBits, m_consensus_params)) return false;\n+\n+    if ((current_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+        // Add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // It's possible the chain grew since we started the sync; so\n+            // potentially we could succeed in syncing the peer's chain if we\n+            // try again later.\n+            LogPrint(BCLog::NET, \"headers chain is too long; giving up sync peer=%d\\n\", m_id);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));\n+    m_last_header_received = current;\n+    ++m_current_height;\n+\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreRedownloadedHeader(const CBlockHeader& header)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    // Ensure that we're working on a header that connects to the chain we're\n+    // downloading.\n+    if (header.hashPrevBlock != m_redownload_buffer_last_hash) {\n+        return false;\n+    }\n+\n+    int64_t next_height = m_redownload_buffer_last_height + 1;\n+\n+    // Check that the difficulty adjustments are within our tolerance:\n+    uint32_t previous_nBits{0};\n+    if (!m_redownloaded_headers.empty()) {\n+        previous_nBits = m_redownloaded_headers.back().nBits;\n+    } else {\n+        previous_nBits = m_chain_start->nBits;\n+    }\n+\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous_nBits, header.nBits)) {\n+        return false;\n+    }\n+\n+    // Track work on the redownloaded chain\n+    m_redownload_chain_work += GetBlockProof(CBlockIndex(header));\n+\n+    if (m_redownload_chain_work >= m_minimum_required_work) {\n+        m_process_all_remaining_headers = true;\n+    }\n+\n+    // If we're at a header for which we previously stored a commitment, verify\n+    // it is correct. Failure will result in aborting download.\n+    // Also, don't check commitments once we've gotten to our target blockhash;\n+    // it's possible our peer has extended its chain between our first sync and\n+    // our second, and we don't want to return failure after we've seen our\n+    // target blockhash just because we ran out of commitments.\n+    if (!m_process_all_remaining_headers && (next_height - m_chain_start->nHeight) % HEADER_COMMITMENT_FREQUENCY == 0) {\n+         bool commitment = m_hasher(header.GetHash()) & 1;\n+         if (m_header_commitments.size() == 0) {\n+            // Somehow our peer managed to feed us a different chain and\n+            // we've run out of commitments.\n+            return false;\n+        }\n+        bool expected_commitment = m_header_commitments.front();\n+        m_header_commitments.pop_front();\n+        if (commitment != expected_commitment) {\n+            return false;\n+        }\n+    }\n+\n+    // Store this header for later processing.\n+    m_redownloaded_headers.push_back(header);\n+    m_redownload_buffer_last_height = next_height;\n+    m_redownload_buffer_last_hash = header.GetHash();\n+\n+    return true;\n+}\n+\n+std::vector<CBlockHeader> HeadersSyncState::RemoveHeadersReadyForAcceptance()",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 294,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "caa2419e65e3b9d7183f2246d66eb7ce6b7ce1f7",
      "in_reply_to_id": 940092129,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-11T20:41:05Z",
      "updated_at": "2022-08-11T20:41:05Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r943922645",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943922645"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 285,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943922880",
      "pull_request_review_id": 1070342076,
      "id": 943922880,
      "node_id": "PRRC_kwDOABII5844Qx7A",
      "diff_hunk": "@@ -5076,6 +5072,24 @@ bool PeerManagerImpl::SendMessages(CNode* pto)\n \n     MaybeSendAddr(*pto, *peer, current_time);\n \n+    // Delay sending SENDHEADERS (BIP 130) until we're done with an\n+    // initial-headers-sync with this peer. Receiving headers announcements for\n+    // new blocks while trying to sync their headers chain is problematic,\n+    // because of the state tracking done.\n+    if (!peer->m_sent_sendheaders && pto->GetCommonVersion() >= SENDHEADERS_VERSION) {",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 32,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "2444ce2f8cb1330ba1dc88b7e5407a3efcfd9aeb",
      "in_reply_to_id": 940138850,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done. :)",
      "created_at": "2022-08-11T20:41:23Z",
      "updated_at": "2022-08-11T20:41:23Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r943922880",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943922880"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 5077,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943923063",
      "pull_request_review_id": 1070342325,
      "id": 943923063,
      "node_id": "PRRC_kwDOABII5844Qx93",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY{571};\n+\n+//! Only feed headers to validation once this many commitment bits on top have been checked.\n+//! During redownloading, a per-peer buffer with space for\n+//! HEADER_COMMITMENT_FREQUENCY * REDOWNLOAD_HEADERS_CHECK_BITS headers will be used.\n+constexpr size_t REDOWNLOAD_HEADERS_CHECK_BITS{25};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params) :\n+    m_id(id), m_consensus_params(consensus_params)\n+{ }\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_blockhash_with_sufficient_work.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+    std::vector<uint256>().swap(m_chain_start_locator.vHave);\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Initialize the parameters for this headers download, validate this first\n+ * batch, and request more headers. */\n+std::optional<CBlockLocator> HeadersSyncState::StartInitialDownload(const CBlockIndex* chain_start,\n+        const std::vector<CBlockHeader>& initial_headers, const arith_uint256&\n+        minimum_required_work, CBlockLocator&& chain_start_locator)\n+{\n+    // A new instance of this object should be instantiated for every headers\n+    // sync, so that we don't reuse our salted hasher between syncs.\n+    assert(m_download_state == State::UNSTARTED);\n+    m_download_state = State::INITIAL_DOWNLOAD;\n+\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+    m_chain_start_locator = std::move(chain_start_locator);\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_FREQUENCY;\n+\n+    if (!ValidateAndStoreHeadersCommitments(initial_headers)) {\n+        return std::nullopt;\n+    }\n+    return MakeNextHeadersRequest();\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!headers.empty());\n+    if (headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold.",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 94,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 939559233,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-11T20:41:38Z",
      "updated_at": "2022-08-11T20:41:38Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r943923063",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943923063"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 80,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943923287",
      "pull_request_review_id": 1070342657,
      "id": 943923287,
      "node_id": "PRRC_kwDOABII5844QyBX",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_FREQUENCY blocks\n+constexpr size_t HEADER_COMMITMENT_FREQUENCY{571};",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 8,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5691c174c5cb2f9947e6cc2db8d32b4b4715e47d",
      "in_reply_to_id": 938950210,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed.",
      "created_at": "2022-08-11T20:41:57Z",
      "updated_at": "2022-08-11T20:41:58Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r943923287",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943923287"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 8,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943923628",
      "pull_request_review_id": 1070343157,
      "id": 943923628,
      "node_id": "PRRC_kwDOABII5844QyGs",
      "diff_hunk": "@@ -71,6 +71,57 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n     return bnNew.GetCompact();\n }\n \n+// Check that on difficulty adjustments, the new difficulty does not increase\n+// or decrease beyond the permitted limits.\n+bool PermittedDifficultyTransition(const Consensus::Params& params, int64_t height, uint32_t old_nbits, uint32_t new_nbits)",
      "path": "src/pow.cpp",
      "position": 6,
      "original_position": 6,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "68c05de96a5b8d0b9a7122d964fe1c9704e2135d",
      "in_reply_to_id": 938148745,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Included your test here, thanks! :)",
      "created_at": "2022-08-11T20:42:28Z",
      "updated_at": "2022-08-11T20:42:28Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r943923628",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/943923628"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 76,
      "original_line": 76,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/944094399",
      "pull_request_review_id": 1070569040,
      "id": 944094399,
      "node_id": "PRRC_kwDOABII5844Rby_",
      "diff_hunk": "@@ -0,0 +1,349 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD))\n+{\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps we issued an extra getheaders\n+        // message, such as after a block INV was received.\n+        // Or it could be that our peer is broken or malicious. If broken,\n+        // sending a new getheaders immediately could trigger an infinite\n+        // loop. Just give up for now; if our peer ever gives us an block\n+        // INV later we will fetch headers then, and likely retrigger this\n+        // logic.\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (commitment phase)\\n\", m_id, next_height);",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 191,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6862e74c70a343d598e7e9fdf82b41bbb748cf6e",
      "in_reply_to_id": null,
      "user": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Any reason for these to be their own category rather than NET?",
      "created_at": "2022-08-12T03:54:28Z",
      "updated_at": "2022-08-15T11:14:47Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r944094399",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/944094399"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 191,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/945610729",
      "pull_request_review_id": 1070569040,
      "id": 945610729,
      "node_id": "PRRC_kwDOABII5844XN_p",
      "diff_hunk": "@@ -2310,6 +2380,100 @@ bool PeerManagerImpl::CheckHeadersAreContinuous(const std::vector<CBlockHeader>&\n     return true;\n }\n \n+bool PeerManagerImpl::IsContinuationOfLowWorkHeadersSync(Peer& peer, CNode& pfrom, const std::vector<CBlockHeader>& headers, std::vector<CBlockHeader>& previously_downloaded_headers)\n+{\n+    if (peer.m_headers_sync) {\n+        auto result = peer.m_headers_sync->ProcessNextHeaders(headers, headers.size() == MAX_HEADERS_RESULTS);\n+        if (result.request_more) {\n+            // Use the parent of the best known header as tip to mix into the locator to send out.\n+            // This matches the behavior of the initial getheaders locator sent, for the same\n+            // reason: making sure we always get information back we can use to prime headers\n+            // synchronization structures with.\n+            const CBlockIndex* headers_tip = WITH_LOCK(::cs_main, return m_chainman.m_best_header);\n+            if (headers_tip && headers_tip->pprev) headers_tip = headers_tip->pprev;\n+            auto locator = peer.m_headers_sync->MakeNextHeadersRequest(headers_tip);\n+            // If we were instructed to ask for a locator, it should not be empty.\n+            Assume(!locator.vHave.empty());\n+            if (!locator.vHave.empty()) {\n+                // It should be impossible for the getheaders request to fail,\n+                // because we should have cleared the last getheaders timestamp\n+                // when processing the headers that triggered this call. But\n+                // it may be possible to bypass this via compactblock\n+                // processing, so check the result before logging just to be\n+                // safe.\n+                bool sent_getheaders = MaybeSendGetHeaders(pfrom, locator, peer);\n+                if (sent_getheaders) {\n+                    LogPrint(BCLog::NET, \"more getheaders (from %s) to peer=%d\\n\",\n+                            locator.vHave.front().ToString(), pfrom.GetId());\n+                }\n+            }\n+        }\n+\n+        if (peer.m_headers_sync->GetState() == HeadersSyncState::State::FINAL) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 139,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6862e74c70a343d598e7e9fdf82b41bbb748cf6e",
      "in_reply_to_id": null,
      "user": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think it might make sense to add an `if (!result.success) return true;` when you end up in FINAL state -- otherwise (at least for me) it looks like ordinary header processing then immediately triggers a restart.\r\n\r\n",
      "created_at": "2022-08-15T10:47:20Z",
      "updated_at": "2022-08-15T11:14:47Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r945610729",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/945610729"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 2412,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 2515,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/945612695",
      "pull_request_review_id": 1070569040,
      "id": 945612695,
      "node_id": "PRRC_kwDOABII5844XOeX",
      "diff_hunk": "@@ -2458,18 +2622,54 @@ void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n                                             const std::vector<CBlockHeader>& headers,\n                                             bool via_compact_block)\n {\n-    const CNetMsgMaker msgMaker(pfrom.GetCommonVersion());\n     size_t nCount = headers.size();\n \n+    LOCK(peer.m_headers_sync_mutex);\n+\n     if (nCount == 0) {\n         // Nothing interesting. Stop asking this peers for more headers.\n+        // If we were in the middle of headers sync, receiving an empty headers\n+        // message suggests that the peer suddenly has nothing to give us\n+        // (perhaps it reorged to our chain). Clear download state for this peer.\n+        if (peer.m_headers_sync) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }\n+        return;\n+    }\n+\n+    // Before we do any processing, make sure these pass basic sanity checks.\n+    // We'll rely on headers having valid proof-of-work further down, as an\n+    // anti-DoS criteria.\n+    if (!CheckHeadersPoW(headers, m_chainparams.GetConsensus(), peer)) {\n         return;\n     }\n \n     const CBlockIndex *pindexLast = nullptr;\n \n+    // The headers-sync logic (m_headers_sync) is responsible for telling us\n+    // what headers to process; if it returns headers to us then we'll run\n+    // those through validation instead of the ones received off the network.\n+    // Use this pointer and a local vector for keeping track of this.\n+    const std::vector<CBlockHeader>* headers_to_process = &headers;\n+    std::vector<CBlockHeader> previously_downloaded_headers;\n+\n+    // We'll set already_validated_work to true if the headers-sync logic\n+    // returns headers for us to process, to bypass the minimum work check\n+    // (which is done separately inside m_headers_sync)\n+    bool already_validated_work = false;\n+\n+    // If we're in the middle of headers sync, let it do its magic.\n+    if (IsContinuationOfLowWorkHeadersSync(peer, pfrom, headers, previously_downloaded_headers)) {\n+        return;",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 248,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6862e74c70a343d598e7e9fdf82b41bbb748cf6e",
      "in_reply_to_id": null,
      "user": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "If the suggestion above is adopted, then header sync failure will result in exiting here without a request for more headers being sent out to this peer. In that case, I think you'd also want to add code to toggle `fSyncStarted` and decrement `nSyncStarted` to allow another peer to pick up where this one left of. Needs some care to avoid `cs_main` lock ordering problems of course.",
      "created_at": "2022-08-15T10:50:43Z",
      "updated_at": "2022-08-15T11:14:47Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r945612695",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/945612695"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 2663,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/945622956",
      "pull_request_review_id": 1070569040,
      "id": 945622956,
      "node_id": "PRRC_kwDOABII5844XQ-s",
      "diff_hunk": "@@ -0,0 +1,349 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD))\n+{\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {",
      "path": "src/headerssync.cpp",
      "position": 105,
      "original_position": 102,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6862e74c70a343d598e7e9fdf82b41bbb748cf6e",
      "in_reply_to_id": null,
      "user": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "We expect to skip headers here due to including locators based on our tip, but `ValidatedAndStoreRedownloadedHeader` can't cope with skipped headers. This doesn't seem sensible to me.",
      "created_at": "2022-08-15T11:08:54Z",
      "updated_at": "2022-08-15T11:14:47Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r945622956",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/945622956"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 105,
      "original_line": 105,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/945959497",
      "pull_request_review_id": 1072958922,
      "id": 945959497,
      "node_id": "PRRC_kwDOABII5844YjJJ",
      "diff_hunk": "@@ -0,0 +1,264 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, we can calculate the work on the chain as we\n+ * go (just by checking the nBits value on each header, and validating the\n+ * proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1, store 1 bit (using a salted hash function) for every N headers\n+ * that we see. With a reasonable choice of N, this uses relatively little\n+ * memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** INITIAL_DOWNLOAD means the peer has not yet demonstrated their\n+         * chain has sufficient work */\n+        INITIAL_DOWNLOAD,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Construct a HeadersSyncState object representing a headers sync via this\n+     *  download-twice mechanism).\n+     *\n+     * id: node id (for logging)\n+     * consensus_params: parameters needed for difficulty adjustment validation\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+            const CBlockIndex* chain_start, const arith_uint256& minimum_required_work);\n+\n+    /** Result data structure for ProcessNextHeaders. */\n+    struct ProcessingResult {\n+        std::vector<CBlockHeader> pow_validated_headers;\n+        bool success{false};\n+        bool request_more{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * received_headers: headers that were received over the network for processing.\n+     *                   Assumes the caller has already verified the headers connect.\n+     * full_headers_message: true if the message was at max capacity,\n+     *                       indicating more headers may be available\n+     * ProcessingResult.pow_validated_headers: will be filled in with any\n+     *                       headers that the caller can fully process and\n+     *                       validate now (because these returned headers are\n+     *                       on a chain with sufficient work)\n+     * ProcessingResult.success: set to false if an error is detected and the sync is\n+     *                       aborted; true otherwise.\n+     * ProcessingResult.request_more: if true, the caller is suggested to call\n+     *                       MakeNextHeadersRequest and send a getheaders message using it.\n+     */\n+    ProcessingResult ProcessNextHeaders(const std::vector<CBlockHeader>&\n+            received_headers, bool full_headers_message);\n+\n+    /** Issue the next GETHEADERS message to our peer.\n+     *\n+     * This will return a locator appropriate for the current sync object, to continue the\n+     * synchronization phase it is in. If tip is provided, the locator will also have entries from\n+     * that tip. This permits the remote party to help us \"skip ahead\" if our global state has\n+     * progressed past where this sync object currently is (because another sync peer was faster,\n+     * perhaps). */\n+    CBlockLocator MakeNextHeadersRequest(const CBlockIndex* tip) const;\n+\n+private:\n+    /** Clear out all download state that might be in progress (freeing any used\n+     * memory), and mark this object as no longer usable.\n+     */\n+    void Finalize();\n+\n+    /**\n+     *  Only called in INITIAL_DOWNLOAD.\n+     *  Validate the work on the headers we received from the network, and\n+     *  store commitments for later. Update overall state with successfully\n+     *  processed headers.\n+     *  On failure, this invokes Finalize() and returns false.\n+     */\n+    bool ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers);\n+\n+    /** In INITIAL_DOWNLOAD, process and update state for a single header */\n+    bool ValidateAndProcessSingleHeader(const CBlockHeader& previous, const\n+            CBlockHeader& current);\n+\n+    /** In REDOWNLOAD, check a header's commitment (if applicable) and add to\n+     * buffer for later processing */\n+    bool ValidateAndStoreRedownloadedHeader(const CBlockHeader& header);\n+\n+    /** Return a set of headers that satisfy our proof-of-work threshold */\n+    std::vector<CBlockHeader> PopHeadersReadyForAcceptance();\n+\n+private:\n+    /** NodeId of the peer (used for log messages) **/\n+    const NodeId m_id;\n+\n+    /** We use the consensus params in our anti-DoS calculations */\n+    const Consensus::Params& m_consensus_params;\n+\n+    /** Store the last block in our block index that the peer's chain builds from */\n+    const CBlockIndex* m_chain_start{nullptr};\n+\n+    /** Minimum work that we're looking for on this chain. */\n+    arith_uint256 m_minimum_required_work;\n+\n+    /** Work that we've seen so far on the peer's chain */\n+    arith_uint256 m_current_chain_work;\n+\n+    /** m_hasher is a salted hasher for making our 1-bit commitments to headers we've seen. */\n+    const SaltedTxidHasher m_hasher;\n+\n+    /** A queue of commitment bits, created during the 1st phase, and verified during the 2nd. */\n+    bitdeque<> m_header_commitments;\n+\n+    /** The (secret) offset on the heights for which to create commitments.\n+     *\n+     * m_header_commitments entries are created at any height h for which\n+     * (h % HEADER_COMMITMENT_PERIOD) == m_commit_offset. */\n+    const unsigned m_commit_offset;\n+\n+    /** m_max_commitments is a bound we calculate on how long an honest peer's chain could be,\n+     * given the MTP rule.\n+     *\n+     * Any peer giving us more headers than this will have its sync aborted. This serves as a\n+     * memory bound on m_header_commitments. */\n+    uint64_t m_max_commitments{0};\n+\n+    /** Store the latest header received while in INITIAL_DOWNLOAD */\n+    CBlockHeader m_last_header_received;\n+\n+    /** Height of m_last_header_received */\n+    int64_t m_current_height{0}; // height of last header received",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 230,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "57d87914639e60b0292748b24944501541cdc88e",
      "in_reply_to_id": null,
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "nit: duplicate comment (see line above)",
      "created_at": "2022-08-15T17:12:26Z",
      "updated_at": "2022-08-15T22:38:26Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r945959497",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/945959497"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 238,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/945975925",
      "pull_request_review_id": 1072958922,
      "id": 945975925,
      "node_id": "PRRC_kwDOABII5844YnJ1",
      "diff_hunk": "@@ -2458,18 +2624,58 @@ void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n                                             const std::vector<CBlockHeader>& headers,\n                                             bool via_compact_block)\n {\n-    const CNetMsgMaker msgMaker(pfrom.GetCommonVersion());\n     size_t nCount = headers.size();\n \n     if (nCount == 0) {\n         // Nothing interesting. Stop asking this peers for more headers.\n+        // If we were in the middle of headers sync, receiving an empty headers\n+        // message suggests that the peer suddenly has nothing to give us\n+        // (perhaps it reorged to our chain). Clear download state for this peer.\n+        LOCK(peer.m_headers_sync_mutex);\n+        if (peer.m_headers_sync) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }\n+        return;\n+    }\n+\n+    // Before we do any processing, make sure these pass basic sanity checks.\n+    // We'll rely on headers having valid proof-of-work further down, as an\n+    // anti-DoS criteria.\n+    if (!CheckHeadersPoW(headers, m_chainparams.GetConsensus(), peer)) {\n         return;",
      "path": "src/net_processing.cpp",
      "position": 424,
      "original_position": 230,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "57d87914639e60b0292748b24944501541cdc88e",
      "in_reply_to_id": null,
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Should we call `Misbehaving()` if the PoW is invalid?",
      "created_at": "2022-08-15T17:34:04Z",
      "updated_at": "2022-08-15T22:38:26Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r945975925",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/945975925"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2758,
      "original_line": 2758,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946010666",
      "pull_request_review_id": 1073031147,
      "id": 946010666,
      "node_id": "PRRC_kwDOABII5844Yvoq",
      "diff_hunk": "@@ -0,0 +1,349 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD))\n+{\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps we issued an extra getheaders\n+        // message, such as after a block INV was received.\n+        // Or it could be that our peer is broken or malicious. If broken,\n+        // sending a new getheaders immediately could trigger an infinite\n+        // loop. Just give up for now; if our peer ever gives us an block\n+        // INV later we will fetch headers then, and likely retrigger this\n+        // logic.\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (commitment phase)\\n\", m_id, next_height);",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 191,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6862e74c70a343d598e7e9fdf82b41bbb748cf6e",
      "in_reply_to_id": 944094399,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think just for code-review/testing's sake -- NET gets a lot of traffic.",
      "created_at": "2022-08-15T18:22:34Z",
      "updated_at": "2022-08-15T18:22:34Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r946010666",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946010666"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 191,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946028190",
      "pull_request_review_id": 1073055197,
      "id": 946028190,
      "node_id": "PRRC_kwDOABII5844Yz6e",
      "diff_hunk": "@@ -2310,6 +2380,100 @@ bool PeerManagerImpl::CheckHeadersAreContinuous(const std::vector<CBlockHeader>&\n     return true;\n }\n \n+bool PeerManagerImpl::IsContinuationOfLowWorkHeadersSync(Peer& peer, CNode& pfrom, const std::vector<CBlockHeader>& headers, std::vector<CBlockHeader>& previously_downloaded_headers)\n+{\n+    if (peer.m_headers_sync) {\n+        auto result = peer.m_headers_sync->ProcessNextHeaders(headers, headers.size() == MAX_HEADERS_RESULTS);\n+        if (result.request_more) {\n+            // Use the parent of the best known header as tip to mix into the locator to send out.\n+            // This matches the behavior of the initial getheaders locator sent, for the same\n+            // reason: making sure we always get information back we can use to prime headers\n+            // synchronization structures with.\n+            const CBlockIndex* headers_tip = WITH_LOCK(::cs_main, return m_chainman.m_best_header);\n+            if (headers_tip && headers_tip->pprev) headers_tip = headers_tip->pprev;\n+            auto locator = peer.m_headers_sync->MakeNextHeadersRequest(headers_tip);\n+            // If we were instructed to ask for a locator, it should not be empty.\n+            Assume(!locator.vHave.empty());\n+            if (!locator.vHave.empty()) {\n+                // It should be impossible for the getheaders request to fail,\n+                // because we should have cleared the last getheaders timestamp\n+                // when processing the headers that triggered this call. But\n+                // it may be possible to bypass this via compactblock\n+                // processing, so check the result before logging just to be\n+                // safe.\n+                bool sent_getheaders = MaybeSendGetHeaders(pfrom, locator, peer);\n+                if (sent_getheaders) {\n+                    LogPrint(BCLog::NET, \"more getheaders (from %s) to peer=%d\\n\",\n+                            locator.vHave.front().ToString(), pfrom.GetId());\n+                }\n+            }\n+        }\n+\n+        if (peer.m_headers_sync->GetState() == HeadersSyncState::State::FINAL) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 139,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6862e74c70a343d598e7e9fdf82b41bbb748cf6e",
      "in_reply_to_id": 945610729,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "As mentioned [here](https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1215121609), the intent is indeed for ordinary headers processing -- that is, a headers message which connects to a different point in the locator we send than continuing from the last header they sent us -- to trigger a restart of this logic (hopefully at a later point in the sync, but potentially anywhere, e.g. if the peer underwent a reorg during sync).",
      "created_at": "2022-08-15T18:46:27Z",
      "updated_at": "2022-08-15T18:47:24Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r946028190",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946028190"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 2412,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 2515,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946031541",
      "pull_request_review_id": 1073059611,
      "id": 946031541,
      "node_id": "PRRC_kwDOABII5844Y0u1",
      "diff_hunk": "@@ -0,0 +1,349 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD))\n+{\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {",
      "path": "src/headerssync.cpp",
      "position": 105,
      "original_position": 102,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6862e74c70a343d598e7e9fdf82b41bbb748cf6e",
      "in_reply_to_id": 945622956,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Again see https://github.com/bitcoin/bitcoin/pull/25717#issuecomment-1215121609; the intent is that by using a locator which mixes in entries from `m_best_header`, the peer can skip ahead in the sync (in a scenario where our best header is updating from other peers) to reduce overall bandwidth.  In such an event, this code would return an error and we'd drop into starting a new sync with the peer.",
      "created_at": "2022-08-15T18:50:52Z",
      "updated_at": "2022-08-15T18:50:52Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r946031541",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946031541"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 105,
      "original_line": 105,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946129185",
      "pull_request_review_id": 1073217147,
      "id": 946129185,
      "node_id": "PRRC_kwDOABII5844ZMkh",
      "diff_hunk": "@@ -0,0 +1,349 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD))\n+{\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps we issued an extra getheaders\n+        // message, such as after a block INV was received.\n+        // Or it could be that our peer is broken or malicious. If broken,\n+        // sending a new getheaders immediately could trigger an infinite\n+        // loop. Just give up for now; if our peer ever gives us an block\n+        // INV later we will fetch headers then, and likely retrigger this\n+        // logic.\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 180,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1178015251732b2db87c0b84dd7ebe3f435a6771",
      "in_reply_to_id": null,
      "user": {
        "login": "achow101",
        "id": 3782274,
        "node_id": "MDQ6VXNlcjM3ODIyNzQ=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3782274?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/achow101",
        "html_url": "https://github.com/achow101",
        "followers_url": "https://api.github.com/users/achow101/followers",
        "following_url": "https://api.github.com/users/achow101/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/achow101/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/achow101/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/achow101/subscriptions",
        "organizations_url": "https://api.github.com/users/achow101/orgs",
        "repos_url": "https://api.github.com/users/achow101/repos",
        "events_url": "https://api.github.com/users/achow101/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/achow101/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "In 1178015251732b2db87c0b84dd7ebe3f435a6771 \"Utilize anti-DoS headers download strategy\"\r\n\r\n`ValidateAndStoreHeadersCommitments`, `ValidateAndProcessSingleHeader`, `ValidateAndStoreRedownloadedHeader`, and `PopHeadersReadyForAcceptance` all have this same check. However they really only have a single valid state they should be operating in, so I think it would be better to check whether the current state is what is expected (`INITIAL_DOWNLOAD` for the first two, and `REDOWNLOAD` for the second) rather than that it is not `FINAL`.",
      "created_at": "2022-08-15T21:10:38Z",
      "updated_at": "2022-08-15T22:13:56Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r946129185",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946129185"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 179,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 180,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946144773",
      "pull_request_review_id": 1072958922,
      "id": 946144773,
      "node_id": "PRRC_kwDOABII5844ZQYF",
      "diff_hunk": "@@ -0,0 +1,349 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD))\n+{\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps we issued an extra getheaders\n+        // message, such as after a block INV was received.\n+        // Or it could be that our peer is broken or malicious. If broken,\n+        // sending a new getheaders immediately could trigger an infinite\n+        // loop. Just give up for now; if our peer ever gives us an block\n+        // INV later we will fetch headers then, and likely retrigger this\n+        // logic.\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (commitment phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    if (!CheckProofOfWork(current.GetHash(), current.nBits, m_consensus_params)) {",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 195,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1178015251732b2db87c0b84dd7ebe3f435a6771",
      "in_reply_to_id": null,
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "We already check the proof of work as part of the basic sanity checks in net_processing  (`ProcessHeadersMessage`), so I don't think the check could ever fail at this later point.",
      "created_at": "2022-08-15T21:29:50Z",
      "updated_at": "2022-08-15T22:38:26Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r946144773",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946144773"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 195,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946146703",
      "pull_request_review_id": 1072958922,
      "id": 946146703,
      "node_id": "PRRC_kwDOABII5844ZQ2P",
      "diff_hunk": "@@ -0,0 +1,349 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD))\n+{\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps we issued an extra getheaders\n+        // message, such as after a block INV was received.\n+        // Or it could be that our peer is broken or malicious. If broken,\n+        // sending a new getheaders immediately could trigger an infinite\n+        // loop. Just give up for now; if our peer ever gives us an block\n+        // INV later we will fetch headers then, and likely retrigger this\n+        // logic.\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 158,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1178015251732b2db87c0b84dd7ebe3f435a6771",
      "in_reply_to_id": null,
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "nit: it only stores a commitment to some headers (depending on `HEADER_COMMITMENT_PERIOD` and `m_commit_offset`), not to each one.",
      "created_at": "2022-08-15T21:32:38Z",
      "updated_at": "2022-08-15T22:38:26Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r946146703",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946146703"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 158,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946947852",
      "pull_request_review_id": 1074358421,
      "id": 946947852,
      "node_id": "PRRC_kwDOABII5844cUcM",
      "diff_hunk": "@@ -0,0 +1,349 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD))\n+{\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps we issued an extra getheaders\n+        // message, such as after a block INV was received.\n+        // Or it could be that our peer is broken or malicious. If broken,\n+        // sending a new getheaders immediately could trigger an infinite\n+        // loop. Just give up for now; if our peer ever gives us an block\n+        // INV later we will fetch headers then, and likely retrigger this\n+        // logic.\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 180,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1178015251732b2db87c0b84dd7ebe3f435a6771",
      "in_reply_to_id": 946129185,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-16T15:41:56Z",
      "updated_at": "2022-08-16T15:41:56Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r946947852",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946947852"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 179,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 180,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946959188",
      "pull_request_review_id": 1074374650,
      "id": 946959188,
      "node_id": "PRRC_kwDOABII5844cXNU",
      "diff_hunk": "@@ -0,0 +1,264 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, we can calculate the work on the chain as we\n+ * go (just by checking the nBits value on each header, and validating the\n+ * proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1, store 1 bit (using a salted hash function) for every N headers\n+ * that we see. With a reasonable choice of N, this uses relatively little\n+ * memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** INITIAL_DOWNLOAD means the peer has not yet demonstrated their\n+         * chain has sufficient work */\n+        INITIAL_DOWNLOAD,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Construct a HeadersSyncState object representing a headers sync via this\n+     *  download-twice mechanism).\n+     *\n+     * id: node id (for logging)\n+     * consensus_params: parameters needed for difficulty adjustment validation\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+            const CBlockIndex* chain_start, const arith_uint256& minimum_required_work);\n+\n+    /** Result data structure for ProcessNextHeaders. */\n+    struct ProcessingResult {\n+        std::vector<CBlockHeader> pow_validated_headers;\n+        bool success{false};\n+        bool request_more{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * received_headers: headers that were received over the network for processing.\n+     *                   Assumes the caller has already verified the headers connect.\n+     * full_headers_message: true if the message was at max capacity,\n+     *                       indicating more headers may be available\n+     * ProcessingResult.pow_validated_headers: will be filled in with any\n+     *                       headers that the caller can fully process and\n+     *                       validate now (because these returned headers are\n+     *                       on a chain with sufficient work)\n+     * ProcessingResult.success: set to false if an error is detected and the sync is\n+     *                       aborted; true otherwise.\n+     * ProcessingResult.request_more: if true, the caller is suggested to call\n+     *                       MakeNextHeadersRequest and send a getheaders message using it.\n+     */\n+    ProcessingResult ProcessNextHeaders(const std::vector<CBlockHeader>&\n+            received_headers, bool full_headers_message);\n+\n+    /** Issue the next GETHEADERS message to our peer.\n+     *\n+     * This will return a locator appropriate for the current sync object, to continue the\n+     * synchronization phase it is in. If tip is provided, the locator will also have entries from\n+     * that tip. This permits the remote party to help us \"skip ahead\" if our global state has\n+     * progressed past where this sync object currently is (because another sync peer was faster,\n+     * perhaps). */\n+    CBlockLocator MakeNextHeadersRequest(const CBlockIndex* tip) const;\n+\n+private:\n+    /** Clear out all download state that might be in progress (freeing any used\n+     * memory), and mark this object as no longer usable.\n+     */\n+    void Finalize();\n+\n+    /**\n+     *  Only called in INITIAL_DOWNLOAD.\n+     *  Validate the work on the headers we received from the network, and\n+     *  store commitments for later. Update overall state with successfully\n+     *  processed headers.\n+     *  On failure, this invokes Finalize() and returns false.\n+     */\n+    bool ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers);\n+\n+    /** In INITIAL_DOWNLOAD, process and update state for a single header */\n+    bool ValidateAndProcessSingleHeader(const CBlockHeader& previous, const\n+            CBlockHeader& current);\n+\n+    /** In REDOWNLOAD, check a header's commitment (if applicable) and add to\n+     * buffer for later processing */\n+    bool ValidateAndStoreRedownloadedHeader(const CBlockHeader& header);\n+\n+    /** Return a set of headers that satisfy our proof-of-work threshold */\n+    std::vector<CBlockHeader> PopHeadersReadyForAcceptance();\n+\n+private:\n+    /** NodeId of the peer (used for log messages) **/\n+    const NodeId m_id;\n+\n+    /** We use the consensus params in our anti-DoS calculations */\n+    const Consensus::Params& m_consensus_params;\n+\n+    /** Store the last block in our block index that the peer's chain builds from */\n+    const CBlockIndex* m_chain_start{nullptr};\n+\n+    /** Minimum work that we're looking for on this chain. */\n+    arith_uint256 m_minimum_required_work;\n+\n+    /** Work that we've seen so far on the peer's chain */\n+    arith_uint256 m_current_chain_work;\n+\n+    /** m_hasher is a salted hasher for making our 1-bit commitments to headers we've seen. */\n+    const SaltedTxidHasher m_hasher;\n+\n+    /** A queue of commitment bits, created during the 1st phase, and verified during the 2nd. */\n+    bitdeque<> m_header_commitments;\n+\n+    /** The (secret) offset on the heights for which to create commitments.\n+     *\n+     * m_header_commitments entries are created at any height h for which\n+     * (h % HEADER_COMMITMENT_PERIOD) == m_commit_offset. */\n+    const unsigned m_commit_offset;\n+\n+    /** m_max_commitments is a bound we calculate on how long an honest peer's chain could be,\n+     * given the MTP rule.\n+     *\n+     * Any peer giving us more headers than this will have its sync aborted. This serves as a\n+     * memory bound on m_header_commitments. */\n+    uint64_t m_max_commitments{0};\n+\n+    /** Store the latest header received while in INITIAL_DOWNLOAD */\n+    CBlockHeader m_last_header_received;\n+\n+    /** Height of m_last_header_received */\n+    int64_t m_current_height{0}; // height of last header received",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 230,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "57d87914639e60b0292748b24944501541cdc88e",
      "in_reply_to_id": 945959497,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed.",
      "created_at": "2022-08-16T15:51:13Z",
      "updated_at": "2022-08-16T15:51:13Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r946959188",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946959188"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 238,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946959525",
      "pull_request_review_id": 1074374955,
      "id": 946959525,
      "node_id": "PRRC_kwDOABII5844cXSl",
      "diff_hunk": "@@ -2458,18 +2624,58 @@ void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n                                             const std::vector<CBlockHeader>& headers,\n                                             bool via_compact_block)\n {\n-    const CNetMsgMaker msgMaker(pfrom.GetCommonVersion());\n     size_t nCount = headers.size();\n \n     if (nCount == 0) {\n         // Nothing interesting. Stop asking this peers for more headers.\n+        // If we were in the middle of headers sync, receiving an empty headers\n+        // message suggests that the peer suddenly has nothing to give us\n+        // (perhaps it reorged to our chain). Clear download state for this peer.\n+        LOCK(peer.m_headers_sync_mutex);\n+        if (peer.m_headers_sync) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }\n+        return;\n+    }\n+\n+    // Before we do any processing, make sure these pass basic sanity checks.\n+    // We'll rely on headers having valid proof-of-work further down, as an\n+    // anti-DoS criteria.\n+    if (!CheckHeadersPoW(headers, m_chainparams.GetConsensus(), peer)) {\n         return;",
      "path": "src/net_processing.cpp",
      "position": 424,
      "original_position": 230,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "57d87914639e60b0292748b24944501541cdc88e",
      "in_reply_to_id": 945975925,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think so -- added.",
      "created_at": "2022-08-16T15:51:24Z",
      "updated_at": "2022-08-16T15:51:25Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r946959525",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946959525"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2758,
      "original_line": 2758,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946960524",
      "pull_request_review_id": 1074375835,
      "id": 946960524,
      "node_id": "PRRC_kwDOABII5844cXiM",
      "diff_hunk": "@@ -0,0 +1,349 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD))\n+{\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps we issued an extra getheaders\n+        // message, such as after a block INV was received.\n+        // Or it could be that our peer is broken or malicious. If broken,\n+        // sending a new getheaders immediately could trigger an infinite\n+        // loop. Just give up for now; if our peer ever gives us an block\n+        // INV later we will fetch headers then, and likely retrigger this\n+        // logic.\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current)\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (commitment phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    if (!CheckProofOfWork(current.GetHash(), current.nBits, m_consensus_params)) {",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 195,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1178015251732b2db87c0b84dd7ebe3f435a6771",
      "in_reply_to_id": 946144773,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I was nervous about omitting such an important check in the module that is designed to handle this, but you're right of course.  I added a comment indicating the caller is required to do this, and removed this call.",
      "created_at": "2022-08-16T15:52:00Z",
      "updated_at": "2022-08-16T15:52:01Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r946960524",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946960524"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 195,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946966750",
      "pull_request_review_id": 1074382754,
      "id": 946966750,
      "node_id": "PRRC_kwDOABII5844cZDe",
      "diff_hunk": "@@ -0,0 +1,349 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD))\n+{\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps we issued an extra getheaders\n+        // message, such as after a block INV was received.\n+        // Or it could be that our peer is broken or malicious. If broken,\n+        // sending a new getheaders immediately could trigger an infinite\n+        // loop. Just give up for now; if our peer ever gives us an block\n+        // INV later we will fetch headers then, and likely retrigger this\n+        // logic.\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and store a commitment to each one.",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 158,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1178015251732b2db87c0b84dd7ebe3f435a6771",
      "in_reply_to_id": 946146703,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed.",
      "created_at": "2022-08-16T15:56:06Z",
      "updated_at": "2022-08-16T15:56:06Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r946966750",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/946966750"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 158,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947113351",
      "pull_request_review_id": 1074585147,
      "id": 947113351,
      "node_id": "PRRC_kwDOABII5844c82H",
      "diff_hunk": "@@ -2421,6 +2443,47 @@ bool PeerManagerImpl::IsContinuationOfLowWorkHeadersSync(Peer& peer, CNode& pfro\n \n         if (peer.m_headers_sync->GetState() == HeadersSyncState::State::FINAL) {\n             peer.m_headers_sync.reset(nullptr);\n+\n+            // Update statistics: delete this peer's entry in allstats, and if this was the",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 89,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "c600a94390559a798f288e662421222c319c3f43",
      "in_reply_to_id": null,
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Comment seems out of date: `allstats` should be `m_headers_presync_stats`, and recomputing is not necessary here, because it's done in the else branch.",
      "created_at": "2022-08-16T18:27:12Z",
      "updated_at": "2022-08-16T20:27:39Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r947113351",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947113351"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 2447,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947136043",
      "pull_request_review_id": 1074622378,
      "id": 947136043,
      "node_id": "PRRC_kwDOABII5844dCYr",
      "diff_hunk": "@@ -851,7 +851,7 @@ class PeerManagerImpl final : public PeerManager\n         EXCLUSIVE_LOCKS_REQUIRED(!m_most_recent_block_mutex, peer.m_getdata_requests_mutex) LOCKS_EXCLUDED(::cs_main);\n \n     /** Process a new block. Perform any post-processing housekeeping */\n-    void ProcessBlock(CNode& node, const std::shared_ptr<const CBlock>& block, bool force_processing);\n+    void ProcessBlock(CNode& node, const std::shared_ptr<const CBlock>& block, bool force_processing, const bool min_pow_checked);",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 5,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "85e4b9e8ed754cc789816680a36e7b98f56efbbb",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "In commit \"Require callers of AcceptBlockHeader() to perform anti-dos checks\"\r\n\r\nNit: having `const` for a by-value function parameter in a declaration is meaningless (it's not part of the function signature, so even if you have `const` on the implementation code, it's not required to be present on the declaration).",
      "created_at": "2022-08-16T18:52:19Z",
      "updated_at": "2022-08-16T19:45:24Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r947136043",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947136043"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 854,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947139086",
      "pull_request_review_id": 1074622378,
      "id": 947139086,
      "node_id": "PRRC_kwDOABII5844dDIO",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD))\n+{\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps we issued an extra getheaders\n+        // message, such as after a block INV was received.\n+        // Or it could be that our peer is broken or malicious. If broken,\n+        // sending a new getheaders immediately could trigger an infinite\n+        // loop. Just give up for now; if our peer ever gives us an block",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 151,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "e778d4f99526de285a6aa3244b6c530305922929",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "In commit \"Utilize anti-DoS headers download strategy\"\r\n\r\nNit: I think this comment is outdated; we don't actually give up in this scenario, but return `false` to the net_processing calling code, which then continues different interpretations with it (possible starting a new sync with it, even).",
      "created_at": "2022-08-16T18:54:38Z",
      "updated_at": "2022-08-16T19:45:24Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r947139086",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947139086"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 151,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947152166",
      "pull_request_review_id": 1074585147,
      "id": 947152166,
      "node_id": "PRRC_kwDOABII5844dGUm",
      "diff_hunk": "@@ -2458,18 +2617,60 @@ void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n                                             const std::vector<CBlockHeader>& headers,\n                                             bool via_compact_block)\n {\n-    const CNetMsgMaker msgMaker(pfrom.GetCommonVersion());\n     size_t nCount = headers.size();\n \n     if (nCount == 0) {\n         // Nothing interesting. Stop asking this peers for more headers.\n+        // If we were in the middle of headers sync, receiving an empty headers\n+        // message suggests that the peer suddenly has nothing to give us\n+        // (perhaps it reorged to our chain). Clear download state for this peer.\n+        LOCK(peer.m_headers_sync_mutex);\n+        if (peer.m_headers_sync) {\n+            peer.m_headers_sync.reset(nullptr);",
      "path": "src/net_processing.cpp",
      "position": 408,
      "original_position": 214,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "e778d4f99526de285a6aa3244b6c530305922929",
      "in_reply_to_id": null,
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think we should remove a possible `m_headers_presync_stats` entry here too.",
      "created_at": "2022-08-16T19:05:24Z",
      "updated_at": "2022-08-16T20:27:39Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r947152166",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947152166"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2742,
      "original_line": 2742,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947174761",
      "pull_request_review_id": 1074622378,
      "id": 947174761,
      "node_id": "PRRC_kwDOABII5844dL1p",
      "diff_hunk": "@@ -857,11 +857,15 @@ class ChainstateManager\n     /**\n      * If a block header hasn't already been seen, call CheckBlockHeader on it, ensure\n      * that it doesn't descend from an invalid block, and then add it to m_block_index.\n+     * Caller must set min_pow_checked=true in order to add a new header to the\n+     * block index (permanent memory storage), indicating that the header is\n+     * known to be part of a sufficiently high-work chain (anti-dos check).\n      */\n     bool AcceptBlockHeader(\n         const CBlockHeader& block,\n         BlockValidationState& state,\n-        CBlockIndex** ppindex) EXCLUSIVE_LOCKS_REQUIRED(cs_main);\n+        CBlockIndex** ppindex,\n+        const bool min_pow_checked) EXCLUSIVE_LOCKS_REQUIRED(cs_main);",
      "path": "src/validation.h",
      "position": null,
      "original_position": 22,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "85e4b9e8ed754cc789816680a36e7b98f56efbbb",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "In commit \"Require callers of AcceptBlockHeader() to perform anti-dos checks\"\r\n\r\nHere too, the `const` is meaningless.",
      "created_at": "2022-08-16T19:25:15Z",
      "updated_at": "2022-08-16T19:45:24Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r947174761",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947174761"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 868,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947186892",
      "pull_request_review_id": 1074622378,
      "id": 947186892,
      "node_id": "PRRC_kwDOABII5844dOzM",
      "diff_hunk": "@@ -0,0 +1,143 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <chain.h>\n+#include <chainparams.h>\n+#include <consensus/params.h>\n+#include <headerssync.h>\n+#include <pow.h>\n+#include <test/util/setup_common.h>\n+#include <validation.h>\n+#include <vector>\n+\n+#include <boost/test/unit_test.hpp>\n+\n+struct HeadersGeneratorSetup : public RegTestingSetup {\n+    /** Search for a nonce to meet (regtest) proof of work */\n+    void FindProofOfWork(CBlockHeader& starting_header);\n+    /**\n+     * Generate headers in a chain that build off a given starting hash, using\n+     * the given nVersion, advancing time by 1 second from the starting\n+     * prev_time, and with a fixed merkle root hash.\n+     */\n+    void GenerateHeaders(std::vector<CBlockHeader>& headers, size_t count,\n+            const uint256& starting_hash, const int nVersion, int prev_time,\n+            const uint256& merkle_root, const uint32_t nBits);\n+};\n+\n+void HeadersGeneratorSetup::FindProofOfWork(CBlockHeader& starting_header)\n+{\n+    while (!CheckProofOfWork(starting_header.GetHash(), starting_header.nBits, Params().GetConsensus())) {\n+        ++(starting_header.nNonce);\n+    }\n+}\n+\n+void HeadersGeneratorSetup::GenerateHeaders(std::vector<CBlockHeader>& headers,\n+        size_t count, const uint256& starting_hash, const int nVersion, int prev_time,\n+        const uint256& merkle_root, const uint32_t nBits)\n+{\n+    uint256 prev_hash = starting_hash;\n+\n+    while (headers.size() < count) {\n+        headers.push_back(CBlockHeader());\n+        CBlockHeader& next_header = headers.back();;\n+        next_header.nVersion = nVersion;\n+        next_header.hashPrevBlock = prev_hash;\n+        next_header.hashMerkleRoot = merkle_root;\n+        next_header.nTime = prev_time+1;\n+        next_header.nBits = nBits;\n+\n+        FindProofOfWork(next_header);\n+        prev_hash = next_header.GetHash();\n+        prev_time = next_header.nTime;\n+    }\n+    return;\n+}\n+\n+BOOST_FIXTURE_TEST_SUITE(headers_sync_chainwork_tests, HeadersGeneratorSetup)\n+\n+// In this test, we construct two sets of headers from genesis, one with\n+// sufficient proof of work and one without.\n+// 1. We deliver the first set of headers and verify that the headers sync state\n+//    updates to the REDOWNLOAD phase successfully.\n+// 2. Then we deliver the second set of headers and verify that they fail\n+//    processing (presumably due to commitments not matching).\n+// 3. Finally, we verify that repeating with the first set of headers in both\n+//    phases is successful.\n+BOOST_AUTO_TEST_CASE(headers_sync_state)\n+{\n+    std::vector<CBlockHeader> first_chain;\n+    std::vector<CBlockHeader> second_chain;\n+\n+    std::unique_ptr<HeadersSyncState> hss;\n+\n+    const int target_blocks = 15000;\n+    arith_uint256 chain_work = target_blocks*2;\n+\n+    // Generate headers for two different chains (using differing merkle roots\n+    // to ensure the headers are different).\n+    GenerateHeaders(first_chain, target_blocks-1, Params().GenesisBlock().GetHash(),\n+            Params().GenesisBlock().nVersion, Params().GenesisBlock().nTime,\n+            ArithToUint256(0), Params().GenesisBlock().nBits);\n+\n+    GenerateHeaders(second_chain, target_blocks-2, Params().GenesisBlock().GetHash(),\n+            Params().GenesisBlock().nVersion, Params().GenesisBlock().nTime,\n+            ArithToUint256(1), Params().GenesisBlock().nBits);\n+\n+    const CBlockIndex* chain_start = WITH_LOCK(::cs_main, return m_node.chainman->m_blockman.LookupBlockIndex(Params().GenesisBlock().GetHash()));\n+    std::vector<CBlockHeader> headers_batch;\n+\n+    // Feed the first chain to HeadersSyncState, by delivering 1 header\n+    // initially and then the rest.\n+    headers_batch.insert(headers_batch.end(), std::next(first_chain.begin(), 1), first_chain.end());",
      "path": "src/test/headers_sync_chainwork_tests.cpp",
      "position": null,
      "original_position": 93,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f05a5641c26eac3a14cb7c99c2be4eb47568d292",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "In commit \"Add unit test for HeadersSyncState\":\r\n\r\nNit: this can be `std::next(first_chain.begin())` or even just `first_chain.begin() + 1`.",
      "created_at": "2022-08-16T19:41:26Z",
      "updated_at": "2022-08-16T19:45:24Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r947186892",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947186892"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 93,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947308983",
      "pull_request_review_id": 1074854751,
      "id": 947308983,
      "node_id": "PRRC_kwDOABII5844dsm3",
      "diff_hunk": "@@ -851,7 +851,7 @@ class PeerManagerImpl final : public PeerManager\n         EXCLUSIVE_LOCKS_REQUIRED(!m_most_recent_block_mutex, peer.m_getdata_requests_mutex) LOCKS_EXCLUDED(::cs_main);\n \n     /** Process a new block. Perform any post-processing housekeeping */\n-    void ProcessBlock(CNode& node, const std::shared_ptr<const CBlock>& block, bool force_processing);\n+    void ProcessBlock(CNode& node, const std::shared_ptr<const CBlock>& block, bool force_processing, const bool min_pow_checked);",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 5,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "85e4b9e8ed754cc789816680a36e7b98f56efbbb",
      "in_reply_to_id": 947136043,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Removed.",
      "created_at": "2022-08-16T22:48:09Z",
      "updated_at": "2022-08-16T22:48:10Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r947308983",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947308983"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 854,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947309078",
      "pull_request_review_id": 1074854866,
      "id": 947309078,
      "node_id": "PRRC_kwDOABII5844dsoW",
      "diff_hunk": "@@ -0,0 +1,318 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD))\n+{\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps we issued an extra getheaders\n+        // message, such as after a block INV was received.\n+        // Or it could be that our peer is broken or malicious. If broken,\n+        // sending a new getheaders immediately could trigger an infinite\n+        // loop. Just give up for now; if our peer ever gives us an block",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 151,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "e778d4f99526de285a6aa3244b6c530305922929",
      "in_reply_to_id": 947139086,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed",
      "created_at": "2022-08-16T22:48:21Z",
      "updated_at": "2022-08-16T22:48:21Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r947309078",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947309078"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 151,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947309121",
      "pull_request_review_id": 1074854920,
      "id": 947309121,
      "node_id": "PRRC_kwDOABII5844dspB",
      "diff_hunk": "@@ -857,11 +857,15 @@ class ChainstateManager\n     /**\n      * If a block header hasn't already been seen, call CheckBlockHeader on it, ensure\n      * that it doesn't descend from an invalid block, and then add it to m_block_index.\n+     * Caller must set min_pow_checked=true in order to add a new header to the\n+     * block index (permanent memory storage), indicating that the header is\n+     * known to be part of a sufficiently high-work chain (anti-dos check).\n      */\n     bool AcceptBlockHeader(\n         const CBlockHeader& block,\n         BlockValidationState& state,\n-        CBlockIndex** ppindex) EXCLUSIVE_LOCKS_REQUIRED(cs_main);\n+        CBlockIndex** ppindex,\n+        const bool min_pow_checked) EXCLUSIVE_LOCKS_REQUIRED(cs_main);",
      "path": "src/validation.h",
      "position": null,
      "original_position": 22,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "85e4b9e8ed754cc789816680a36e7b98f56efbbb",
      "in_reply_to_id": 947174761,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Removed.",
      "created_at": "2022-08-16T22:48:26Z",
      "updated_at": "2022-08-16T22:48:26Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r947309121",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947309121"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 868,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947309180",
      "pull_request_review_id": 1074855010,
      "id": 947309180,
      "node_id": "PRRC_kwDOABII5844dsp8",
      "diff_hunk": "@@ -0,0 +1,143 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <chain.h>\n+#include <chainparams.h>\n+#include <consensus/params.h>\n+#include <headerssync.h>\n+#include <pow.h>\n+#include <test/util/setup_common.h>\n+#include <validation.h>\n+#include <vector>\n+\n+#include <boost/test/unit_test.hpp>\n+\n+struct HeadersGeneratorSetup : public RegTestingSetup {\n+    /** Search for a nonce to meet (regtest) proof of work */\n+    void FindProofOfWork(CBlockHeader& starting_header);\n+    /**\n+     * Generate headers in a chain that build off a given starting hash, using\n+     * the given nVersion, advancing time by 1 second from the starting\n+     * prev_time, and with a fixed merkle root hash.\n+     */\n+    void GenerateHeaders(std::vector<CBlockHeader>& headers, size_t count,\n+            const uint256& starting_hash, const int nVersion, int prev_time,\n+            const uint256& merkle_root, const uint32_t nBits);\n+};\n+\n+void HeadersGeneratorSetup::FindProofOfWork(CBlockHeader& starting_header)\n+{\n+    while (!CheckProofOfWork(starting_header.GetHash(), starting_header.nBits, Params().GetConsensus())) {\n+        ++(starting_header.nNonce);\n+    }\n+}\n+\n+void HeadersGeneratorSetup::GenerateHeaders(std::vector<CBlockHeader>& headers,\n+        size_t count, const uint256& starting_hash, const int nVersion, int prev_time,\n+        const uint256& merkle_root, const uint32_t nBits)\n+{\n+    uint256 prev_hash = starting_hash;\n+\n+    while (headers.size() < count) {\n+        headers.push_back(CBlockHeader());\n+        CBlockHeader& next_header = headers.back();;\n+        next_header.nVersion = nVersion;\n+        next_header.hashPrevBlock = prev_hash;\n+        next_header.hashMerkleRoot = merkle_root;\n+        next_header.nTime = prev_time+1;\n+        next_header.nBits = nBits;\n+\n+        FindProofOfWork(next_header);\n+        prev_hash = next_header.GetHash();\n+        prev_time = next_header.nTime;\n+    }\n+    return;\n+}\n+\n+BOOST_FIXTURE_TEST_SUITE(headers_sync_chainwork_tests, HeadersGeneratorSetup)\n+\n+// In this test, we construct two sets of headers from genesis, one with\n+// sufficient proof of work and one without.\n+// 1. We deliver the first set of headers and verify that the headers sync state\n+//    updates to the REDOWNLOAD phase successfully.\n+// 2. Then we deliver the second set of headers and verify that they fail\n+//    processing (presumably due to commitments not matching).\n+// 3. Finally, we verify that repeating with the first set of headers in both\n+//    phases is successful.\n+BOOST_AUTO_TEST_CASE(headers_sync_state)\n+{\n+    std::vector<CBlockHeader> first_chain;\n+    std::vector<CBlockHeader> second_chain;\n+\n+    std::unique_ptr<HeadersSyncState> hss;\n+\n+    const int target_blocks = 15000;\n+    arith_uint256 chain_work = target_blocks*2;\n+\n+    // Generate headers for two different chains (using differing merkle roots\n+    // to ensure the headers are different).\n+    GenerateHeaders(first_chain, target_blocks-1, Params().GenesisBlock().GetHash(),\n+            Params().GenesisBlock().nVersion, Params().GenesisBlock().nTime,\n+            ArithToUint256(0), Params().GenesisBlock().nBits);\n+\n+    GenerateHeaders(second_chain, target_blocks-2, Params().GenesisBlock().GetHash(),\n+            Params().GenesisBlock().nVersion, Params().GenesisBlock().nTime,\n+            ArithToUint256(1), Params().GenesisBlock().nBits);\n+\n+    const CBlockIndex* chain_start = WITH_LOCK(::cs_main, return m_node.chainman->m_blockman.LookupBlockIndex(Params().GenesisBlock().GetHash()));\n+    std::vector<CBlockHeader> headers_batch;\n+\n+    // Feed the first chain to HeadersSyncState, by delivering 1 header\n+    // initially and then the rest.\n+    headers_batch.insert(headers_batch.end(), std::next(first_chain.begin(), 1), first_chain.end());",
      "path": "src/test/headers_sync_chainwork_tests.cpp",
      "position": null,
      "original_position": 93,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f05a5641c26eac3a14cb7c99c2be4eb47568d292",
      "in_reply_to_id": 947186892,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed.",
      "created_at": "2022-08-16T22:48:36Z",
      "updated_at": "2022-08-16T22:48:37Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r947309180",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947309180"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 93,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947309241",
      "pull_request_review_id": 1074855097,
      "id": 947309241,
      "node_id": "PRRC_kwDOABII5844dsq5",
      "diff_hunk": "@@ -2421,6 +2443,47 @@ bool PeerManagerImpl::IsContinuationOfLowWorkHeadersSync(Peer& peer, CNode& pfro\n \n         if (peer.m_headers_sync->GetState() == HeadersSyncState::State::FINAL) {\n             peer.m_headers_sync.reset(nullptr);\n+\n+            // Update statistics: delete this peer's entry in allstats, and if this was the",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 89,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "c600a94390559a798f288e662421222c319c3f43",
      "in_reply_to_id": 947113351,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Updated comment.",
      "created_at": "2022-08-16T22:48:44Z",
      "updated_at": "2022-08-16T22:48:45Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r947309241",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947309241"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 2447,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947309284",
      "pull_request_review_id": 1074855156,
      "id": 947309284,
      "node_id": "PRRC_kwDOABII5844dsrk",
      "diff_hunk": "@@ -2458,18 +2617,60 @@ void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n                                             const std::vector<CBlockHeader>& headers,\n                                             bool via_compact_block)\n {\n-    const CNetMsgMaker msgMaker(pfrom.GetCommonVersion());\n     size_t nCount = headers.size();\n \n     if (nCount == 0) {\n         // Nothing interesting. Stop asking this peers for more headers.\n+        // If we were in the middle of headers sync, receiving an empty headers\n+        // message suggests that the peer suddenly has nothing to give us\n+        // (perhaps it reorged to our chain). Clear download state for this peer.\n+        LOCK(peer.m_headers_sync_mutex);\n+        if (peer.m_headers_sync) {\n+            peer.m_headers_sync.reset(nullptr);",
      "path": "src/net_processing.cpp",
      "position": 408,
      "original_position": 214,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "e778d4f99526de285a6aa3244b6c530305922929",
      "in_reply_to_id": 947152166,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed.",
      "created_at": "2022-08-16T22:48:50Z",
      "updated_at": "2022-08-16T22:48:50Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r947309284",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947309284"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2742,
      "original_line": 2742,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947957867",
      "pull_request_review_id": 1075764216,
      "id": 947957867,
      "node_id": "PRRC_kwDOABII5844gLBr",
      "diff_hunk": "@@ -0,0 +1,349 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD))\n+{\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::INITIAL_DOWNLOAD) {\n+        // During INITIAL_DOWNLOAD, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in INITIAL_DOWNLOAD and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (commitment phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {",
      "path": "src/headerssync.cpp",
      "position": 105,
      "original_position": 102,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6862e74c70a343d598e7e9fdf82b41bbb748cf6e",
      "in_reply_to_id": 945622956,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "This should be moot now that mixing in locator entries from `m_best_header` has been removed.",
      "created_at": "2022-08-17T13:48:02Z",
      "updated_at": "2022-08-17T13:48:02Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r947957867",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947957867"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 105,
      "original_line": 105,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947959284",
      "pull_request_review_id": 1075766427,
      "id": 947959284,
      "node_id": "PRRC_kwDOABII5844gLX0",
      "diff_hunk": "@@ -2310,6 +2380,100 @@ bool PeerManagerImpl::CheckHeadersAreContinuous(const std::vector<CBlockHeader>&\n     return true;\n }\n \n+bool PeerManagerImpl::IsContinuationOfLowWorkHeadersSync(Peer& peer, CNode& pfrom, const std::vector<CBlockHeader>& headers, std::vector<CBlockHeader>& previously_downloaded_headers)\n+{\n+    if (peer.m_headers_sync) {\n+        auto result = peer.m_headers_sync->ProcessNextHeaders(headers, headers.size() == MAX_HEADERS_RESULTS);\n+        if (result.request_more) {\n+            // Use the parent of the best known header as tip to mix into the locator to send out.\n+            // This matches the behavior of the initial getheaders locator sent, for the same\n+            // reason: making sure we always get information back we can use to prime headers\n+            // synchronization structures with.\n+            const CBlockIndex* headers_tip = WITH_LOCK(::cs_main, return m_chainman.m_best_header);\n+            if (headers_tip && headers_tip->pprev) headers_tip = headers_tip->pprev;\n+            auto locator = peer.m_headers_sync->MakeNextHeadersRequest(headers_tip);\n+            // If we were instructed to ask for a locator, it should not be empty.\n+            Assume(!locator.vHave.empty());\n+            if (!locator.vHave.empty()) {\n+                // It should be impossible for the getheaders request to fail,\n+                // because we should have cleared the last getheaders timestamp\n+                // when processing the headers that triggered this call. But\n+                // it may be possible to bypass this via compactblock\n+                // processing, so check the result before logging just to be\n+                // safe.\n+                bool sent_getheaders = MaybeSendGetHeaders(pfrom, locator, peer);\n+                if (sent_getheaders) {\n+                    LogPrint(BCLog::NET, \"more getheaders (from %s) to peer=%d\\n\",\n+                            locator.vHave.front().ToString(), pfrom.GetId());\n+                }\n+            }\n+        }\n+\n+        if (peer.m_headers_sync->GetState() == HeadersSyncState::State::FINAL) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 139,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6862e74c70a343d598e7e9fdf82b41bbb748cf6e",
      "in_reply_to_id": 945610729,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I assume this is also moot, now that mixing in locators from `m_best_header` has been removed.",
      "created_at": "2022-08-17T13:49:06Z",
      "updated_at": "2022-08-17T13:49:06Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r947959284",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/947959284"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 2412,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 2515,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/948122467",
      "pull_request_review_id": 1075991624,
      "id": 948122467,
      "node_id": "PRRC_kwDOABII5844gzNj",
      "diff_hunk": "@@ -0,0 +1,314 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD))\n+{\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps our peer reorged away from the chain\n+        // they were on. Give up on this sync for now (likely we will start a\n+        // new sync with a new starting point).\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (presync phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and occasionally store\n+    // commitments.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current)\n+{\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (presync phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    if (next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        // Add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // It's possible the chain grew since we started the sync; so\n+            // potentially we could succeed in syncing the peer's chain if we\n+            // try again later.\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: exceeded max commitments at height=%i (presync phase)\\n\", m_id, next_height);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));\n+    m_last_header_received = current;\n+    m_current_height = next_height;\n+\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreRedownloadedHeader(const CBlockHeader& header)\n+{\n+    Assume(m_download_state == State::REDOWNLOAD);\n+    if (m_download_state != State::REDOWNLOAD) return false;\n+\n+    int64_t next_height = m_redownload_buffer_last_height + 1;\n+\n+    // Ensure that we're working on a header that connects to the chain we're\n+    // downloading.\n+    if (header.hashPrevBlock != m_redownload_buffer_last_hash) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (redownload phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    // Check that the difficulty adjustments are within our tolerance:\n+    uint32_t previous_nBits{0};\n+    if (!m_redownloaded_headers.empty()) {\n+        previous_nBits = m_redownloaded_headers.back().nBits;\n+    } else {\n+        previous_nBits = m_chain_start->nBits;\n+    }\n+\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous_nBits, header.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (redownload phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    // Track work on the redownloaded chain\n+    m_redownload_chain_work += GetBlockProof(CBlockIndex(header));\n+\n+    if (m_redownload_chain_work >= m_minimum_required_work) {\n+        m_process_all_remaining_headers = true;\n+    }\n+\n+    // If we're at a header for which we previously stored a commitment, verify\n+    // it is correct. Failure will result in aborting download.\n+    // Also, don't check commitments once we've gotten to our target blockhash;\n+    // it's possible our peer has extended its chain between our first sync and\n+    // our second, and we don't want to return failure after we've seen our\n+    // target blockhash just because we ran out of commitments.\n+    if (!m_process_all_remaining_headers && next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+         bool commitment = m_hasher(header.GetHash()) & 1;\n+         if (m_header_commitments.size() == 0) {\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: commitment overrun at height=%i (redownload phase)\\n\", m_id, next_height);\n+            // Somehow our peer managed to feed us a different chain and\n+            // we've run out of commitments.\n+            return false;\n+        }\n+        bool expected_commitment = m_header_commitments.front();\n+        m_header_commitments.pop_front();\n+        if (commitment != expected_commitment) {\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: commitment mismatch at height=%i (redownload phase)\\n\", m_id, next_height);\n+            return false;\n+        }\n+    }\n+\n+    // Store this header for later processing.\n+    m_redownloaded_headers.push_back(header);\n+    m_redownload_buffer_last_height = next_height;\n+    m_redownload_buffer_last_hash = header.GetHash();\n+\n+    return true;\n+}\n+\n+std::vector<CBlockHeader> HeadersSyncState::PopHeadersReadyForAcceptance()\n+{\n+    std::vector<CBlockHeader> ret;\n+\n+    Assume(m_download_state == State::REDOWNLOAD);\n+    if (m_download_state != State::REDOWNLOAD) return ret;\n+\n+    while (m_redownloaded_headers.size() > REDOWNLOAD_BUFFER_SIZE ||\n+            (m_redownloaded_headers.size() > 0 && m_process_all_remaining_headers)) {\n+        ret.emplace_back(m_redownloaded_headers.front().GetFullHeader(m_redownload_buffer_first_prev_hash));\n+        m_redownloaded_headers.pop_front();\n+        m_redownload_buffer_first_prev_hash = ret.back().GetHash();\n+    }\n+    return ret;\n+}\n+\n+CBlockLocator HeadersSyncState::MakeNextHeadersRequest() const\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return {};\n+\n+    auto chain_start_locator = LocatorEntries(m_chain_start);\n+    std::vector<uint256> locator;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During pre-synchronization, we continue from the last header received.\n+        locator.push_back(m_last_header_received.GetHash());\n+    }\n+\n+    if (m_download_state == State::REDOWNLOAD) {\n+        // During redownload, we will download from the last received header that we stored.\n+        locator.push_back(m_redownload_buffer_last_hash);\n+    }\n+\n+    locator.insert(locator.end(), chain_start_locator.begin(), chain_start_locator.end());",
      "path": "src/headerssync.cpp",
      "position": 314,
      "original_position": 311,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "07b068e1568efdb6473f1d1eb4b870eb339c748b",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "07b068e1568efdb6473f1d1eb4b870eb339c748b: do we need the `chain_start_locator` at all? We know the peer has the hash in `locator`, because they sent it to us.",
      "created_at": "2022-08-17T15:38:06Z",
      "updated_at": "2022-08-17T15:38:07Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r948122467",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/948122467"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 314,
      "original_line": 314,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/948126152",
      "pull_request_review_id": 1075996536,
      "id": 948126152,
      "node_id": "PRRC_kwDOABII5844g0HI",
      "diff_hunk": "@@ -0,0 +1,314 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD))\n+{\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps our peer reorged away from the chain\n+        // they were on. Give up on this sync for now (likely we will start a\n+        // new sync with a new starting point).\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (presync phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and occasionally store\n+    // commitments.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current)\n+{\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (presync phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    if (next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        // Add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // It's possible the chain grew since we started the sync; so\n+            // potentially we could succeed in syncing the peer's chain if we\n+            // try again later.\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: exceeded max commitments at height=%i (presync phase)\\n\", m_id, next_height);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));\n+    m_last_header_received = current;\n+    m_current_height = next_height;\n+\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreRedownloadedHeader(const CBlockHeader& header)\n+{\n+    Assume(m_download_state == State::REDOWNLOAD);\n+    if (m_download_state != State::REDOWNLOAD) return false;\n+\n+    int64_t next_height = m_redownload_buffer_last_height + 1;\n+\n+    // Ensure that we're working on a header that connects to the chain we're\n+    // downloading.\n+    if (header.hashPrevBlock != m_redownload_buffer_last_hash) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (redownload phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    // Check that the difficulty adjustments are within our tolerance:\n+    uint32_t previous_nBits{0};\n+    if (!m_redownloaded_headers.empty()) {\n+        previous_nBits = m_redownloaded_headers.back().nBits;\n+    } else {\n+        previous_nBits = m_chain_start->nBits;\n+    }\n+\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous_nBits, header.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (redownload phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    // Track work on the redownloaded chain\n+    m_redownload_chain_work += GetBlockProof(CBlockIndex(header));\n+\n+    if (m_redownload_chain_work >= m_minimum_required_work) {\n+        m_process_all_remaining_headers = true;\n+    }\n+\n+    // If we're at a header for which we previously stored a commitment, verify\n+    // it is correct. Failure will result in aborting download.\n+    // Also, don't check commitments once we've gotten to our target blockhash;\n+    // it's possible our peer has extended its chain between our first sync and\n+    // our second, and we don't want to return failure after we've seen our\n+    // target blockhash just because we ran out of commitments.\n+    if (!m_process_all_remaining_headers && next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+         bool commitment = m_hasher(header.GetHash()) & 1;\n+         if (m_header_commitments.size() == 0) {\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: commitment overrun at height=%i (redownload phase)\\n\", m_id, next_height);\n+            // Somehow our peer managed to feed us a different chain and\n+            // we've run out of commitments.\n+            return false;\n+        }\n+        bool expected_commitment = m_header_commitments.front();\n+        m_header_commitments.pop_front();\n+        if (commitment != expected_commitment) {\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: commitment mismatch at height=%i (redownload phase)\\n\", m_id, next_height);\n+            return false;\n+        }\n+    }\n+\n+    // Store this header for later processing.\n+    m_redownloaded_headers.push_back(header);\n+    m_redownload_buffer_last_height = next_height;\n+    m_redownload_buffer_last_hash = header.GetHash();\n+\n+    return true;\n+}\n+\n+std::vector<CBlockHeader> HeadersSyncState::PopHeadersReadyForAcceptance()\n+{\n+    std::vector<CBlockHeader> ret;\n+\n+    Assume(m_download_state == State::REDOWNLOAD);\n+    if (m_download_state != State::REDOWNLOAD) return ret;\n+\n+    while (m_redownloaded_headers.size() > REDOWNLOAD_BUFFER_SIZE ||\n+            (m_redownloaded_headers.size() > 0 && m_process_all_remaining_headers)) {\n+        ret.emplace_back(m_redownloaded_headers.front().GetFullHeader(m_redownload_buffer_first_prev_hash));\n+        m_redownloaded_headers.pop_front();\n+        m_redownload_buffer_first_prev_hash = ret.back().GetHash();\n+    }\n+    return ret;\n+}\n+\n+CBlockLocator HeadersSyncState::MakeNextHeadersRequest() const\n+{\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return {};\n+\n+    auto chain_start_locator = LocatorEntries(m_chain_start);\n+    std::vector<uint256> locator;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During pre-synchronization, we continue from the last header received.\n+        locator.push_back(m_last_header_received.GetHash());\n+    }\n+\n+    if (m_download_state == State::REDOWNLOAD) {\n+        // During redownload, we will download from the last received header that we stored.\n+        locator.push_back(m_redownload_buffer_last_hash);\n+    }\n+\n+    locator.insert(locator.end(), chain_start_locator.begin(), chain_start_locator.end());",
      "path": "src/headerssync.cpp",
      "position": 314,
      "original_position": 311,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "07b068e1568efdb6473f1d1eb4b870eb339c748b",
      "in_reply_to_id": 948122467,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "We include it to better handle the case where our peer may reorg during the sync.  (This way, they won't go all the way back to genesis just because we omitted locator entries.)",
      "created_at": "2022-08-17T15:41:15Z",
      "updated_at": "2022-08-17T15:41:15Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r948126152",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/948126152"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 314,
      "original_line": 314,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951191470",
      "pull_request_review_id": 1080118855,
      "id": 951191470,
      "node_id": "PRRC_kwDOABII5844sgeu",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential",
      "path": "src/headerssync.h",
      "position": 96,
      "original_position": 96,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "07b068e1568efdb6473f1d1eb4b870eb339c748b",
      "in_reply_to_id": null,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "nit in 956109788e\r\n\r\n```suggestion\r\n* parameterization, we can achieve a given security target for potential\r\n```",
      "created_at": "2022-08-22T09:04:22Z",
      "updated_at": "2022-08-23T16:48:03Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r951191470",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951191470"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 96,
      "original_line": 96,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951250735",
      "pull_request_review_id": 1080118855,
      "id": 951250735,
      "node_id": "PRRC_kwDOABII5844su8v",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** PRESYNC means the peer has not yet demonstrated their chain has\n+         * sufficient work and we're only building commitments to the chain they\n+         * serve us. */\n+        PRESYNC,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Construct a HeadersSyncState object representing a headers sync via this\n+     *  download-twice mechanism).\n+     *\n+     * id: node id (for logging)\n+     * consensus_params: parameters needed for difficulty adjustment validation\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+            const CBlockIndex* chain_start, const arith_uint256& minimum_required_work);\n+\n+    /** Result data structure for ProcessNextHeaders. */\n+    struct ProcessingResult {\n+        std::vector<CBlockHeader> pow_validated_headers;\n+        bool success{false};\n+        bool request_more{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * received_headers: headers that were received over the network for processing.\n+     *                   Assumes the caller has already verified the headers connect,\n+     *                   and has checked that each header satisfies the\n+     *                   proof-of-work target included in the header (but not\n+     *                   necessarily verified that the proof-of-work target is\n+     *                   correct and passes consensus rules).\n+     * full_headers_message: true if the message was at max capacity,\n+     *                       indicating more headers may be available\n+     * ProcessingResult.pow_validated_headers: will be filled in with any\n+     *                       headers that the caller can fully process and\n+     *                       validate now (because these returned headers are\n+     *                       on a chain with sufficient work)\n+     * ProcessingResult.success: set to false if an error is detected and the sync is\n+     *                       aborted; true otherwise.\n+     * ProcessingResult.request_more: if true, the caller is suggested to call\n+     *                       MakeNextHeadersRequest and send a getheaders message using it.\n+     */\n+    ProcessingResult ProcessNextHeaders(const std::vector<CBlockHeader>&\n+            received_headers, bool full_headers_message);\n+\n+    /** Issue the next GETHEADERS message to our peer.\n+     *\n+     * This will return a locator appropriate for the current sync object, to continue the\n+     * synchronization phase it is in.\n+     */\n+    CBlockLocator MakeNextHeadersRequest() const;\n+\n+private:\n+    /** Clear out all download state that might be in progress (freeing any used\n+     * memory), and mark this object as no longer usable.\n+     */\n+    void Finalize();\n+\n+    /**\n+     *  Only called in PRESYNC.\n+     *  Validate the work on the headers we received from the network, and\n+     *  store commitments for later. Update overall state with successfully\n+     *  processed headers.\n+     *  On failure, this invokes Finalize() and returns false.\n+     */\n+    bool ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers);\n+\n+    /** In PRESYNC, process and update state for a single header */\n+    bool ValidateAndProcessSingleHeader(const CBlockHeader& previous, const\n+            CBlockHeader& current);\n+\n+    /** In REDOWNLOAD, check a header's commitment (if applicable) and add to\n+     * buffer for later processing */\n+    bool ValidateAndStoreRedownloadedHeader(const CBlockHeader& header);\n+\n+    /** Return a set of headers that satisfy our proof-of-work threshold */\n+    std::vector<CBlockHeader> PopHeadersReadyForAcceptance();\n+\n+private:",
      "path": "src/headerssync.h",
      "position": 203,
      "original_position": 194,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "07b068e1568efdb6473f1d1eb4b870eb339c748b",
      "in_reply_to_id": null,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "nit in 956109788e:\r\nNot needed",
      "created_at": "2022-08-22T10:03:40Z",
      "updated_at": "2022-08-23T16:48:03Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r951250735",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951250735"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 203,
      "original_line": 203,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951528580",
      "pull_request_review_id": 1080118855,
      "id": 951528580,
      "node_id": "PRRC_kwDOABII5844tyyE",
      "diff_hunk": "@@ -0,0 +1,143 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <chain.h>\n+#include <chainparams.h>\n+#include <consensus/params.h>\n+#include <headerssync.h>\n+#include <pow.h>\n+#include <test/util/setup_common.h>\n+#include <validation.h>\n+#include <vector>\n+\n+#include <boost/test/unit_test.hpp>\n+\n+struct HeadersGeneratorSetup : public RegTestingSetup {\n+    /** Search for a nonce to meet (regtest) proof of work */\n+    void FindProofOfWork(CBlockHeader& starting_header);\n+    /**\n+     * Generate headers in a chain that build off a given starting hash, using\n+     * the given nVersion, advancing time by 1 second from the starting\n+     * prev_time, and with a fixed merkle root hash.\n+     */\n+    void GenerateHeaders(std::vector<CBlockHeader>& headers, size_t count,\n+            const uint256& starting_hash, const int nVersion, int prev_time,\n+            const uint256& merkle_root, const uint32_t nBits);\n+};\n+\n+void HeadersGeneratorSetup::FindProofOfWork(CBlockHeader& starting_header)\n+{\n+    while (!CheckProofOfWork(starting_header.GetHash(), starting_header.nBits, Params().GetConsensus())) {\n+        ++(starting_header.nNonce);\n+    }\n+}\n+\n+void HeadersGeneratorSetup::GenerateHeaders(std::vector<CBlockHeader>& headers,\n+        size_t count, const uint256& starting_hash, const int nVersion, int prev_time,\n+        const uint256& merkle_root, const uint32_t nBits)\n+{\n+    uint256 prev_hash = starting_hash;\n+\n+    while (headers.size() < count) {\n+        headers.push_back(CBlockHeader());\n+        CBlockHeader& next_header = headers.back();;\n+        next_header.nVersion = nVersion;\n+        next_header.hashPrevBlock = prev_hash;\n+        next_header.hashMerkleRoot = merkle_root;\n+        next_header.nTime = prev_time+1;\n+        next_header.nBits = nBits;\n+\n+        FindProofOfWork(next_header);\n+        prev_hash = next_header.GetHash();\n+        prev_time = next_header.nTime;\n+    }\n+    return;\n+}\n+\n+BOOST_FIXTURE_TEST_SUITE(headers_sync_chainwork_tests, HeadersGeneratorSetup)\n+\n+// In this test, we construct two sets of headers from genesis, one with\n+// sufficient proof of work and one without.\n+// 1. We deliver the first set of headers and verify that the headers sync state\n+//    updates to the REDOWNLOAD phase successfully.\n+// 2. Then we deliver the second set of headers and verify that they fail\n+//    processing (presumably due to commitments not matching).\n+// 3. Finally, we verify that repeating with the first set of headers in both\n+//    phases is successful.\n+BOOST_AUTO_TEST_CASE(headers_sync_state)\n+{\n+    std::vector<CBlockHeader> first_chain;\n+    std::vector<CBlockHeader> second_chain;\n+\n+    std::unique_ptr<HeadersSyncState> hss;\n+\n+    const int target_blocks = 15000;\n+    arith_uint256 chain_work = target_blocks*2;\n+\n+    // Generate headers for two different chains (using differing merkle roots\n+    // to ensure the headers are different).\n+    GenerateHeaders(first_chain, target_blocks-1, Params().GenesisBlock().GetHash(),\n+            Params().GenesisBlock().nVersion, Params().GenesisBlock().nTime,\n+            ArithToUint256(0), Params().GenesisBlock().nBits);\n+\n+    GenerateHeaders(second_chain, target_blocks-2, Params().GenesisBlock().GetHash(),\n+            Params().GenesisBlock().nVersion, Params().GenesisBlock().nTime,\n+            ArithToUint256(1), Params().GenesisBlock().nBits);\n+\n+    const CBlockIndex* chain_start = WITH_LOCK(::cs_main, return m_node.chainman->m_blockman.LookupBlockIndex(Params().GenesisBlock().GetHash()));\n+    std::vector<CBlockHeader> headers_batch;\n+\n+    // Feed the first chain to HeadersSyncState, by delivering 1 header\n+    // initially and then the rest.\n+    headers_batch.insert(headers_batch.end(), std::next(first_chain.begin()), first_chain.end());\n+\n+    hss.reset(new HeadersSyncState(0, Params().GetConsensus(), chain_start, chain_work));\n+    (void)hss->ProcessNextHeaders({first_chain.front()}, true);\n+    // Pretend the first header is still \"full\", so we don't abort.\n+    auto result = hss->ProcessNextHeaders(headers_batch, true);\n+\n+    // This chain should look valid, and we should have met the proof-of-work\n+    // requirement.\n+    BOOST_CHECK(result.success);\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::REDOWNLOAD);\n+\n+    // Try to sneakily feed back the second chain.\n+    result = hss->ProcessNextHeaders(second_chain, true);\n+    BOOST_CHECK(!result.success); // foiled!\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::FINAL);\n+\n+    // Now try again, this time feeding the first chain twice.\n+    hss.reset(new HeadersSyncState(0, Params().GetConsensus(), chain_start, chain_work));\n+    (void)hss->ProcessNextHeaders(first_chain, true);\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::REDOWNLOAD);\n+\n+    result = hss->ProcessNextHeaders(first_chain, true);\n+    BOOST_CHECK(result.success);\n+    // All headers should be ready for acceptance:\n+    BOOST_CHECK(result.pow_validated_headers.size() == first_chain.size());\n+    // Nothing left for the sync logic to do:\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::FINAL);\n+\n+    // Finally, verify that just trying to process the second chain would not\n+    // succeed (too little work)\n+    hss.reset(new HeadersSyncState(0, Params().GetConsensus(), chain_start, chain_work));\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::PRESYNC);\n+     // Pretend just the first message is \"full\", so we don't abort.\n+    (void)hss->ProcessNextHeaders({second_chain.front()}, true);\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::PRESYNC);\n+\n+    headers_batch.clear();\n+    headers_batch.insert(headers_batch.end(), std::next(second_chain.begin(), 1), second_chain.end());\n+    // Tell the sync logic that the headers message was not full, implying no\n+    // more headers can be requested. For a low-work-chain, this should causes\n+    // the sync to end with no headers for acceptance.\n+    result = hss->ProcessNextHeaders(headers_batch, false);\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::FINAL);\n+    BOOST_CHECK(result.pow_validated_headers.empty());\n+    // Nevertheless, no validation errors should have been detected with the\n+    // chain:\n+    BOOST_CHECK(result.success);",
      "path": "src/test/headers_sync_chainwork_tests.cpp",
      "position": 143,
      "original_position": 140,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f4d912a8e4d93f427101577f9fc7ec0476bc01d6",
      "in_reply_to_id": null,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "in 2263c2e646, maybe add check for `request_more`?\r\n```suggestion\r\n    BOOST_CHECK(result.success);\r\n    BOOST_CHECK(!result.request_more);\r\n```\r\n",
      "created_at": "2022-08-22T14:42:57Z",
      "updated_at": "2022-08-23T16:48:03Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r951528580",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951528580"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 143,
      "original_line": 143,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951530240",
      "pull_request_review_id": 1080118855,
      "id": 951530240,
      "node_id": "PRRC_kwDOABII5844tzMA",
      "diff_hunk": "@@ -0,0 +1,143 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <chain.h>\n+#include <chainparams.h>\n+#include <consensus/params.h>\n+#include <headerssync.h>\n+#include <pow.h>\n+#include <test/util/setup_common.h>\n+#include <validation.h>\n+#include <vector>\n+\n+#include <boost/test/unit_test.hpp>\n+\n+struct HeadersGeneratorSetup : public RegTestingSetup {\n+    /** Search for a nonce to meet (regtest) proof of work */\n+    void FindProofOfWork(CBlockHeader& starting_header);\n+    /**\n+     * Generate headers in a chain that build off a given starting hash, using\n+     * the given nVersion, advancing time by 1 second from the starting\n+     * prev_time, and with a fixed merkle root hash.\n+     */\n+    void GenerateHeaders(std::vector<CBlockHeader>& headers, size_t count,\n+            const uint256& starting_hash, const int nVersion, int prev_time,\n+            const uint256& merkle_root, const uint32_t nBits);\n+};\n+\n+void HeadersGeneratorSetup::FindProofOfWork(CBlockHeader& starting_header)\n+{\n+    while (!CheckProofOfWork(starting_header.GetHash(), starting_header.nBits, Params().GetConsensus())) {\n+        ++(starting_header.nNonce);\n+    }\n+}\n+\n+void HeadersGeneratorSetup::GenerateHeaders(std::vector<CBlockHeader>& headers,\n+        size_t count, const uint256& starting_hash, const int nVersion, int prev_time,\n+        const uint256& merkle_root, const uint32_t nBits)\n+{\n+    uint256 prev_hash = starting_hash;\n+\n+    while (headers.size() < count) {\n+        headers.push_back(CBlockHeader());\n+        CBlockHeader& next_header = headers.back();;\n+        next_header.nVersion = nVersion;\n+        next_header.hashPrevBlock = prev_hash;\n+        next_header.hashMerkleRoot = merkle_root;\n+        next_header.nTime = prev_time+1;\n+        next_header.nBits = nBits;\n+\n+        FindProofOfWork(next_header);\n+        prev_hash = next_header.GetHash();\n+        prev_time = next_header.nTime;\n+    }\n+    return;\n+}\n+\n+BOOST_FIXTURE_TEST_SUITE(headers_sync_chainwork_tests, HeadersGeneratorSetup)\n+\n+// In this test, we construct two sets of headers from genesis, one with\n+// sufficient proof of work and one without.\n+// 1. We deliver the first set of headers and verify that the headers sync state\n+//    updates to the REDOWNLOAD phase successfully.\n+// 2. Then we deliver the second set of headers and verify that they fail\n+//    processing (presumably due to commitments not matching).\n+// 3. Finally, we verify that repeating with the first set of headers in both\n+//    phases is successful.\n+BOOST_AUTO_TEST_CASE(headers_sync_state)\n+{\n+    std::vector<CBlockHeader> first_chain;\n+    std::vector<CBlockHeader> second_chain;\n+\n+    std::unique_ptr<HeadersSyncState> hss;\n+\n+    const int target_blocks = 15000;\n+    arith_uint256 chain_work = target_blocks*2;\n+\n+    // Generate headers for two different chains (using differing merkle roots\n+    // to ensure the headers are different).\n+    GenerateHeaders(first_chain, target_blocks-1, Params().GenesisBlock().GetHash(),\n+            Params().GenesisBlock().nVersion, Params().GenesisBlock().nTime,\n+            ArithToUint256(0), Params().GenesisBlock().nBits);\n+\n+    GenerateHeaders(second_chain, target_blocks-2, Params().GenesisBlock().GetHash(),\n+            Params().GenesisBlock().nVersion, Params().GenesisBlock().nTime,\n+            ArithToUint256(1), Params().GenesisBlock().nBits);\n+\n+    const CBlockIndex* chain_start = WITH_LOCK(::cs_main, return m_node.chainman->m_blockman.LookupBlockIndex(Params().GenesisBlock().GetHash()));\n+    std::vector<CBlockHeader> headers_batch;\n+\n+    // Feed the first chain to HeadersSyncState, by delivering 1 header\n+    // initially and then the rest.\n+    headers_batch.insert(headers_batch.end(), std::next(first_chain.begin()), first_chain.end());\n+\n+    hss.reset(new HeadersSyncState(0, Params().GetConsensus(), chain_start, chain_work));\n+    (void)hss->ProcessNextHeaders({first_chain.front()}, true);\n+    // Pretend the first header is still \"full\", so we don't abort.\n+    auto result = hss->ProcessNextHeaders(headers_batch, true);\n+\n+    // This chain should look valid, and we should have met the proof-of-work\n+    // requirement.\n+    BOOST_CHECK(result.success);\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::REDOWNLOAD);\n+\n+    // Try to sneakily feed back the second chain.\n+    result = hss->ProcessNextHeaders(second_chain, true);\n+    BOOST_CHECK(!result.success); // foiled!\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::FINAL);\n+\n+    // Now try again, this time feeding the first chain twice.\n+    hss.reset(new HeadersSyncState(0, Params().GetConsensus(), chain_start, chain_work));\n+    (void)hss->ProcessNextHeaders(first_chain, true);\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::REDOWNLOAD);\n+\n+    result = hss->ProcessNextHeaders(first_chain, true);\n+    BOOST_CHECK(result.success);",
      "path": "src/test/headers_sync_chainwork_tests.cpp",
      "position": 117,
      "original_position": 116,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f4d912a8e4d93f427101577f9fc7ec0476bc01d6",
      "in_reply_to_id": null,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "in 2263c2e646, same here, succeeded but `request_more` should be false since sync completed?\r\n```suggestion\r\n    BOOST_CHECK(result.success);\r\n    BOOST_CHECK(!result.request_more);\r\n```",
      "created_at": "2022-08-22T14:44:17Z",
      "updated_at": "2022-08-23T16:48:03Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r951530240",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951530240"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 117,
      "original_line": 117,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951532304",
      "pull_request_review_id": 1080118855,
      "id": 951532304,
      "node_id": "PRRC_kwDOABII5844tzsQ",
      "diff_hunk": "@@ -0,0 +1,143 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <chain.h>\n+#include <chainparams.h>\n+#include <consensus/params.h>\n+#include <headerssync.h>\n+#include <pow.h>\n+#include <test/util/setup_common.h>\n+#include <validation.h>\n+#include <vector>\n+\n+#include <boost/test/unit_test.hpp>\n+\n+struct HeadersGeneratorSetup : public RegTestingSetup {\n+    /** Search for a nonce to meet (regtest) proof of work */\n+    void FindProofOfWork(CBlockHeader& starting_header);\n+    /**\n+     * Generate headers in a chain that build off a given starting hash, using\n+     * the given nVersion, advancing time by 1 second from the starting\n+     * prev_time, and with a fixed merkle root hash.\n+     */\n+    void GenerateHeaders(std::vector<CBlockHeader>& headers, size_t count,\n+            const uint256& starting_hash, const int nVersion, int prev_time,\n+            const uint256& merkle_root, const uint32_t nBits);\n+};\n+\n+void HeadersGeneratorSetup::FindProofOfWork(CBlockHeader& starting_header)\n+{\n+    while (!CheckProofOfWork(starting_header.GetHash(), starting_header.nBits, Params().GetConsensus())) {\n+        ++(starting_header.nNonce);\n+    }\n+}\n+\n+void HeadersGeneratorSetup::GenerateHeaders(std::vector<CBlockHeader>& headers,\n+        size_t count, const uint256& starting_hash, const int nVersion, int prev_time,\n+        const uint256& merkle_root, const uint32_t nBits)\n+{\n+    uint256 prev_hash = starting_hash;\n+\n+    while (headers.size() < count) {\n+        headers.push_back(CBlockHeader());\n+        CBlockHeader& next_header = headers.back();;\n+        next_header.nVersion = nVersion;\n+        next_header.hashPrevBlock = prev_hash;\n+        next_header.hashMerkleRoot = merkle_root;\n+        next_header.nTime = prev_time+1;\n+        next_header.nBits = nBits;\n+\n+        FindProofOfWork(next_header);\n+        prev_hash = next_header.GetHash();\n+        prev_time = next_header.nTime;\n+    }\n+    return;\n+}\n+\n+BOOST_FIXTURE_TEST_SUITE(headers_sync_chainwork_tests, HeadersGeneratorSetup)\n+\n+// In this test, we construct two sets of headers from genesis, one with\n+// sufficient proof of work and one without.\n+// 1. We deliver the first set of headers and verify that the headers sync state\n+//    updates to the REDOWNLOAD phase successfully.\n+// 2. Then we deliver the second set of headers and verify that they fail\n+//    processing (presumably due to commitments not matching).\n+// 3. Finally, we verify that repeating with the first set of headers in both\n+//    phases is successful.\n+BOOST_AUTO_TEST_CASE(headers_sync_state)\n+{\n+    std::vector<CBlockHeader> first_chain;\n+    std::vector<CBlockHeader> second_chain;\n+\n+    std::unique_ptr<HeadersSyncState> hss;\n+\n+    const int target_blocks = 15000;\n+    arith_uint256 chain_work = target_blocks*2;\n+\n+    // Generate headers for two different chains (using differing merkle roots\n+    // to ensure the headers are different).\n+    GenerateHeaders(first_chain, target_blocks-1, Params().GenesisBlock().GetHash(),\n+            Params().GenesisBlock().nVersion, Params().GenesisBlock().nTime,\n+            ArithToUint256(0), Params().GenesisBlock().nBits);\n+\n+    GenerateHeaders(second_chain, target_blocks-2, Params().GenesisBlock().GetHash(),\n+            Params().GenesisBlock().nVersion, Params().GenesisBlock().nTime,\n+            ArithToUint256(1), Params().GenesisBlock().nBits);\n+\n+    const CBlockIndex* chain_start = WITH_LOCK(::cs_main, return m_node.chainman->m_blockman.LookupBlockIndex(Params().GenesisBlock().GetHash()));\n+    std::vector<CBlockHeader> headers_batch;\n+\n+    // Feed the first chain to HeadersSyncState, by delivering 1 header\n+    // initially and then the rest.\n+    headers_batch.insert(headers_batch.end(), std::next(first_chain.begin()), first_chain.end());\n+\n+    hss.reset(new HeadersSyncState(0, Params().GetConsensus(), chain_start, chain_work));\n+    (void)hss->ProcessNextHeaders({first_chain.front()}, true);\n+    // Pretend the first header is still \"full\", so we don't abort.\n+    auto result = hss->ProcessNextHeaders(headers_batch, true);\n+\n+    // This chain should look valid, and we should have met the proof-of-work\n+    // requirement.\n+    BOOST_CHECK(result.success);",
      "path": "src/test/headers_sync_chainwork_tests.cpp",
      "position": 102,
      "original_position": 102,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f4d912a8e4d93f427101577f9fc7ec0476bc01d6",
      "in_reply_to_id": null,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "in 2263c2e646, similarly, check that it said to request more?\r\n```suggestion\r\n    BOOST_CHECK(result.success);\r\n    BOOST_CHECK(result.request_more);\r\n```",
      "created_at": "2022-08-22T14:45:48Z",
      "updated_at": "2022-08-23T16:48:03Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r951532304",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951532304"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 102,
      "original_line": 102,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951551748",
      "pull_request_review_id": 1080118855,
      "id": 951551748,
      "node_id": "PRRC_kwDOABII5844t4cE",
      "diff_hunk": "@@ -0,0 +1,314 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD))\n+{\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 41,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "07b068e1568efdb6473f1d1eb4b870eb339c748b",
      "in_reply_to_id": null,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "In 956109788e, is there a reason to put these assignments in the function body instead of member initializer list? Could make `m_minimum_required_work` and `m_max_commitments` const members?",
      "created_at": "2022-08-22T15:01:34Z",
      "updated_at": "2022-08-23T16:48:03Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r951551748",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951551748"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 26,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 41,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951566903",
      "pull_request_review_id": 1080677272,
      "id": 951566903,
      "node_id": "PRRC_kwDOABII5844t8I3",
      "diff_hunk": "@@ -477,4 +477,10 @@ class CChain\n     CBlockIndex* FindEarliestAtLeast(int64_t nTime, int height) const;\n };\n \n+/** Get a locator for a block index entry. */\n+CBlockLocator GetLocator(const CBlockIndex* index);",
      "path": "src/chain.h",
      "position": 16,
      "original_position": 5,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5ea6f9a34bf3e8751fbca8f6107e99a9f553e31f",
      "in_reply_to_id": null,
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Having both `GetLocator()` and a `CChain::GetLocator()` with identical names, but slightly different behavior (if a nullptr is passed,  the latter returns a locator to the tip, the former returns an empty locator) seems like a possible source for errors. Maybe rename `CChain::GetLocator()` to `CChain::GetLocatorToIndexOrTip()` (could be done as a follow-up)?",
      "created_at": "2022-08-22T15:14:47Z",
      "updated_at": "2022-08-23T15:34:20Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r951566903",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951566903"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 487,
      "original_line": 487,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951609089",
      "pull_request_review_id": 1080677272,
      "id": 951609089,
      "node_id": "PRRC_kwDOABII5844uGcB",
      "diff_hunk": "@@ -4111,7 +4120,7 @@ void PeerManagerImpl::ProcessMessage(CNode& pfrom, const std::string& msg_type,\n             // we have a chain with at least nMinimumChainWork), and we ignore\n             // compact blocks with less work than our tip, it is safe to treat\n             // reconstructed compact blocks as having been requested.\n-            ProcessBlock(pfrom, pblock, /*force_processing=*/true);\n+            ProcessBlock(pfrom, pblock, /*force_processing=*/true, true);",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 75,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cdc1dd66a3fd0c3e6fa99057c827a3bb87431d0c",
      "in_reply_to_id": null,
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "nit (here and elsewhere): would be nice to have more named arguments, at least in all the places where they are already used for other args.",
      "created_at": "2022-08-22T15:54:14Z",
      "updated_at": "2022-08-23T15:34:20Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r951609089",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951609089"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 4259,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951626393",
      "pull_request_review_id": 1080677272,
      "id": 951626393,
      "node_id": "PRRC_kwDOABII5844uKqZ",
      "diff_hunk": "@@ -987,7 +991,7 @@ class ChainstateManager\n      * @param[out]  new_block A boolean which is set to indicate if the block was first received via this call\n      * @returns     If the block was processed, independently of block validity\n      */\n-    bool ProcessNewBlock(const std::shared_ptr<const CBlock>& block, bool force_processing, bool* new_block) LOCKS_EXCLUDED(cs_main);\n+    bool ProcessNewBlock(const std::shared_ptr<const CBlock>& block, bool force_processing, bool* new_block, const bool min_pow_checked) LOCKS_EXCLUDED(cs_main);",
      "path": "src/validation.h",
      "position": null,
      "original_position": 31,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cdc1dd66a3fd0c3e6fa99057c827a3bb87431d0c",
      "in_reply_to_id": null,
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "doc above needs an update, plus the ordering is no longer (in, in-out, out) per developer notes.",
      "created_at": "2022-08-22T16:11:42Z",
      "updated_at": "2022-08-23T15:34:20Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r951626393",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951626393"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 1010,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951626732",
      "pull_request_review_id": 1080677272,
      "id": 951626732,
      "node_id": "PRRC_kwDOABII5844uKvs",
      "diff_hunk": "@@ -999,7 +1003,7 @@ class ChainstateManager\n      * @param[out] state This may be set to an Error state if any error occurred processing them\n      * @param[out] ppindex If set, the pointer will be set to point to the last new block index object for the given headers\n      */\n-    bool ProcessNewBlockHeaders(const std::vector<CBlockHeader>& block, BlockValidationState& state, const CBlockIndex** ppindex = nullptr) LOCKS_EXCLUDED(cs_main);\n+    bool ProcessNewBlockHeaders(const std::vector<CBlockHeader>& block, BlockValidationState& state, const bool min_pow_checked, const CBlockIndex** ppindex = nullptr) LOCKS_EXCLUDED(cs_main);",
      "path": "src/validation.h",
      "position": null,
      "original_position": 40,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cdc1dd66a3fd0c3e6fa99057c827a3bb87431d0c",
      "in_reply_to_id": null,
      "user": {
        "login": "mzumsande",
        "id": 48763452,
        "node_id": "MDQ6VXNlcjQ4NzYzNDUy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48763452?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mzumsande",
        "html_url": "https://github.com/mzumsande",
        "followers_url": "https://api.github.com/users/mzumsande/followers",
        "following_url": "https://api.github.com/users/mzumsande/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/mzumsande/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/mzumsande/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/mzumsande/subscriptions",
        "organizations_url": "https://api.github.com/users/mzumsande/orgs",
        "repos_url": "https://api.github.com/users/mzumsande/repos",
        "events_url": "https://api.github.com/users/mzumsande/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/mzumsande/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "doc above needs an update, plus the ordering is no longer (in, in-out, out) per developer notes.",
      "created_at": "2022-08-22T16:12:02Z",
      "updated_at": "2022-08-23T15:34:20Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r951626732",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/951626732"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 1022,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/952573960",
      "pull_request_review_id": 1080118855,
      "id": 952573960,
      "node_id": "PRRC_kwDOABII5844xyAI",
      "diff_hunk": "@@ -215,13 +215,17 @@ QString ClientModel::blocksDir() const\n     return GUIUtil::PathToQString(gArgs.GetBlocksDirPath());\n }\n \n-void ClientModel::TipChanged(SynchronizationState sync_state, interfaces::BlockTip tip, double verification_progress, bool header)\n+void ClientModel::TipChanged(SynchronizationState sync_state, interfaces::BlockTip tip, double verification_progress, int header)",
      "path": "src/qt/clientmodel.cpp",
      "position": null,
      "original_position": 5,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f4d912a8e4d93f427101577f9fc7ec0476bc01d6",
      "in_reply_to_id": null,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "in 52eb9f33a1, `header` seems better suited as an enum, but I don't feel very strongly",
      "created_at": "2022-08-23T12:45:38Z",
      "updated_at": "2022-08-23T16:48:03Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r952573960",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/952573960"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 218,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/952726449",
      "pull_request_review_id": 1082319701,
      "id": 952726449,
      "node_id": "PRRC_kwDOABII5844yXOx",
      "diff_hunk": "@@ -2549,6 +2553,18 @@ bool PeerManagerImpl::TryLowWorkHeadersSync(Peer& peer, CNode& pfrom, const CBlo\n     return false;\n }\n \n+bool PeerManagerImpl::IsAncestorOfBestHeaderOrTip(const CBlockIndex* header)\n+{\n+    if (header == nullptr) {\n+        return false;\n+    } else if (m_chainman.m_best_header != nullptr && header == m_chainman.m_best_header->GetAncestor(header->nHeight)) {\n+        return true;\n+    } else if (m_chainman.ActiveTip() != nullptr && header == m_chainman.ActiveTip()->GetAncestor(header->nHeight)) {",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 21,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "96140e947937dfd1d275a2af9fd9782e5f17c41e",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think this can be written more efficiently as `(m_chainman.m_chain.Contains(header))`.",
      "created_at": "2022-08-23T14:47:53Z",
      "updated_at": "2022-08-23T14:48:44Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r952726449",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/952726449"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 2574,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/952891531",
      "pull_request_review_id": 1082534223,
      "id": 952891531,
      "node_id": "PRRC_kwDOABII5844y_iL",
      "diff_hunk": "@@ -215,13 +215,17 @@ QString ClientModel::blocksDir() const\n     return GUIUtil::PathToQString(gArgs.GetBlocksDirPath());\n }\n \n-void ClientModel::TipChanged(SynchronizationState sync_state, interfaces::BlockTip tip, double verification_progress, bool header)\n+void ClientModel::TipChanged(SynchronizationState sync_state, interfaces::BlockTip tip, double verification_progress, int header)",
      "path": "src/qt/clientmodel.cpp",
      "position": null,
      "original_position": 5,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f4d912a8e4d93f427101577f9fc7ec0476bc01d6",
      "in_reply_to_id": 952573960,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "@glozow The Qt signals framework can't enqueue enum values, unfortunately. The alternative is creating some wrapper class etc, but that seemed overkill. I agree this is suboptimal, but I think cleaner alternatives aren't worth the effort here.",
      "created_at": "2022-08-23T16:57:51Z",
      "updated_at": "2022-08-23T16:57:51Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r952891531",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/952891531"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 218,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/952892646",
      "pull_request_review_id": 1082535706,
      "id": 952892646,
      "node_id": "PRRC_kwDOABII5844y_zm",
      "diff_hunk": "@@ -215,13 +215,17 @@ QString ClientModel::blocksDir() const\n     return GUIUtil::PathToQString(gArgs.GetBlocksDirPath());\n }\n \n-void ClientModel::TipChanged(SynchronizationState sync_state, interfaces::BlockTip tip, double verification_progress, bool header)\n+void ClientModel::TipChanged(SynchronizationState sync_state, interfaces::BlockTip tip, double verification_progress, int header)",
      "path": "src/qt/clientmodel.cpp",
      "position": null,
      "original_position": 5,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f4d912a8e4d93f427101577f9fc7ec0476bc01d6",
      "in_reply_to_id": 952573960,
      "user": {
        "login": "glozow",
        "id": 25183001,
        "node_id": "MDQ6VXNlcjI1MTgzMDAx",
        "avatar_url": "https://avatars.githubusercontent.com/u/25183001?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/glozow",
        "html_url": "https://github.com/glozow",
        "followers_url": "https://api.github.com/users/glozow/followers",
        "following_url": "https://api.github.com/users/glozow/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/glozow/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/glozow/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/glozow/subscriptions",
        "organizations_url": "https://api.github.com/users/glozow/orgs",
        "repos_url": "https://api.github.com/users/glozow/repos",
        "events_url": "https://api.github.com/users/glozow/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/glozow/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Aha, that's my poor understanding of qt showing :sweat_smile: thanks @sipa ",
      "created_at": "2022-08-23T16:58:58Z",
      "updated_at": "2022-08-23T16:58:58Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r952892646",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/952892646"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 218,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/952989716",
      "pull_request_review_id": 1082668213,
      "id": 952989716,
      "node_id": "PRRC_kwDOABII5844zXgU",
      "diff_hunk": "@@ -2549,6 +2553,18 @@ bool PeerManagerImpl::TryLowWorkHeadersSync(Peer& peer, CNode& pfrom, const CBlo\n     return false;\n }\n \n+bool PeerManagerImpl::IsAncestorOfBestHeaderOrTip(const CBlockIndex* header)\n+{\n+    if (header == nullptr) {\n+        return false;\n+    } else if (m_chainman.m_best_header != nullptr && header == m_chainman.m_best_header->GetAncestor(header->nHeight)) {\n+        return true;\n+    } else if (m_chainman.ActiveTip() != nullptr && header == m_chainman.ActiveTip()->GetAncestor(header->nHeight)) {",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 21,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "96140e947937dfd1d275a2af9fd9782e5f17c41e",
      "in_reply_to_id": 952726449,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-23T18:40:01Z",
      "updated_at": "2022-08-23T18:40:01Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r952989716",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/952989716"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 2574,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/952990014",
      "pull_request_review_id": 1082668598,
      "id": 952990014,
      "node_id": "PRRC_kwDOABII5844zXk-",
      "diff_hunk": "@@ -4111,7 +4120,7 @@ void PeerManagerImpl::ProcessMessage(CNode& pfrom, const std::string& msg_type,\n             // we have a chain with at least nMinimumChainWork), and we ignore\n             // compact blocks with less work than our tip, it is safe to treat\n             // reconstructed compact blocks as having been requested.\n-            ProcessBlock(pfrom, pblock, /*force_processing=*/true);\n+            ProcessBlock(pfrom, pblock, /*force_processing=*/true, true);",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 75,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cdc1dd66a3fd0c3e6fa99057c827a3bb87431d0c",
      "in_reply_to_id": 951609089,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-23T18:40:22Z",
      "updated_at": "2022-08-23T18:40:22Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r952990014",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/952990014"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 4259,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/952990546",
      "pull_request_review_id": 1082669299,
      "id": 952990546,
      "node_id": "PRRC_kwDOABII5844zXtS",
      "diff_hunk": "@@ -987,7 +991,7 @@ class ChainstateManager\n      * @param[out]  new_block A boolean which is set to indicate if the block was first received via this call\n      * @returns     If the block was processed, independently of block validity\n      */\n-    bool ProcessNewBlock(const std::shared_ptr<const CBlock>& block, bool force_processing, bool* new_block) LOCKS_EXCLUDED(cs_main);\n+    bool ProcessNewBlock(const std::shared_ptr<const CBlock>& block, bool force_processing, bool* new_block, const bool min_pow_checked) LOCKS_EXCLUDED(cs_main);",
      "path": "src/validation.h",
      "position": null,
      "original_position": 31,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cdc1dd66a3fd0c3e6fa99057c827a3bb87431d0c",
      "in_reply_to_id": 951626393,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed.",
      "created_at": "2022-08-23T18:40:58Z",
      "updated_at": "2022-08-23T18:40:59Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r952990546",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/952990546"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 1010,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/952990972",
      "pull_request_review_id": 1082669928,
      "id": 952990972,
      "node_id": "PRRC_kwDOABII5844zXz8",
      "diff_hunk": "@@ -999,7 +1003,7 @@ class ChainstateManager\n      * @param[out] state This may be set to an Error state if any error occurred processing them\n      * @param[out] ppindex If set, the pointer will be set to point to the last new block index object for the given headers\n      */\n-    bool ProcessNewBlockHeaders(const std::vector<CBlockHeader>& block, BlockValidationState& state, const CBlockIndex** ppindex = nullptr) LOCKS_EXCLUDED(cs_main);\n+    bool ProcessNewBlockHeaders(const std::vector<CBlockHeader>& block, BlockValidationState& state, const bool min_pow_checked, const CBlockIndex** ppindex = nullptr) LOCKS_EXCLUDED(cs_main);",
      "path": "src/validation.h",
      "position": null,
      "original_position": 40,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "cdc1dd66a3fd0c3e6fa99057c827a3bb87431d0c",
      "in_reply_to_id": 951626732,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed.",
      "created_at": "2022-08-23T18:41:31Z",
      "updated_at": "2022-08-23T18:41:31Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r952990972",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/952990972"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 1022,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/952992515",
      "pull_request_review_id": 1082672167,
      "id": 952992515,
      "node_id": "PRRC_kwDOABII5844zYMD",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential",
      "path": "src/headerssync.h",
      "position": 96,
      "original_position": 96,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "07b068e1568efdb6473f1d1eb4b870eb339c748b",
      "in_reply_to_id": 951191470,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "According to https://www.merriam-webster.com/dictionary/parametrization and https://en.wikipedia.org/wiki/Parametrization, both spellings are acceptable!",
      "created_at": "2022-08-23T18:43:28Z",
      "updated_at": "2022-08-23T18:43:28Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r952992515",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/952992515"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 96,
      "original_line": 96,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953018693",
      "pull_request_review_id": 1082709597,
      "id": 953018693,
      "node_id": "PRRC_kwDOABII5844zelF",
      "diff_hunk": "@@ -0,0 +1,143 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <chain.h>\n+#include <chainparams.h>\n+#include <consensus/params.h>\n+#include <headerssync.h>\n+#include <pow.h>\n+#include <test/util/setup_common.h>\n+#include <validation.h>\n+#include <vector>\n+\n+#include <boost/test/unit_test.hpp>\n+\n+struct HeadersGeneratorSetup : public RegTestingSetup {\n+    /** Search for a nonce to meet (regtest) proof of work */\n+    void FindProofOfWork(CBlockHeader& starting_header);\n+    /**\n+     * Generate headers in a chain that build off a given starting hash, using\n+     * the given nVersion, advancing time by 1 second from the starting\n+     * prev_time, and with a fixed merkle root hash.\n+     */\n+    void GenerateHeaders(std::vector<CBlockHeader>& headers, size_t count,\n+            const uint256& starting_hash, const int nVersion, int prev_time,\n+            const uint256& merkle_root, const uint32_t nBits);\n+};\n+\n+void HeadersGeneratorSetup::FindProofOfWork(CBlockHeader& starting_header)\n+{\n+    while (!CheckProofOfWork(starting_header.GetHash(), starting_header.nBits, Params().GetConsensus())) {\n+        ++(starting_header.nNonce);\n+    }\n+}\n+\n+void HeadersGeneratorSetup::GenerateHeaders(std::vector<CBlockHeader>& headers,\n+        size_t count, const uint256& starting_hash, const int nVersion, int prev_time,\n+        const uint256& merkle_root, const uint32_t nBits)\n+{\n+    uint256 prev_hash = starting_hash;\n+\n+    while (headers.size() < count) {\n+        headers.push_back(CBlockHeader());\n+        CBlockHeader& next_header = headers.back();;\n+        next_header.nVersion = nVersion;\n+        next_header.hashPrevBlock = prev_hash;\n+        next_header.hashMerkleRoot = merkle_root;\n+        next_header.nTime = prev_time+1;\n+        next_header.nBits = nBits;\n+\n+        FindProofOfWork(next_header);\n+        prev_hash = next_header.GetHash();\n+        prev_time = next_header.nTime;\n+    }\n+    return;\n+}\n+\n+BOOST_FIXTURE_TEST_SUITE(headers_sync_chainwork_tests, HeadersGeneratorSetup)\n+\n+// In this test, we construct two sets of headers from genesis, one with\n+// sufficient proof of work and one without.\n+// 1. We deliver the first set of headers and verify that the headers sync state\n+//    updates to the REDOWNLOAD phase successfully.\n+// 2. Then we deliver the second set of headers and verify that they fail\n+//    processing (presumably due to commitments not matching).\n+// 3. Finally, we verify that repeating with the first set of headers in both\n+//    phases is successful.\n+BOOST_AUTO_TEST_CASE(headers_sync_state)\n+{\n+    std::vector<CBlockHeader> first_chain;\n+    std::vector<CBlockHeader> second_chain;\n+\n+    std::unique_ptr<HeadersSyncState> hss;\n+\n+    const int target_blocks = 15000;\n+    arith_uint256 chain_work = target_blocks*2;\n+\n+    // Generate headers for two different chains (using differing merkle roots\n+    // to ensure the headers are different).\n+    GenerateHeaders(first_chain, target_blocks-1, Params().GenesisBlock().GetHash(),\n+            Params().GenesisBlock().nVersion, Params().GenesisBlock().nTime,\n+            ArithToUint256(0), Params().GenesisBlock().nBits);\n+\n+    GenerateHeaders(second_chain, target_blocks-2, Params().GenesisBlock().GetHash(),\n+            Params().GenesisBlock().nVersion, Params().GenesisBlock().nTime,\n+            ArithToUint256(1), Params().GenesisBlock().nBits);\n+\n+    const CBlockIndex* chain_start = WITH_LOCK(::cs_main, return m_node.chainman->m_blockman.LookupBlockIndex(Params().GenesisBlock().GetHash()));\n+    std::vector<CBlockHeader> headers_batch;\n+\n+    // Feed the first chain to HeadersSyncState, by delivering 1 header\n+    // initially and then the rest.\n+    headers_batch.insert(headers_batch.end(), std::next(first_chain.begin()), first_chain.end());\n+\n+    hss.reset(new HeadersSyncState(0, Params().GetConsensus(), chain_start, chain_work));\n+    (void)hss->ProcessNextHeaders({first_chain.front()}, true);\n+    // Pretend the first header is still \"full\", so we don't abort.\n+    auto result = hss->ProcessNextHeaders(headers_batch, true);\n+\n+    // This chain should look valid, and we should have met the proof-of-work\n+    // requirement.\n+    BOOST_CHECK(result.success);\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::REDOWNLOAD);\n+\n+    // Try to sneakily feed back the second chain.\n+    result = hss->ProcessNextHeaders(second_chain, true);\n+    BOOST_CHECK(!result.success); // foiled!\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::FINAL);\n+\n+    // Now try again, this time feeding the first chain twice.\n+    hss.reset(new HeadersSyncState(0, Params().GetConsensus(), chain_start, chain_work));\n+    (void)hss->ProcessNextHeaders(first_chain, true);\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::REDOWNLOAD);\n+\n+    result = hss->ProcessNextHeaders(first_chain, true);\n+    BOOST_CHECK(result.success);\n+    // All headers should be ready for acceptance:\n+    BOOST_CHECK(result.pow_validated_headers.size() == first_chain.size());\n+    // Nothing left for the sync logic to do:\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::FINAL);\n+\n+    // Finally, verify that just trying to process the second chain would not\n+    // succeed (too little work)\n+    hss.reset(new HeadersSyncState(0, Params().GetConsensus(), chain_start, chain_work));\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::PRESYNC);\n+     // Pretend just the first message is \"full\", so we don't abort.\n+    (void)hss->ProcessNextHeaders({second_chain.front()}, true);\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::PRESYNC);\n+\n+    headers_batch.clear();\n+    headers_batch.insert(headers_batch.end(), std::next(second_chain.begin(), 1), second_chain.end());\n+    // Tell the sync logic that the headers message was not full, implying no\n+    // more headers can be requested. For a low-work-chain, this should causes\n+    // the sync to end with no headers for acceptance.\n+    result = hss->ProcessNextHeaders(headers_batch, false);\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::FINAL);\n+    BOOST_CHECK(result.pow_validated_headers.empty());\n+    // Nevertheless, no validation errors should have been detected with the\n+    // chain:\n+    BOOST_CHECK(result.success);",
      "path": "src/test/headers_sync_chainwork_tests.cpp",
      "position": 143,
      "original_position": 140,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f4d912a8e4d93f427101577f9fc7ec0476bc01d6",
      "in_reply_to_id": 951528580,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done, thanks.",
      "created_at": "2022-08-23T19:16:37Z",
      "updated_at": "2022-08-23T19:16:37Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953018693",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953018693"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 143,
      "original_line": 143,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953021160",
      "pull_request_review_id": 1082713417,
      "id": 953021160,
      "node_id": "PRRC_kwDOABII5844zfLo",
      "diff_hunk": "@@ -0,0 +1,143 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <chain.h>\n+#include <chainparams.h>\n+#include <consensus/params.h>\n+#include <headerssync.h>\n+#include <pow.h>\n+#include <test/util/setup_common.h>\n+#include <validation.h>\n+#include <vector>\n+\n+#include <boost/test/unit_test.hpp>\n+\n+struct HeadersGeneratorSetup : public RegTestingSetup {\n+    /** Search for a nonce to meet (regtest) proof of work */\n+    void FindProofOfWork(CBlockHeader& starting_header);\n+    /**\n+     * Generate headers in a chain that build off a given starting hash, using\n+     * the given nVersion, advancing time by 1 second from the starting\n+     * prev_time, and with a fixed merkle root hash.\n+     */\n+    void GenerateHeaders(std::vector<CBlockHeader>& headers, size_t count,\n+            const uint256& starting_hash, const int nVersion, int prev_time,\n+            const uint256& merkle_root, const uint32_t nBits);\n+};\n+\n+void HeadersGeneratorSetup::FindProofOfWork(CBlockHeader& starting_header)\n+{\n+    while (!CheckProofOfWork(starting_header.GetHash(), starting_header.nBits, Params().GetConsensus())) {\n+        ++(starting_header.nNonce);\n+    }\n+}\n+\n+void HeadersGeneratorSetup::GenerateHeaders(std::vector<CBlockHeader>& headers,\n+        size_t count, const uint256& starting_hash, const int nVersion, int prev_time,\n+        const uint256& merkle_root, const uint32_t nBits)\n+{\n+    uint256 prev_hash = starting_hash;\n+\n+    while (headers.size() < count) {\n+        headers.push_back(CBlockHeader());\n+        CBlockHeader& next_header = headers.back();;\n+        next_header.nVersion = nVersion;\n+        next_header.hashPrevBlock = prev_hash;\n+        next_header.hashMerkleRoot = merkle_root;\n+        next_header.nTime = prev_time+1;\n+        next_header.nBits = nBits;\n+\n+        FindProofOfWork(next_header);\n+        prev_hash = next_header.GetHash();\n+        prev_time = next_header.nTime;\n+    }\n+    return;\n+}\n+\n+BOOST_FIXTURE_TEST_SUITE(headers_sync_chainwork_tests, HeadersGeneratorSetup)\n+\n+// In this test, we construct two sets of headers from genesis, one with\n+// sufficient proof of work and one without.\n+// 1. We deliver the first set of headers and verify that the headers sync state\n+//    updates to the REDOWNLOAD phase successfully.\n+// 2. Then we deliver the second set of headers and verify that they fail\n+//    processing (presumably due to commitments not matching).\n+// 3. Finally, we verify that repeating with the first set of headers in both\n+//    phases is successful.\n+BOOST_AUTO_TEST_CASE(headers_sync_state)\n+{\n+    std::vector<CBlockHeader> first_chain;\n+    std::vector<CBlockHeader> second_chain;\n+\n+    std::unique_ptr<HeadersSyncState> hss;\n+\n+    const int target_blocks = 15000;\n+    arith_uint256 chain_work = target_blocks*2;\n+\n+    // Generate headers for two different chains (using differing merkle roots\n+    // to ensure the headers are different).\n+    GenerateHeaders(first_chain, target_blocks-1, Params().GenesisBlock().GetHash(),\n+            Params().GenesisBlock().nVersion, Params().GenesisBlock().nTime,\n+            ArithToUint256(0), Params().GenesisBlock().nBits);\n+\n+    GenerateHeaders(second_chain, target_blocks-2, Params().GenesisBlock().GetHash(),\n+            Params().GenesisBlock().nVersion, Params().GenesisBlock().nTime,\n+            ArithToUint256(1), Params().GenesisBlock().nBits);\n+\n+    const CBlockIndex* chain_start = WITH_LOCK(::cs_main, return m_node.chainman->m_blockman.LookupBlockIndex(Params().GenesisBlock().GetHash()));\n+    std::vector<CBlockHeader> headers_batch;\n+\n+    // Feed the first chain to HeadersSyncState, by delivering 1 header\n+    // initially and then the rest.\n+    headers_batch.insert(headers_batch.end(), std::next(first_chain.begin()), first_chain.end());\n+\n+    hss.reset(new HeadersSyncState(0, Params().GetConsensus(), chain_start, chain_work));\n+    (void)hss->ProcessNextHeaders({first_chain.front()}, true);\n+    // Pretend the first header is still \"full\", so we don't abort.\n+    auto result = hss->ProcessNextHeaders(headers_batch, true);\n+\n+    // This chain should look valid, and we should have met the proof-of-work\n+    // requirement.\n+    BOOST_CHECK(result.success);\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::REDOWNLOAD);\n+\n+    // Try to sneakily feed back the second chain.\n+    result = hss->ProcessNextHeaders(second_chain, true);\n+    BOOST_CHECK(!result.success); // foiled!\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::FINAL);\n+\n+    // Now try again, this time feeding the first chain twice.\n+    hss.reset(new HeadersSyncState(0, Params().GetConsensus(), chain_start, chain_work));\n+    (void)hss->ProcessNextHeaders(first_chain, true);\n+    BOOST_CHECK(hss->GetState() == HeadersSyncState::State::REDOWNLOAD);\n+\n+    result = hss->ProcessNextHeaders(first_chain, true);\n+    BOOST_CHECK(result.success);",
      "path": "src/test/headers_sync_chainwork_tests.cpp",
      "position": 117,
      "original_position": 116,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f4d912a8e4d93f427101577f9fc7ec0476bc01d6",
      "in_reply_to_id": 951530240,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-23T19:19:50Z",
      "updated_at": "2022-08-23T19:19:51Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953021160",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953021160"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 117,
      "original_line": 117,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953021235",
      "pull_request_review_id": 1082713504,
      "id": 953021235,
      "node_id": "PRRC_kwDOABII5844zfMz",
      "diff_hunk": "@@ -0,0 +1,143 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <chain.h>\n+#include <chainparams.h>\n+#include <consensus/params.h>\n+#include <headerssync.h>\n+#include <pow.h>\n+#include <test/util/setup_common.h>\n+#include <validation.h>\n+#include <vector>\n+\n+#include <boost/test/unit_test.hpp>\n+\n+struct HeadersGeneratorSetup : public RegTestingSetup {\n+    /** Search for a nonce to meet (regtest) proof of work */\n+    void FindProofOfWork(CBlockHeader& starting_header);\n+    /**\n+     * Generate headers in a chain that build off a given starting hash, using\n+     * the given nVersion, advancing time by 1 second from the starting\n+     * prev_time, and with a fixed merkle root hash.\n+     */\n+    void GenerateHeaders(std::vector<CBlockHeader>& headers, size_t count,\n+            const uint256& starting_hash, const int nVersion, int prev_time,\n+            const uint256& merkle_root, const uint32_t nBits);\n+};\n+\n+void HeadersGeneratorSetup::FindProofOfWork(CBlockHeader& starting_header)\n+{\n+    while (!CheckProofOfWork(starting_header.GetHash(), starting_header.nBits, Params().GetConsensus())) {\n+        ++(starting_header.nNonce);\n+    }\n+}\n+\n+void HeadersGeneratorSetup::GenerateHeaders(std::vector<CBlockHeader>& headers,\n+        size_t count, const uint256& starting_hash, const int nVersion, int prev_time,\n+        const uint256& merkle_root, const uint32_t nBits)\n+{\n+    uint256 prev_hash = starting_hash;\n+\n+    while (headers.size() < count) {\n+        headers.push_back(CBlockHeader());\n+        CBlockHeader& next_header = headers.back();;\n+        next_header.nVersion = nVersion;\n+        next_header.hashPrevBlock = prev_hash;\n+        next_header.hashMerkleRoot = merkle_root;\n+        next_header.nTime = prev_time+1;\n+        next_header.nBits = nBits;\n+\n+        FindProofOfWork(next_header);\n+        prev_hash = next_header.GetHash();\n+        prev_time = next_header.nTime;\n+    }\n+    return;\n+}\n+\n+BOOST_FIXTURE_TEST_SUITE(headers_sync_chainwork_tests, HeadersGeneratorSetup)\n+\n+// In this test, we construct two sets of headers from genesis, one with\n+// sufficient proof of work and one without.\n+// 1. We deliver the first set of headers and verify that the headers sync state\n+//    updates to the REDOWNLOAD phase successfully.\n+// 2. Then we deliver the second set of headers and verify that they fail\n+//    processing (presumably due to commitments not matching).\n+// 3. Finally, we verify that repeating with the first set of headers in both\n+//    phases is successful.\n+BOOST_AUTO_TEST_CASE(headers_sync_state)\n+{\n+    std::vector<CBlockHeader> first_chain;\n+    std::vector<CBlockHeader> second_chain;\n+\n+    std::unique_ptr<HeadersSyncState> hss;\n+\n+    const int target_blocks = 15000;\n+    arith_uint256 chain_work = target_blocks*2;\n+\n+    // Generate headers for two different chains (using differing merkle roots\n+    // to ensure the headers are different).\n+    GenerateHeaders(first_chain, target_blocks-1, Params().GenesisBlock().GetHash(),\n+            Params().GenesisBlock().nVersion, Params().GenesisBlock().nTime,\n+            ArithToUint256(0), Params().GenesisBlock().nBits);\n+\n+    GenerateHeaders(second_chain, target_blocks-2, Params().GenesisBlock().GetHash(),\n+            Params().GenesisBlock().nVersion, Params().GenesisBlock().nTime,\n+            ArithToUint256(1), Params().GenesisBlock().nBits);\n+\n+    const CBlockIndex* chain_start = WITH_LOCK(::cs_main, return m_node.chainman->m_blockman.LookupBlockIndex(Params().GenesisBlock().GetHash()));\n+    std::vector<CBlockHeader> headers_batch;\n+\n+    // Feed the first chain to HeadersSyncState, by delivering 1 header\n+    // initially and then the rest.\n+    headers_batch.insert(headers_batch.end(), std::next(first_chain.begin()), first_chain.end());\n+\n+    hss.reset(new HeadersSyncState(0, Params().GetConsensus(), chain_start, chain_work));\n+    (void)hss->ProcessNextHeaders({first_chain.front()}, true);\n+    // Pretend the first header is still \"full\", so we don't abort.\n+    auto result = hss->ProcessNextHeaders(headers_batch, true);\n+\n+    // This chain should look valid, and we should have met the proof-of-work\n+    // requirement.\n+    BOOST_CHECK(result.success);",
      "path": "src/test/headers_sync_chainwork_tests.cpp",
      "position": 102,
      "original_position": 102,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f4d912a8e4d93f427101577f9fc7ec0476bc01d6",
      "in_reply_to_id": 951532304,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-23T19:19:55Z",
      "updated_at": "2022-08-23T19:19:55Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953021235",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953021235"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 102,
      "original_line": 102,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953035385",
      "pull_request_review_id": 1082732552,
      "id": 953035385,
      "node_id": "PRRC_kwDOABII5844zip5",
      "diff_hunk": "@@ -0,0 +1,314 @@\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD))\n+{\n+    m_chain_start = chain_start;\n+    m_minimum_required_work = minimum_required_work;\n+    m_current_chain_work = chain_start->nChainWork;\n+    m_current_height = chain_start->nHeight;\n+\n+    m_last_header_received = m_chain_start->GetBlockHeader();\n+\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(GetAdjustedTime() - chain_start->GetMedianTimePast() + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 41,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "07b068e1568efdb6473f1d1eb4b870eb339c748b",
      "in_reply_to_id": 951551748,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done, except I left `m_max_commitments`, just because the calculation and comments are so long that it seems unwieldy to put in the initializer list.",
      "created_at": "2022-08-23T19:28:04Z",
      "updated_at": "2022-08-23T19:28:04Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953035385",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953035385"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": 26,
      "start_side": "RIGHT",
      "line": null,
      "original_line": 41,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953064161",
      "pull_request_review_id": 1082775092,
      "id": 953064161,
      "node_id": "PRRC_kwDOABII5844zprh",
      "diff_hunk": "@@ -477,4 +477,10 @@ class CChain\n     CBlockIndex* FindEarliestAtLeast(int64_t nTime, int height) const;\n };\n \n+/** Get a locator for a block index entry. */\n+CBlockLocator GetLocator(const CBlockIndex* index);",
      "path": "src/chain.h",
      "position": 16,
      "original_position": 5,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5ea6f9a34bf3e8751fbca8f6107e99a9f553e31f",
      "in_reply_to_id": 951566903,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Included a fixup by @sipa to eliminate the `CChain::GetLocator(const CBlockIndex *)`, in favor of `CChain::GetLocator()` which just operates on the tip.",
      "created_at": "2022-08-23T20:06:19Z",
      "updated_at": "2022-08-23T20:06:19Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953064161",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953064161"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 487,
      "original_line": 487,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953066194",
      "pull_request_review_id": 1082778055,
      "id": 953066194,
      "node_id": "PRRC_kwDOABII5844zqLS",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** PRESYNC means the peer has not yet demonstrated their chain has\n+         * sufficient work and we're only building commitments to the chain they\n+         * serve us. */\n+        PRESYNC,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Construct a HeadersSyncState object representing a headers sync via this\n+     *  download-twice mechanism).\n+     *\n+     * id: node id (for logging)\n+     * consensus_params: parameters needed for difficulty adjustment validation\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+            const CBlockIndex* chain_start, const arith_uint256& minimum_required_work);\n+\n+    /** Result data structure for ProcessNextHeaders. */\n+    struct ProcessingResult {\n+        std::vector<CBlockHeader> pow_validated_headers;\n+        bool success{false};\n+        bool request_more{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * received_headers: headers that were received over the network for processing.\n+     *                   Assumes the caller has already verified the headers connect,\n+     *                   and has checked that each header satisfies the\n+     *                   proof-of-work target included in the header (but not\n+     *                   necessarily verified that the proof-of-work target is\n+     *                   correct and passes consensus rules).\n+     * full_headers_message: true if the message was at max capacity,\n+     *                       indicating more headers may be available\n+     * ProcessingResult.pow_validated_headers: will be filled in with any\n+     *                       headers that the caller can fully process and\n+     *                       validate now (because these returned headers are\n+     *                       on a chain with sufficient work)\n+     * ProcessingResult.success: set to false if an error is detected and the sync is\n+     *                       aborted; true otherwise.\n+     * ProcessingResult.request_more: if true, the caller is suggested to call\n+     *                       MakeNextHeadersRequest and send a getheaders message using it.\n+     */\n+    ProcessingResult ProcessNextHeaders(const std::vector<CBlockHeader>&\n+            received_headers, bool full_headers_message);\n+\n+    /** Issue the next GETHEADERS message to our peer.\n+     *\n+     * This will return a locator appropriate for the current sync object, to continue the\n+     * synchronization phase it is in.\n+     */\n+    CBlockLocator MakeNextHeadersRequest() const;\n+\n+private:\n+    /** Clear out all download state that might be in progress (freeing any used\n+     * memory), and mark this object as no longer usable.\n+     */\n+    void Finalize();\n+\n+    /**\n+     *  Only called in PRESYNC.\n+     *  Validate the work on the headers we received from the network, and\n+     *  store commitments for later. Update overall state with successfully\n+     *  processed headers.\n+     *  On failure, this invokes Finalize() and returns false.\n+     */\n+    bool ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers);\n+\n+    /** In PRESYNC, process and update state for a single header */\n+    bool ValidateAndProcessSingleHeader(const CBlockHeader& previous, const\n+            CBlockHeader& current);\n+\n+    /** In REDOWNLOAD, check a header's commitment (if applicable) and add to\n+     * buffer for later processing */\n+    bool ValidateAndStoreRedownloadedHeader(const CBlockHeader& header);\n+\n+    /** Return a set of headers that satisfy our proof-of-work threshold */\n+    std::vector<CBlockHeader> PopHeadersReadyForAcceptance();\n+\n+private:",
      "path": "src/headerssync.h",
      "position": 203,
      "original_position": 194,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "07b068e1568efdb6473f1d1eb4b870eb339c748b",
      "in_reply_to_id": 951250735,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Yeah, I just like the separation between member functions and data variables, but if this is bothersome I can remove it.",
      "created_at": "2022-08-23T20:09:00Z",
      "updated_at": "2022-08-23T20:09:00Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953066194",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953066194"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 203,
      "original_line": 203,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953152216",
      "pull_request_review_id": 1082900947,
      "id": 953152216,
      "node_id": "PRRC_kwDOABII5844z_LY",
      "diff_hunk": "@@ -0,0 +1,312 @@\n+#include <headerssync.h>",
      "path": "src/headerssync.cpp",
      "position": 5,
      "original_position": 1,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5d492952165ff396480e170ac20adc6e8005af91",
      "in_reply_to_id": null,
      "user": {
        "login": "fjahr",
        "id": 1322187,
        "node_id": "MDQ6VXNlcjEzMjIxODc=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1322187?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/fjahr",
        "html_url": "https://github.com/fjahr",
        "followers_url": "https://api.github.com/users/fjahr/followers",
        "following_url": "https://api.github.com/users/fjahr/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/fjahr/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/fjahr/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/fjahr/subscriptions",
        "organizations_url": "https://api.github.com/users/fjahr/orgs",
        "repos_url": "https://api.github.com/users/fjahr/repos",
        "events_url": "https://api.github.com/users/fjahr/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/fjahr/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "This file is missing the copyright notice.",
      "created_at": "2022-08-23T22:12:25Z",
      "updated_at": "2022-08-23T22:12:25Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953152216",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953152216"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 5,
      "original_line": 5,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953679745",
      "pull_request_review_id": 1083624569,
      "id": 953679745,
      "node_id": "PRRC_kwDOABII58441_-B",
      "diff_hunk": "@@ -215,13 +215,17 @@ QString ClientModel::blocksDir() const\n     return GUIUtil::PathToQString(gArgs.GetBlocksDirPath());\n }\n \n-void ClientModel::TipChanged(SynchronizationState sync_state, interfaces::BlockTip tip, double verification_progress, bool header)\n+void ClientModel::TipChanged(SynchronizationState sync_state, interfaces::BlockTip tip, double verification_progress, int header)",
      "path": "src/qt/clientmodel.cpp",
      "position": null,
      "original_position": 5,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f4d912a8e4d93f427101577f9fc7ec0476bc01d6",
      "in_reply_to_id": 952573960,
      "user": {
        "login": "hebasto",
        "id": 32963518,
        "node_id": "MDQ6VXNlcjMyOTYzNTE4",
        "avatar_url": "https://avatars.githubusercontent.com/u/32963518?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/hebasto",
        "html_url": "https://github.com/hebasto",
        "followers_url": "https://api.github.com/users/hebasto/followers",
        "following_url": "https://api.github.com/users/hebasto/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/hebasto/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/hebasto/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/hebasto/subscriptions",
        "organizations_url": "https://api.github.com/users/hebasto/orgs",
        "repos_url": "https://api.github.com/users/hebasto/repos",
        "events_url": "https://api.github.com/users/hebasto/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/hebasto/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "@glozow @sipa \r\n\r\n> The Qt signals framework can't enqueue enum values, unfortunately.\r\n\r\nThey could. See #18152 as an example.",
      "created_at": "2022-08-24T11:33:17Z",
      "updated_at": "2022-08-24T11:33:17Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953679745",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953679745"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 218,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953683429",
      "pull_request_review_id": 1083629623,
      "id": 953683429,
      "node_id": "PRRC_kwDOABII58442A3l",
      "diff_hunk": "@@ -0,0 +1,312 @@\n+#include <headerssync.h>",
      "path": "src/headerssync.cpp",
      "position": 5,
      "original_position": 1,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5d492952165ff396480e170ac20adc6e8005af91",
      "in_reply_to_id": 953152216,
      "user": {
        "login": "fanquake",
        "id": 863730,
        "node_id": "MDQ6VXNlcjg2MzczMA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/863730?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/fanquake",
        "html_url": "https://github.com/fanquake",
        "followers_url": "https://api.github.com/users/fanquake/followers",
        "following_url": "https://api.github.com/users/fanquake/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/fanquake/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/fanquake/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/fanquake/subscriptions",
        "organizations_url": "https://api.github.com/users/fanquake/orgs",
        "repos_url": "https://api.github.com/users/fanquake/repos",
        "events_url": "https://api.github.com/users/fanquake/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/fanquake/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Has been added.",
      "created_at": "2022-08-24T11:37:28Z",
      "updated_at": "2022-08-24T11:37:28Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953683429",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953683429"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 5,
      "original_line": 5,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953690718",
      "pull_request_review_id": 1083645668,
      "id": 953690718,
      "node_id": "PRRC_kwDOABII58442Cpe",
      "diff_hunk": "@@ -264,11 +268,11 @@ void ClientModel::subscribeToCoreSignals()\n         });\n     m_handler_notify_block_tip = m_node.handleNotifyBlockTip(\n         [this](SynchronizationState sync_state, interfaces::BlockTip tip, double verification_progress) {\n-            TipChanged(sync_state, tip, verification_progress, /*header=*/false);\n+            TipChanged(sync_state, tip, verification_progress, /*header=*/0);\n         });\n     m_handler_notify_header_tip = m_node.handleNotifyHeaderTip(\n-        [this](SynchronizationState sync_state, interfaces::BlockTip tip, double verification_progress) {\n-            TipChanged(sync_state, tip, verification_progress, /*header=*/true);\n+        [this](SynchronizationState sync_state, interfaces::BlockTip tip, bool presync) {\n+            TipChanged(sync_state, tip, /*verification_progress=*/0.0, /*header=*/2 - presync);",
      "path": "src/qt/clientmodel.cpp",
      "position": null,
      "original_position": 32,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "bbef38fdac90a03020ad70cbe6b97fbf09fccbc6",
      "in_reply_to_id": null,
      "user": {
        "login": "hebasto",
        "id": 32963518,
        "node_id": "MDQ6VXNlcjMyOTYzNTE4",
        "avatar_url": "https://avatars.githubusercontent.com/u/32963518?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/hebasto",
        "html_url": "https://github.com/hebasto",
        "followers_url": "https://api.github.com/users/hebasto/followers",
        "following_url": "https://api.github.com/users/hebasto/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/hebasto/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/hebasto/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/hebasto/subscriptions",
        "organizations_url": "https://api.github.com/users/hebasto/orgs",
        "repos_url": "https://api.github.com/users/hebasto/repos",
        "events_url": "https://api.github.com/users/hebasto/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/hebasto/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "nit: Could an implicit `bool` to `int` conversion be avoided?",
      "created_at": "2022-08-24T11:45:31Z",
      "updated_at": "2022-08-24T11:45:46Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953690718",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953690718"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 275,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953741265",
      "pull_request_review_id": 1083725409,
      "id": 953741265,
      "node_id": "PRRC_kwDOABII58442O_R",
      "diff_hunk": "@@ -581,18 +588,59 @@ class PeerManagerImpl final : public PeerManager\n \n     void ProcessOrphanTx(std::set<uint256>& orphan_work_set) EXCLUSIVE_LOCKS_REQUIRED(cs_main, g_cs_orphans)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n-    /** Process a single headers message from a peer. */\n+    /** Process a single headers message from a peer.\n+     *\n+     * @param[in]   pfrom     CNode of the peer\n+     * @param[in]   peer      The peer sending us the headers\n+     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.",
      "path": "src/net_processing.cpp",
      "position": 54,
      "original_position": 30,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212: `[in,out]`?",
      "created_at": "2022-08-24T12:35:35Z",
      "updated_at": "2022-08-24T19:36:23Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953741265",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953741265"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 598,
      "original_line": 598,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953760195",
      "pull_request_review_id": 1083725409,
      "id": 953760195,
      "node_id": "PRRC_kwDOABII58442TnD",
      "diff_hunk": "@@ -581,18 +588,59 @@ class PeerManagerImpl final : public PeerManager\n \n     void ProcessOrphanTx(std::set<uint256>& orphan_work_set) EXCLUSIVE_LOCKS_REQUIRED(cs_main, g_cs_orphans)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n-    /** Process a single headers message from a peer. */\n+    /** Process a single headers message from a peer.\n+     *\n+     * @param[in]   pfrom     CNode of the peer\n+     * @param[in]   peer      The peer sending us the headers\n+     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.\n+     * @param[in]   via_compact_block   Whether this header came in via compact block handling.\n+    */\n     void ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n-                               const std::vector<CBlockHeader>& headers,\n+                               std::vector<CBlockHeader>& headers,\n                                bool via_compact_block)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n     /** Various helpers for headers processing, invoked by ProcessHeadersMessage() */\n+    /** Return true if headers are continuous and have valid proof-of-work */\n+    bool CheckHeadersPoW(const std::vector<CBlockHeader>& headers, const Consensus::Params& consensusParams, Peer& peer);\n+    /** Calculate an anti-DoS work threshold for headers chains */\n+    arith_uint256 GetAntiDoSWorkThreshold();\n     /** Deal with state tracking and headers sync for peers that send the\n      * occasional non-connecting header (this can happen due to BIP 130 headers\n      * announcements for blocks interacting with the 2hr (MAX_FUTURE_BLOCK_TIME) rule). */\n     void HandleFewUnconnectingHeaders(CNode& pfrom, Peer& peer, const std::vector<CBlockHeader>& headers);\n     /** Return true if the headers connect to each other, false otherwise */\n     bool CheckHeadersAreContinuous(const std::vector<CBlockHeader>& headers) const;\n+    /** Try to continue a low-work headers sync that has already begun.",
      "path": "src/net_processing.cpp",
      "position": 74,
      "original_position": 49,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212 Given that this calls `ProcessNextHeaders()` we should probably repeat the requirement for that function:\r\n\r\n`Assumes the caller has already verified the headers connect, and has checked that each header satisfies proof-of-work target included in the header`",
      "created_at": "2022-08-24T12:50:04Z",
      "updated_at": "2022-08-24T19:36:23Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953760195",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953760195"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 616,
      "original_line": 616,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953764078",
      "pull_request_review_id": 1083725409,
      "id": 953764078,
      "node_id": "PRRC_kwDOABII58442Uju",
      "diff_hunk": "@@ -581,18 +588,59 @@ class PeerManagerImpl final : public PeerManager\n \n     void ProcessOrphanTx(std::set<uint256>& orphan_work_set) EXCLUSIVE_LOCKS_REQUIRED(cs_main, g_cs_orphans)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n-    /** Process a single headers message from a peer. */\n+    /** Process a single headers message from a peer.\n+     *\n+     * @param[in]   pfrom     CNode of the peer\n+     * @param[in]   peer      The peer sending us the headers\n+     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.\n+     * @param[in]   via_compact_block   Whether this header came in via compact block handling.\n+    */\n     void ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n-                               const std::vector<CBlockHeader>& headers,\n+                               std::vector<CBlockHeader>& headers,\n                                bool via_compact_block)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n     /** Various helpers for headers processing, invoked by ProcessHeadersMessage() */\n+    /** Return true if headers are continuous and have valid proof-of-work */\n+    bool CheckHeadersPoW(const std::vector<CBlockHeader>& headers, const Consensus::Params& consensusParams, Peer& peer);\n+    /** Calculate an anti-DoS work threshold for headers chains */\n+    arith_uint256 GetAntiDoSWorkThreshold();\n     /** Deal with state tracking and headers sync for peers that send the\n      * occasional non-connecting header (this can happen due to BIP 130 headers\n      * announcements for blocks interacting with the 2hr (MAX_FUTURE_BLOCK_TIME) rule). */\n     void HandleFewUnconnectingHeaders(CNode& pfrom, Peer& peer, const std::vector<CBlockHeader>& headers);\n     /** Return true if the headers connect to each other, false otherwise */\n     bool CheckHeadersAreContinuous(const std::vector<CBlockHeader>& headers) const;\n+    /** Try to continue a low-work headers sync that has already begun.\n+     *  @param[in]  peer                            The peer we're syncing with.\n+     *  @param[in]  pfrom                           CNode of the peer\n+     *  @param[in,out] headers                      The headers to be processed.\n+     *  @return     True if the passed in headers were successfully processed; false otherwise.",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 53,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212 to avoid using the ambiguous term \"processed\" twice, maybe replace \"were successfully processed\" with \"passed the checks in ValidateAndStoreHeadersCommitments\" or \"connect and their difficulty adjustment is within bounds\".",
      "created_at": "2022-08-24T12:53:44Z",
      "updated_at": "2022-08-24T19:36:23Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953764078",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953764078"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 617,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953766769",
      "pull_request_review_id": 1083725409,
      "id": 953766769,
      "node_id": "PRRC_kwDOABII58442VNx",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** PRESYNC means the peer has not yet demonstrated their chain has\n+         * sufficient work and we're only building commitments to the chain they\n+         * serve us. */\n+        PRESYNC,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Construct a HeadersSyncState object representing a headers sync via this\n+     *  download-twice mechanism).\n+     *\n+     * id: node id (for logging)\n+     * consensus_params: parameters needed for difficulty adjustment validation\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+            const CBlockIndex* chain_start, const arith_uint256& minimum_required_work);\n+\n+    /** Result data structure for ProcessNextHeaders. */\n+    struct ProcessingResult {\n+        std::vector<CBlockHeader> pow_validated_headers;\n+        bool success{false};\n+        bool request_more{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * received_headers: headers that were received over the network for processing.\n+     *                   Assumes the caller has already verified the headers connect,",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 142,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212 `ProcessNextHeaders` calls `ValidateAndStoreHeadersCommitments` which checks that headers connect. So does it really assume it?",
      "created_at": "2022-08-24T12:56:15Z",
      "updated_at": "2022-08-24T19:36:23Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953766769",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953766769"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 151,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953776691",
      "pull_request_review_id": 1083775678,
      "id": 953776691,
      "node_id": "PRRC_kwDOABII58442Xoz",
      "diff_hunk": "@@ -105,13 +105,13 @@ class ClientModel : public QObject\n     //! A thread to interact with m_node asynchronously\n     QThread* const m_thread;\n \n-    void TipChanged(SynchronizationState sync_state, interfaces::BlockTip tip, double verification_progress, bool header) EXCLUSIVE_LOCKS_REQUIRED(!m_cached_tip_mutex);\n+    void TipChanged(SynchronizationState sync_state, interfaces::BlockTip tip, double verification_progress, int header) EXCLUSIVE_LOCKS_REQUIRED(!m_cached_tip_mutex);",
      "path": "src/qt/clientmodel.h",
      "position": null,
      "original_position": 5,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "bbef38fdac90a03020ad70cbe6b97fbf09fccbc6",
      "in_reply_to_id": null,
      "user": {
        "login": "hebasto",
        "id": 32963518,
        "node_id": "MDQ6VXNlcjMyOTYzNTE4",
        "avatar_url": "https://avatars.githubusercontent.com/u/32963518?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/hebasto",
        "html_url": "https://github.com/hebasto",
        "followers_url": "https://api.github.com/users/hebasto/followers",
        "following_url": "https://api.github.com/users/hebasto/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/hebasto/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/hebasto/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/hebasto/subscriptions",
        "organizations_url": "https://api.github.com/users/hebasto/orgs",
        "repos_url": "https://api.github.com/users/hebasto/repos",
        "events_url": "https://api.github.com/users/hebasto/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/hebasto/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "As a follow up, instead of this function the following two ones could be introduced:\r\n- `BlockTipChanged(sync_state, tip, verification_progress)`\r\n- `HeaderTipChanged(sync_state, tip, presync)`\r\n\r\nImplementation each of them will be much simpler than the current `TipChanged`'s implementation.",
      "created_at": "2022-08-24T13:05:28Z",
      "updated_at": "2022-08-24T13:05:28Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953776691",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953776691"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 108,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953811246",
      "pull_request_review_id": 1083725409,
      "id": 953811246,
      "node_id": "PRRC_kwDOABII58442gEu",
      "diff_hunk": "@@ -2316,6 +2393,104 @@ bool PeerManagerImpl::CheckHeadersAreContinuous(const std::vector<CBlockHeader>&\n     return true;\n }\n \n+bool PeerManagerImpl::IsContinuationOfLowWorkHeadersSync(Peer& peer, CNode& pfrom, std::vector<CBlockHeader>& headers)\n+{\n+    if (peer.m_headers_sync) {\n+        auto result = peer.m_headers_sync->ProcessNextHeaders(headers, headers.size() == MAX_HEADERS_RESULTS);\n+        if (result.request_more) {\n+            auto locator = peer.m_headers_sync->MakeNextHeadersRequest();\n+            // If we were instructed to ask for a locator, it should not be empty.\n+            Assume(!locator.vHave.empty());\n+            if (!locator.vHave.empty()) {\n+                // It should be impossible for the getheaders request to fail,\n+                // because we should have cleared the last getheaders timestamp\n+                // when processing the headers that triggered this call. But\n+                // it may be possible to bypass this via compactblock\n+                // processing, so check the result before logging just to be\n+                // safe.\n+                bool sent_getheaders = MaybeSendGetHeaders(pfrom, locator, peer);\n+                if (sent_getheaders) {\n+                    LogPrint(BCLog::NET, \"more getheaders (from %s) to peer=%d\\n\",\n+                            locator.vHave.front().ToString(), pfrom.GetId());\n+                }",
      "path": "src/net_processing.cpp",
      "position": 269,
      "original_position": 142,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212  probably worth logging if this _does_ go wrong.",
      "created_at": "2022-08-24T13:35:06Z",
      "updated_at": "2022-08-24T19:36:23Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953811246",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953811246"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2467,
      "original_line": 2467,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953815564",
      "pull_request_review_id": 1083725409,
      "id": 953815564,
      "node_id": "PRRC_kwDOABII58442hIM",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** PRESYNC means the peer has not yet demonstrated their chain has\n+         * sufficient work and we're only building commitments to the chain they\n+         * serve us. */\n+        PRESYNC,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Construct a HeadersSyncState object representing a headers sync via this\n+     *  download-twice mechanism).\n+     *\n+     * id: node id (for logging)\n+     * consensus_params: parameters needed for difficulty adjustment validation\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+            const CBlockIndex* chain_start, const arith_uint256& minimum_required_work);\n+\n+    /** Result data structure for ProcessNextHeaders. */\n+    struct ProcessingResult {\n+        std::vector<CBlockHeader> pow_validated_headers;\n+        bool success{false};\n+        bool request_more{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * received_headers: headers that were received over the network for processing.\n+     *                   Assumes the caller has already verified the headers connect,\n+     *                   and has checked that each header satisfies the\n+     *                   proof-of-work target included in the header (but not\n+     *                   necessarily verified that the proof-of-work target is\n+     *                   correct and passes consensus rules).\n+     * full_headers_message: true if the message was at max capacity,\n+     *                       indicating more headers may be available\n+     * ProcessingResult.pow_validated_headers: will be filled in with any\n+     *                       headers that the caller can fully process and\n+     *                       validate now (because these returned headers are\n+     *                       on a chain with sufficient work)\n+     * ProcessingResult.success: set to false if an error is detected and the sync is\n+     *                       aborted; true otherwise.\n+     * ProcessingResult.request_more: if true, the caller is suggested to call\n+     *                       MakeNextHeadersRequest and send a getheaders message using it.\n+     */\n+    ProcessingResult ProcessNextHeaders(const std::vector<CBlockHeader>&\n+            received_headers, bool full_headers_message);\n+\n+    /** Issue the next GETHEADERS message to our peer.\n+     *\n+     * This will return a locator appropriate for the current sync object, to continue the\n+     * synchronization phase it is in.\n+     */\n+    CBlockLocator MakeNextHeadersRequest() const;",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 166,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212: `NextHeadersRequestLocator` might be a better name",
      "created_at": "2022-08-24T13:38:43Z",
      "updated_at": "2022-08-24T19:36:23Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953815564",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953815564"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 166,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953820788",
      "pull_request_review_id": 1083725409,
      "id": 953820788,
      "node_id": "PRRC_kwDOABII58442iZ0",
      "diff_hunk": "@@ -581,18 +588,59 @@ class PeerManagerImpl final : public PeerManager\n \n     void ProcessOrphanTx(std::set<uint256>& orphan_work_set) EXCLUSIVE_LOCKS_REQUIRED(cs_main, g_cs_orphans)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n-    /** Process a single headers message from a peer. */\n+    /** Process a single headers message from a peer.\n+     *\n+     * @param[in]   pfrom     CNode of the peer\n+     * @param[in]   peer      The peer sending us the headers\n+     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.\n+     * @param[in]   via_compact_block   Whether this header came in via compact block handling.\n+    */\n     void ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n-                               const std::vector<CBlockHeader>& headers,\n+                               std::vector<CBlockHeader>& headers,\n                                bool via_compact_block)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n     /** Various helpers for headers processing, invoked by ProcessHeadersMessage() */\n+    /** Return true if headers are continuous and have valid proof-of-work */\n+    bool CheckHeadersPoW(const std::vector<CBlockHeader>& headers, const Consensus::Params& consensusParams, Peer& peer);\n+    /** Calculate an anti-DoS work threshold for headers chains */\n+    arith_uint256 GetAntiDoSWorkThreshold();\n     /** Deal with state tracking and headers sync for peers that send the\n      * occasional non-connecting header (this can happen due to BIP 130 headers\n      * announcements for blocks interacting with the 2hr (MAX_FUTURE_BLOCK_TIME) rule). */\n     void HandleFewUnconnectingHeaders(CNode& pfrom, Peer& peer, const std::vector<CBlockHeader>& headers);\n     /** Return true if the headers connect to each other, false otherwise */\n     bool CheckHeadersAreContinuous(const std::vector<CBlockHeader>& headers) const;\n+    /** Try to continue a low-work headers sync that has already begun.\n+     *  @param[in]  peer                            The peer we're syncing with.\n+     *  @param[in]  pfrom                           CNode of the peer\n+     *  @param[in,out] headers                      The headers to be processed.",
      "path": "src/net_processing.cpp",
      "position": 80,
      "original_position": 52,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212: is there any downside to just having a `const headers_in` and a separate `headers_out` param? Inside `IsContinuationOfLowWorkHeadersSync` you're not editing `headers` in place anyway, using `swap` instead.",
      "created_at": "2022-08-24T13:43:00Z",
      "updated_at": "2022-08-24T19:36:23Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953820788",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953820788"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 622,
      "original_line": 622,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953833317",
      "pull_request_review_id": 1083725409,
      "id": 953833317,
      "node_id": "PRRC_kwDOABII58442ldl",
      "diff_hunk": "@@ -2461,21 +2636,65 @@ void PeerManagerImpl::UpdatePeerStateForReceivedHeaders(CNode& pfrom,\n }\n \n void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n-                                            const std::vector<CBlockHeader>& headers,\n+                                            std::vector<CBlockHeader>& headers,\n                                             bool via_compact_block)\n {\n-    const CNetMsgMaker msgMaker(pfrom.GetCommonVersion());\n     size_t nCount = headers.size();\n \n     if (nCount == 0) {\n         // Nothing interesting. Stop asking this peers for more headers.\n+        // If we were in the middle of headers sync, receiving an empty headers\n+        // message suggests that the peer suddenly has nothing to give us\n+        // (perhaps it reorged to our chain). Clear download state for this peer.\n+        LOCK(peer.m_headers_sync_mutex);\n+        if (peer.m_headers_sync) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }\n+        return;\n+    }\n+\n+    // Before we do any processing, make sure these pass basic sanity checks.\n+    // We'll rely on headers having valid proof-of-work further down, as an\n+    // anti-DoS criteria (note: this check is required before passing any\n+    // headers/ into HeadersSyncState).",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 250,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212  nit: spurious `/`",
      "created_at": "2022-08-24T13:53:11Z",
      "updated_at": "2022-08-24T19:36:23Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953833317",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953833317"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 2659,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953853582",
      "pull_request_review_id": 1083725409,
      "id": 953853582,
      "node_id": "PRRC_kwDOABII58442qaO",
      "diff_hunk": "@@ -2461,21 +2636,65 @@ void PeerManagerImpl::UpdatePeerStateForReceivedHeaders(CNode& pfrom,\n }\n \n void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n-                                            const std::vector<CBlockHeader>& headers,\n+                                            std::vector<CBlockHeader>& headers,\n                                             bool via_compact_block)\n {\n-    const CNetMsgMaker msgMaker(pfrom.GetCommonVersion());\n     size_t nCount = headers.size();\n \n     if (nCount == 0) {\n         // Nothing interesting. Stop asking this peers for more headers.\n+        // If we were in the middle of headers sync, receiving an empty headers\n+        // message suggests that the peer suddenly has nothing to give us\n+        // (perhaps it reorged to our chain). Clear download state for this peer.\n+        LOCK(peer.m_headers_sync_mutex);\n+        if (peer.m_headers_sync) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }\n+        return;\n+    }\n+\n+    // Before we do any processing, make sure these pass basic sanity checks.\n+    // We'll rely on headers having valid proof-of-work further down, as an\n+    // anti-DoS criteria (note: this check is required before passing any\n+    // headers/ into HeadersSyncState).\n+    if (!CheckHeadersPoW(headers, m_chainparams.GetConsensus(), peer)) {\n+        Misbehaving(peer, 100, \"invalid proof-of-work\");\n         return;\n     }\n \n     const CBlockIndex *pindexLast = nullptr;\n \n+    // We'll set already_validated_work to true if the headers-sync logic\n+    // returns headers for us to process, to bypass the minimum work check\n+    // (which is done separately inside m_headers_sync)\n+    bool already_validated_work = false;\n+\n+    // If we're in the middle of headers sync, let it do its magic.\n+    bool have_headers_sync = false;\n+    {\n+        LOCK(peer.m_headers_sync_mutex);\n+\n+        already_validated_work = IsContinuationOfLowWorkHeadersSync(peer, pfrom, headers);",
      "path": "src/net_processing.cpp",
      "position": 441,
      "original_position": 268,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212  This can be `true` if we're still still in the `PRESYNC` state, even though we haven't checked against the minimum work yet. In that case this function returns early, so it's fine. But a clarifying comment would be good here.",
      "created_at": "2022-08-24T14:09:42Z",
      "updated_at": "2022-08-24T19:36:23Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953853582",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953853582"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2775,
      "original_line": 2775,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953858402",
      "pull_request_review_id": 1083725409,
      "id": 953858402,
      "node_id": "PRRC_kwDOABII58442rli",
      "diff_hunk": "@@ -2461,21 +2636,65 @@ void PeerManagerImpl::UpdatePeerStateForReceivedHeaders(CNode& pfrom,\n }\n \n void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n-                                            const std::vector<CBlockHeader>& headers,\n+                                            std::vector<CBlockHeader>& headers,\n                                             bool via_compact_block)\n {\n-    const CNetMsgMaker msgMaker(pfrom.GetCommonVersion());\n     size_t nCount = headers.size();\n \n     if (nCount == 0) {\n         // Nothing interesting. Stop asking this peers for more headers.\n+        // If we were in the middle of headers sync, receiving an empty headers\n+        // message suggests that the peer suddenly has nothing to give us\n+        // (perhaps it reorged to our chain). Clear download state for this peer.\n+        LOCK(peer.m_headers_sync_mutex);\n+        if (peer.m_headers_sync) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }\n+        return;\n+    }\n+\n+    // Before we do any processing, make sure these pass basic sanity checks.\n+    // We'll rely on headers having valid proof-of-work further down, as an\n+    // anti-DoS criteria (note: this check is required before passing any\n+    // headers/ into HeadersSyncState).\n+    if (!CheckHeadersPoW(headers, m_chainparams.GetConsensus(), peer)) {\n+        Misbehaving(peer, 100, \"invalid proof-of-work\");\n         return;\n     }\n \n     const CBlockIndex *pindexLast = nullptr;\n \n+    // We'll set already_validated_work to true if the headers-sync logic\n+    // returns headers for us to process, to bypass the minimum work check",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 259,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212  Maybe say \"minimum total work check\", so it's not confused with a minimal check on the work.",
      "created_at": "2022-08-24T14:13:36Z",
      "updated_at": "2022-08-24T19:36:23Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953858402",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953858402"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 2668,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953875491",
      "pull_request_review_id": 1083920605,
      "id": 953875491,
      "node_id": "PRRC_kwDOABII58442vwj",
      "diff_hunk": "@@ -215,13 +215,17 @@ QString ClientModel::blocksDir() const\n     return GUIUtil::PathToQString(gArgs.GetBlocksDirPath());\n }\n \n-void ClientModel::TipChanged(SynchronizationState sync_state, interfaces::BlockTip tip, double verification_progress, bool header)\n+void ClientModel::TipChanged(SynchronizationState sync_state, interfaces::BlockTip tip, double verification_progress, int header)",
      "path": "src/qt/clientmodel.cpp",
      "position": null,
      "original_position": 5,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f4d912a8e4d93f427101577f9fc7ec0476bc01d6",
      "in_reply_to_id": 952573960,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "@hebasto Here is a commit that uses an enum instead of an int for the TipChanged signal, which compiles, but at runtime I get a `GUI: QObject::connect: Cannot queue arguments of type 'SyncType'` error: https://github.com/sipa/bitcoin/commits/202208_headerssync_log",
      "created_at": "2022-08-24T14:27:36Z",
      "updated_at": "2022-08-24T14:27:36Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953875491",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953875491"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 218,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953878835",
      "pull_request_review_id": 1083725409,
      "id": 953878835,
      "node_id": "PRRC_kwDOABII58442wkz",
      "diff_hunk": "@@ -2461,21 +2636,65 @@ void PeerManagerImpl::UpdatePeerStateForReceivedHeaders(CNode& pfrom,\n }\n \n void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n-                                            const std::vector<CBlockHeader>& headers,\n+                                            std::vector<CBlockHeader>& headers,\n                                             bool via_compact_block)\n {\n-    const CNetMsgMaker msgMaker(pfrom.GetCommonVersion());\n     size_t nCount = headers.size();\n \n     if (nCount == 0) {\n         // Nothing interesting. Stop asking this peers for more headers.\n+        // If we were in the middle of headers sync, receiving an empty headers\n+        // message suggests that the peer suddenly has nothing to give us\n+        // (perhaps it reorged to our chain). Clear download state for this peer.\n+        LOCK(peer.m_headers_sync_mutex);\n+        if (peer.m_headers_sync) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }\n+        return;\n+    }\n+\n+    // Before we do any processing, make sure these pass basic sanity checks.\n+    // We'll rely on headers having valid proof-of-work further down, as an\n+    // anti-DoS criteria (note: this check is required before passing any\n+    // headers/ into HeadersSyncState).\n+    if (!CheckHeadersPoW(headers, m_chainparams.GetConsensus(), peer)) {\n+        Misbehaving(peer, 100, \"invalid proof-of-work\");",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 252,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212  maybe add a note that, even if `via_compact_block` is set, BIP 152 requires a valid header.",
      "created_at": "2022-08-24T14:30:25Z",
      "updated_at": "2022-08-24T19:36:23Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953878835",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953878835"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 2757,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953884672",
      "pull_request_review_id": 1083725409,
      "id": 953884672,
      "node_id": "PRRC_kwDOABII58442yAA",
      "diff_hunk": "@@ -3913,7 +4154,8 @@ void PeerManagerImpl::ProcessMessage(CNode& pfrom, const std::string& msg_type,\n             // the peer if the header turns out to be for an invalid block.\n             // Note that if a peer tries to build on an invalid chain, that\n             // will be detected and the peer will be disconnected/discouraged.\n-            return ProcessHeadersMessage(pfrom, *peer, {cmpctblock.header}, /*via_compact_block=*/true);\n+            std::vector<CBlockHeader> dummy{cmpctblock.header};",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 351,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212: it's not really a dummy is it? E.g. the proof of work gets checked by `ProcessHeadersMessage()`",
      "created_at": "2022-08-24T14:35:24Z",
      "updated_at": "2022-08-24T19:36:23Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953884672",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953884672"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 4157,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953890377",
      "pull_request_review_id": 1083942586,
      "id": 953890377,
      "node_id": "PRRC_kwDOABII58442zZJ",
      "diff_hunk": "@@ -215,13 +215,17 @@ QString ClientModel::blocksDir() const\n     return GUIUtil::PathToQString(gArgs.GetBlocksDirPath());\n }\n \n-void ClientModel::TipChanged(SynchronizationState sync_state, interfaces::BlockTip tip, double verification_progress, bool header)\n+void ClientModel::TipChanged(SynchronizationState sync_state, interfaces::BlockTip tip, double verification_progress, int header)",
      "path": "src/qt/clientmodel.cpp",
      "position": null,
      "original_position": 5,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f4d912a8e4d93f427101577f9fc7ec0476bc01d6",
      "in_reply_to_id": 952573960,
      "user": {
        "login": "hebasto",
        "id": 32963518,
        "node_id": "MDQ6VXNlcjMyOTYzNTE4",
        "avatar_url": "https://avatars.githubusercontent.com/u/32963518?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/hebasto",
        "html_url": "https://github.com/hebasto",
        "followers_url": "https://api.github.com/users/hebasto/followers",
        "following_url": "https://api.github.com/users/hebasto/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/hebasto/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/hebasto/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/hebasto/subscriptions",
        "organizations_url": "https://api.github.com/users/hebasto/orgs",
        "repos_url": "https://api.github.com/users/hebasto/repos",
        "events_url": "https://api.github.com/users/hebasto/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/hebasto/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "@sipa\r\nSuggesting the following diff to your branch:\r\n```diff\r\n--- a/src/qt/bitcoin.cpp\r\n+++ b/src/qt/bitcoin.cpp\r\n@@ -75,6 +75,7 @@ Q_IMPORT_PLUGIN(QAndroidPlatformIntegrationPlugin)\r\n Q_DECLARE_METATYPE(bool*)\r\n Q_DECLARE_METATYPE(CAmount)\r\n Q_DECLARE_METATYPE(SynchronizationState)\r\n+Q_DECLARE_METATYPE(SyncType)\r\n Q_DECLARE_METATYPE(uint256)\r\n \r\n static void RegisterMetaTypes()\r\n@@ -82,6 +83,7 @@ static void RegisterMetaTypes()\r\n     // Register meta types used for QMetaObject::invokeMethod and Qt::QueuedConnection\r\n     qRegisterMetaType<bool*>();\r\n     qRegisterMetaType<SynchronizationState>();\r\n+    qRegisterMetaType<SyncType>();\r\n   #ifdef ENABLE_WALLET\r\n     qRegisterMetaType<WalletModel*>();\r\n   #endif\r\n```",
      "created_at": "2022-08-24T14:40:13Z",
      "updated_at": "2022-08-24T14:40:14Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953890377",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953890377"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 218,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953904748",
      "pull_request_review_id": 1083963412,
      "id": 953904748,
      "node_id": "PRRC_kwDOABII5844225s",
      "diff_hunk": "@@ -215,13 +215,17 @@ QString ClientModel::blocksDir() const\n     return GUIUtil::PathToQString(gArgs.GetBlocksDirPath());\n }\n \n-void ClientModel::TipChanged(SynchronizationState sync_state, interfaces::BlockTip tip, double verification_progress, bool header)\n+void ClientModel::TipChanged(SynchronizationState sync_state, interfaces::BlockTip tip, double verification_progress, int header)",
      "path": "src/qt/clientmodel.cpp",
      "position": null,
      "original_position": 5,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f4d912a8e4d93f427101577f9fc7ec0476bc01d6",
      "in_reply_to_id": 952573960,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "@hebasto Ah, thanks, that works!",
      "created_at": "2022-08-24T14:51:59Z",
      "updated_at": "2022-08-24T14:51:59Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953904748",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953904748"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 218,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953964777",
      "pull_request_review_id": 1083725409,
      "id": 953964777,
      "node_id": "PRRC_kwDOABII58443Fjp",
      "diff_hunk": "@@ -0,0 +1,316 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 19,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212: so this doesn't need to be a multiple of `HEADER_COMMITMENT_PERIOD`? Maybe mention that it's approximately 24 commitment periods worth of headers, i.e. 24 bits of security (? Or ~9 more because an attacker has to account for every possible offset)",
      "created_at": "2022-08-24T15:39:03Z",
      "updated_at": "2022-08-24T19:36:23Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953964777",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953964777"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 19,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953979307",
      "pull_request_review_id": 1083725409,
      "id": 953979307,
      "node_id": "PRRC_kwDOABII58443JGr",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}",
      "path": "src/headerssync.h",
      "position": 103,
      "original_position": 103,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212 : maybe `Assume(m_download_state == State::FINAL);`",
      "created_at": "2022-08-24T15:50:36Z",
      "updated_at": "2022-08-24T19:36:23Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953979307",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953979307"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 103,
      "original_line": 103,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953981817",
      "pull_request_review_id": 1083725409,
      "id": 953981817,
      "node_id": "PRRC_kwDOABII58443Jt5",
      "diff_hunk": "@@ -0,0 +1,316 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 56,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212  is that much faster than `clear()`?",
      "created_at": "2022-08-24T15:52:57Z",
      "updated_at": "2022-08-24T19:36:23Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953981817",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953981817"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 56,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953995060",
      "pull_request_review_id": 1083725409,
      "id": 953995060,
      "node_id": "PRRC_kwDOABII58443M80",
      "diff_hunk": "@@ -0,0 +1,316 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {",
      "path": "src/headerssync.cpp",
      "position": 90,
      "original_position": 90,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212 : `Assume(m_download_state == State::PRESYNC)`",
      "created_at": "2022-08-24T16:04:26Z",
      "updated_at": "2022-08-24T19:36:23Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r953995060",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/953995060"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 90,
      "original_line": 90,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954198307",
      "pull_request_review_id": 1083725409,
      "id": 954198307,
      "node_id": "PRRC_kwDOABII58443-kj",
      "diff_hunk": "@@ -0,0 +1,316 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();",
      "path": "src/headerssync.cpp",
      "position": 135,
      "original_position": 134,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212: calling `Finalize()` here is safe, because we've already popped the headers we need into `ret`.",
      "created_at": "2022-08-24T19:09:35Z",
      "updated_at": "2022-08-24T19:36:24Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954198307",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954198307"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 135,
      "original_line": 135,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954202598",
      "pull_request_review_id": 1083725409,
      "id": 954202598,
      "node_id": "PRRC_kwDOABII58443_nm",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** PRESYNC means the peer has not yet demonstrated their chain has\n+         * sufficient work and we're only building commitments to the chain they\n+         * serve us. */\n+        PRESYNC,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Construct a HeadersSyncState object representing a headers sync via this\n+     *  download-twice mechanism).\n+     *\n+     * id: node id (for logging)\n+     * consensus_params: parameters needed for difficulty adjustment validation\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+            const CBlockIndex* chain_start, const arith_uint256& minimum_required_work);\n+\n+    /** Result data structure for ProcessNextHeaders. */\n+    struct ProcessingResult {\n+        std::vector<CBlockHeader> pow_validated_headers;\n+        bool success{false};\n+        bool request_more{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * received_headers: headers that were received over the network for processing.\n+     *                   Assumes the caller has already verified the headers connect,\n+     *                   and has checked that each header satisfies the\n+     *                   proof-of-work target included in the header (but not\n+     *                   necessarily verified that the proof-of-work target is\n+     *                   correct and passes consensus rules).\n+     * full_headers_message: true if the message was at max capacity,\n+     *                       indicating more headers may be available\n+     * ProcessingResult.pow_validated_headers: will be filled in with any\n+     *                       headers that the caller can fully process and\n+     *                       validate now (because these returned headers are\n+     *                       on a chain with sufficient work)\n+     * ProcessingResult.success: set to false if an error is detected and the sync is\n+     *                       aborted; true otherwise.\n+     * ProcessingResult.request_more: if true, the caller is suggested to call\n+     *                       MakeNextHeadersRequest and send a getheaders message using it.\n+     */\n+    ProcessingResult ProcessNextHeaders(const std::vector<CBlockHeader>&\n+            received_headers, bool full_headers_message);\n+\n+    /** Issue the next GETHEADERS message to our peer.\n+     *\n+     * This will return a locator appropriate for the current sync object, to continue the\n+     * synchronization phase it is in.\n+     */\n+    CBlockLocator MakeNextHeadersRequest() const;\n+\n+private:\n+    /** Clear out all download state that might be in progress (freeing any used\n+     * memory), and mark this object as no longer usable.\n+     */\n+    void Finalize();\n+\n+    /**\n+     *  Only called in PRESYNC.\n+     *  Validate the work on the headers we received from the network, and\n+     *  store commitments for later. Update overall state with successfully\n+     *  processed headers.\n+     *  On failure, this invokes Finalize() and returns false.\n+     */\n+    bool ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers);\n+\n+    /** In PRESYNC, process and update state for a single header */\n+    bool ValidateAndProcessSingleHeader(const CBlockHeader& previous, const\n+            CBlockHeader& current);\n+\n+    /** In REDOWNLOAD, check a header's commitment (if applicable) and add to\n+     * buffer for later processing */\n+    bool ValidateAndStoreRedownloadedHeader(const CBlockHeader& header);\n+\n+    /** Return a set of headers that satisfy our proof-of-work threshold */\n+    std::vector<CBlockHeader> PopHeadersReadyForAcceptance();\n+\n+private:\n+    /** NodeId of the peer (used for log messages) **/\n+    const NodeId m_id;\n+\n+    /** We use the consensus params in our anti-DoS calculations */\n+    const Consensus::Params& m_consensus_params;\n+\n+    /** Store the last block in our block index that the peer's chain builds from */\n+    const CBlockIndex* m_chain_start{nullptr};\n+\n+    /** Minimum work that we're looking for on this chain. */\n+    const arith_uint256 m_minimum_required_work;\n+\n+    /** Work that we've seen so far on the peer's chain */\n+    arith_uint256 m_current_chain_work;\n+\n+    /** m_hasher is a salted hasher for making our 1-bit commitments to headers we've seen. */\n+    const SaltedTxidHasher m_hasher;\n+\n+    /** A queue of commitment bits, created during the 1st phase, and verified during the 2nd. */\n+    bitdeque<> m_header_commitments;\n+\n+    /** The (secret) offset on the heights for which to create commitments.\n+     *\n+     * m_header_commitments entries are created at any height h for which\n+     * (h % HEADER_COMMITMENT_PERIOD) == m_commit_offset. */\n+    const unsigned m_commit_offset;\n+\n+    /** m_max_commitments is a bound we calculate on how long an honest peer's chain could be,\n+     * given the MTP rule.\n+     *\n+     * Any peer giving us more headers than this will have its sync aborted. This serves as a\n+     * memory bound on m_header_commitments. */\n+    uint64_t m_max_commitments{0};\n+\n+    /** Store the latest header received while in PRESYNC */",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 229,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212 ` or the hash of m_chain_start`.",
      "created_at": "2022-08-24T19:13:41Z",
      "updated_at": "2022-08-24T19:36:24Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954202598",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954202598"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 229,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954205962",
      "pull_request_review_id": 1083725409,
      "id": 954205962,
      "node_id": "PRRC_kwDOABII58444AcK",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** PRESYNC means the peer has not yet demonstrated their chain has\n+         * sufficient work and we're only building commitments to the chain they\n+         * serve us. */\n+        PRESYNC,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Construct a HeadersSyncState object representing a headers sync via this\n+     *  download-twice mechanism).\n+     *\n+     * id: node id (for logging)\n+     * consensus_params: parameters needed for difficulty adjustment validation\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+            const CBlockIndex* chain_start, const arith_uint256& minimum_required_work);\n+\n+    /** Result data structure for ProcessNextHeaders. */\n+    struct ProcessingResult {\n+        std::vector<CBlockHeader> pow_validated_headers;\n+        bool success{false};\n+        bool request_more{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * received_headers: headers that were received over the network for processing.\n+     *                   Assumes the caller has already verified the headers connect,\n+     *                   and has checked that each header satisfies the\n+     *                   proof-of-work target included in the header (but not\n+     *                   necessarily verified that the proof-of-work target is\n+     *                   correct and passes consensus rules).\n+     * full_headers_message: true if the message was at max capacity,\n+     *                       indicating more headers may be available\n+     * ProcessingResult.pow_validated_headers: will be filled in with any\n+     *                       headers that the caller can fully process and\n+     *                       validate now (because these returned headers are\n+     *                       on a chain with sufficient work)\n+     * ProcessingResult.success: set to false if an error is detected and the sync is\n+     *                       aborted; true otherwise.\n+     * ProcessingResult.request_more: if true, the caller is suggested to call\n+     *                       MakeNextHeadersRequest and send a getheaders message using it.\n+     */\n+    ProcessingResult ProcessNextHeaders(const std::vector<CBlockHeader>&\n+            received_headers, bool full_headers_message);\n+\n+    /** Issue the next GETHEADERS message to our peer.\n+     *\n+     * This will return a locator appropriate for the current sync object, to continue the\n+     * synchronization phase it is in.\n+     */\n+    CBlockLocator MakeNextHeadersRequest() const;\n+\n+private:\n+    /** Clear out all download state that might be in progress (freeing any used\n+     * memory), and mark this object as no longer usable.\n+     */\n+    void Finalize();\n+\n+    /**\n+     *  Only called in PRESYNC.\n+     *  Validate the work on the headers we received from the network, and\n+     *  store commitments for later. Update overall state with successfully\n+     *  processed headers.\n+     *  On failure, this invokes Finalize() and returns false.\n+     */\n+    bool ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers);\n+\n+    /** In PRESYNC, process and update state for a single header */\n+    bool ValidateAndProcessSingleHeader(const CBlockHeader& previous, const",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 184,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212: the `previous` argument is a bit pointless, since it's a member variable.",
      "created_at": "2022-08-24T19:16:25Z",
      "updated_at": "2022-08-24T19:36:24Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954205962",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954205962"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 184,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954208859",
      "pull_request_review_id": 1083725409,
      "id": 954208859,
      "node_id": "PRRC_kwDOABII58444BJb",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** PRESYNC means the peer has not yet demonstrated their chain has\n+         * sufficient work and we're only building commitments to the chain they\n+         * serve us. */\n+        PRESYNC,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Construct a HeadersSyncState object representing a headers sync via this\n+     *  download-twice mechanism).\n+     *\n+     * id: node id (for logging)\n+     * consensus_params: parameters needed for difficulty adjustment validation\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+            const CBlockIndex* chain_start, const arith_uint256& minimum_required_work);\n+\n+    /** Result data structure for ProcessNextHeaders. */\n+    struct ProcessingResult {\n+        std::vector<CBlockHeader> pow_validated_headers;\n+        bool success{false};\n+        bool request_more{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * received_headers: headers that were received over the network for processing.\n+     *                   Assumes the caller has already verified the headers connect,\n+     *                   and has checked that each header satisfies the\n+     *                   proof-of-work target included in the header (but not\n+     *                   necessarily verified that the proof-of-work target is\n+     *                   correct and passes consensus rules).\n+     * full_headers_message: true if the message was at max capacity,\n+     *                       indicating more headers may be available\n+     * ProcessingResult.pow_validated_headers: will be filled in with any\n+     *                       headers that the caller can fully process and\n+     *                       validate now (because these returned headers are\n+     *                       on a chain with sufficient work)\n+     * ProcessingResult.success: set to false if an error is detected and the sync is\n+     *                       aborted; true otherwise.\n+     * ProcessingResult.request_more: if true, the caller is suggested to call\n+     *                       MakeNextHeadersRequest and send a getheaders message using it.\n+     */\n+    ProcessingResult ProcessNextHeaders(const std::vector<CBlockHeader>&\n+            received_headers, bool full_headers_message);\n+\n+    /** Issue the next GETHEADERS message to our peer.\n+     *\n+     * This will return a locator appropriate for the current sync object, to continue the\n+     * synchronization phase it is in.\n+     */\n+    CBlockLocator MakeNextHeadersRequest() const;\n+\n+private:\n+    /** Clear out all download state that might be in progress (freeing any used\n+     * memory), and mark this object as no longer usable.\n+     */\n+    void Finalize();\n+\n+    /**\n+     *  Only called in PRESYNC.\n+     *  Validate the work on the headers we received from the network, and\n+     *  store commitments for later. Update overall state with successfully\n+     *  processed headers.\n+     *  On failure, this invokes Finalize() and returns false.\n+     */\n+    bool ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers);\n+\n+    /** In PRESYNC, process and update state for a single header */\n+    bool ValidateAndProcessSingleHeader(const CBlockHeader& previous, const\n+            CBlockHeader& current);\n+\n+    /** In REDOWNLOAD, check a header's commitment (if applicable) and add to\n+     * buffer for later processing */\n+    bool ValidateAndStoreRedownloadedHeader(const CBlockHeader& header);\n+\n+    /** Return a set of headers that satisfy our proof-of-work threshold */\n+    std::vector<CBlockHeader> PopHeadersReadyForAcceptance();\n+\n+private:\n+    /** NodeId of the peer (used for log messages) **/\n+    const NodeId m_id;\n+\n+    /** We use the consensus params in our anti-DoS calculations */\n+    const Consensus::Params& m_consensus_params;\n+\n+    /** Store the last block in our block index that the peer's chain builds from */\n+    const CBlockIndex* m_chain_start{nullptr};\n+\n+    /** Minimum work that we're looking for on this chain. */\n+    const arith_uint256 m_minimum_required_work;\n+\n+    /** Work that we've seen so far on the peer's chain */\n+    arith_uint256 m_current_chain_work;\n+\n+    /** m_hasher is a salted hasher for making our 1-bit commitments to headers we've seen. */\n+    const SaltedTxidHasher m_hasher;\n+\n+    /** A queue of commitment bits, created during the 1st phase, and verified during the 2nd. */\n+    bitdeque<> m_header_commitments;\n+\n+    /** The (secret) offset on the heights for which to create commitments.\n+     *\n+     * m_header_commitments entries are created at any height h for which\n+     * (h % HEADER_COMMITMENT_PERIOD) == m_commit_offset. */\n+    const unsigned m_commit_offset;\n+\n+    /** m_max_commitments is a bound we calculate on how long an honest peer's chain could be,\n+     * given the MTP rule.\n+     *\n+     * Any peer giving us more headers than this will have its sync aborted. This serves as a\n+     * memory bound on m_header_commitments. */\n+    uint64_t m_max_commitments{0};\n+\n+    /** Store the latest header received while in PRESYNC */\n+    CBlockHeader m_last_header_received;\n+\n+    /** Height of m_last_header_received */\n+    int64_t m_current_height{0};\n+\n+    /** During phase 2 (REDOWNLOAD), we buffer redownloaded headers in memory\n+     *  until enough commitments have been verified; those are stored in\n+     *  m_redownloaded_headers */\n+    std::deque<CompressedHeader> m_redownloaded_headers;\n+\n+    /** Height of last header in m_redownloaded_headers */\n+    int64_t m_redownload_buffer_last_height{0};\n+\n+    /** Hash of last header in m_redownloaded_headers (we have to cache it",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 243,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212 ` or the hash of m_chain_start`.",
      "created_at": "2022-08-24T19:19:09Z",
      "updated_at": "2022-08-24T19:36:24Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954208859",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954208859"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 243,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954213541",
      "pull_request_review_id": 1083725409,
      "id": 954213541,
      "node_id": "PRRC_kwDOABII58444CSl",
      "diff_hunk": "@@ -0,0 +1,316 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps our peer reorged away from the chain\n+        // they were on. Give up on this sync for now (likely we will start a\n+        // new sync with a new starting point).\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (presync phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and occasionally store\n+    // commitments.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current)\n+{\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (presync phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    if (next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        // Add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // It's possible the chain grew since we started the sync; so\n+            // potentially we could succeed in syncing the peer's chain if we",
      "path": "src/headerssync.cpp",
      "position": 201,
      "original_position": 200,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212: in the case of IBD this is just a remote theoretical possibility, right? But for a shorter catchup there's at least a less-remote chance someone produced lots of blocks with timestamps progressing only 1 second per 6 blocks, e.g. as part of an going timewarp attack.",
      "created_at": "2022-08-24T19:23:31Z",
      "updated_at": "2022-08-24T19:36:24Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954213541",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954213541"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 201,
      "original_line": 201,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954217787",
      "pull_request_review_id": 1083725409,
      "id": 954217787,
      "node_id": "PRRC_kwDOABII58444DU7",
      "diff_hunk": "@@ -0,0 +1,316 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps our peer reorged away from the chain\n+        // they were on. Give up on this sync for now (likely we will start a\n+        // new sync with a new starting point).\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (presync phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and occasionally store\n+    // commitments.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current)\n+{\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (presync phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    if (next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        // Add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // It's possible the chain grew since we started the sync; so\n+            // potentially we could succeed in syncing the peer's chain if we\n+            // try again later.\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: exceeded max commitments at height=%i (presync phase)\\n\", m_id, next_height);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));\n+    m_last_header_received = current;\n+    m_current_height = next_height;\n+\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreRedownloadedHeader(const CBlockHeader& header)\n+{\n+    Assume(m_download_state == State::REDOWNLOAD);\n+    if (m_download_state != State::REDOWNLOAD) return false;\n+\n+    int64_t next_height = m_redownload_buffer_last_height + 1;\n+\n+    // Ensure that we're working on a header that connects to the chain we're\n+    // downloading.\n+    if (header.hashPrevBlock != m_redownload_buffer_last_hash) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (redownload phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    // Check that the difficulty adjustments are within our tolerance:\n+    uint32_t previous_nBits{0};\n+    if (!m_redownloaded_headers.empty()) {\n+        previous_nBits = m_redownloaded_headers.back().nBits;\n+    } else {\n+        previous_nBits = m_chain_start->nBits;\n+    }\n+\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous_nBits, header.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (redownload phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    // Track work on the redownloaded chain\n+    m_redownload_chain_work += GetBlockProof(CBlockIndex(header));\n+\n+    if (m_redownload_chain_work >= m_minimum_required_work) {\n+        m_process_all_remaining_headers = true;\n+    }\n+\n+    // If we're at a header for which we previously stored a commitment, verify\n+    // it is correct. Failure will result in aborting download.\n+    // Also, don't check commitments once we've gotten to our target blockhash;\n+    // it's possible our peer has extended its chain between our first sync and\n+    // our second, and we don't want to return failure after we've seen our\n+    // target blockhash just because we ran out of commitments.\n+    if (!m_process_all_remaining_headers && next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+         bool commitment = m_hasher(header.GetHash()) & 1;",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 256,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "1fca7ed49eca6c2cd71d37415fa2d419779cf212: maybe move this line down so it's right before or after `bool expected_commitment =`",
      "created_at": "2022-08-24T19:27:33Z",
      "updated_at": "2022-08-24T19:36:24Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954217787",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954217787"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 256,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954237990",
      "pull_request_review_id": 1084422024,
      "id": 954237990,
      "node_id": "PRRC_kwDOABII58444IQm",
      "diff_hunk": "@@ -581,18 +588,59 @@ class PeerManagerImpl final : public PeerManager\n \n     void ProcessOrphanTx(std::set<uint256>& orphan_work_set) EXCLUSIVE_LOCKS_REQUIRED(cs_main, g_cs_orphans)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n-    /** Process a single headers message from a peer. */\n+    /** Process a single headers message from a peer.\n+     *\n+     * @param[in]   pfrom     CNode of the peer\n+     * @param[in]   peer      The peer sending us the headers\n+     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.",
      "path": "src/net_processing.cpp",
      "position": 54,
      "original_position": 30,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953741265,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Hmm, interesting point. I think the intention is not to think of the headers in the vector after calling ProcessHeaderMessage as the output of the function, but rather just that the caller shouldn't assume anything about what is in the vector.  So I think it's better to not have the docs say \"[in,out]\", given that understanding?",
      "created_at": "2022-08-24T19:45:54Z",
      "updated_at": "2022-08-24T19:45:54Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954237990",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954237990"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 598,
      "original_line": 598,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954250188",
      "pull_request_review_id": 1084440072,
      "id": 954250188,
      "node_id": "PRRC_kwDOABII58444LPM",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** PRESYNC means the peer has not yet demonstrated their chain has\n+         * sufficient work and we're only building commitments to the chain they\n+         * serve us. */\n+        PRESYNC,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Construct a HeadersSyncState object representing a headers sync via this\n+     *  download-twice mechanism).\n+     *\n+     * id: node id (for logging)\n+     * consensus_params: parameters needed for difficulty adjustment validation\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+            const CBlockIndex* chain_start, const arith_uint256& minimum_required_work);\n+\n+    /** Result data structure for ProcessNextHeaders. */\n+    struct ProcessingResult {\n+        std::vector<CBlockHeader> pow_validated_headers;\n+        bool success{false};\n+        bool request_more{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * received_headers: headers that were received over the network for processing.\n+     *                   Assumes the caller has already verified the headers connect,",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 142,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953766769,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "That's a fair point; I think what I was thinking here is that headers *within a headers message* that don't connect aren't going to be well-handled by this logic, because that is a level of brokenness that we'd want to detect at the `net_processing` level and just disconnect the peer (certainly it's not a type of failure that we would handle by issuing another `getheaders` and hoping for a different response).  \r\n\r\nThe logic does (or should!) correctly deal with a case where the first header in a message doesn't connect with the last header in a prior message, which is not necessarily indicative of a broken peer (it could happen due to the peer reorging, for instance).  In that type of situation, we want to return a failure to the caller that indicates the caller could try again with a new headers sync from a different starting point.\r\n\r\nSo in total, I think the idea that the caller should be checking for this error is still the right idea to get across, but perhaps this comment is slightly misleading.",
      "created_at": "2022-08-24T20:00:33Z",
      "updated_at": "2022-08-24T20:00:33Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954250188",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954250188"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 151,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954261274",
      "pull_request_review_id": 1084456483,
      "id": 954261274,
      "node_id": "PRRC_kwDOABII58444N8a",
      "diff_hunk": "@@ -2461,21 +2636,65 @@ void PeerManagerImpl::UpdatePeerStateForReceivedHeaders(CNode& pfrom,\n }\n \n void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n-                                            const std::vector<CBlockHeader>& headers,\n+                                            std::vector<CBlockHeader>& headers,\n                                             bool via_compact_block)\n {\n-    const CNetMsgMaker msgMaker(pfrom.GetCommonVersion());\n     size_t nCount = headers.size();\n \n     if (nCount == 0) {\n         // Nothing interesting. Stop asking this peers for more headers.\n+        // If we were in the middle of headers sync, receiving an empty headers\n+        // message suggests that the peer suddenly has nothing to give us\n+        // (perhaps it reorged to our chain). Clear download state for this peer.\n+        LOCK(peer.m_headers_sync_mutex);\n+        if (peer.m_headers_sync) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }\n+        return;\n+    }\n+\n+    // Before we do any processing, make sure these pass basic sanity checks.\n+    // We'll rely on headers having valid proof-of-work further down, as an\n+    // anti-DoS criteria (note: this check is required before passing any\n+    // headers/ into HeadersSyncState).\n+    if (!CheckHeadersPoW(headers, m_chainparams.GetConsensus(), peer)) {\n+        Misbehaving(peer, 100, \"invalid proof-of-work\");",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 252,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953878835,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Will add.  Also, you just made me realize that the anti-DoS work calculation to avoid processing compact blocks that are low-work doesn't first verify the proof-of-work on the header before doing the work check.  After giving this more thought, I don't think it's a problem, because if the claimed work is low we ignore it, and if the claimed work is high then we might try to validate it, which would catch any problems -- but wanted to mention it in case I am overlooking something.",
      "created_at": "2022-08-24T20:15:08Z",
      "updated_at": "2022-08-24T20:15:08Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954261274",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954261274"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 2757,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954277672",
      "pull_request_review_id": 1084479808,
      "id": 954277672,
      "node_id": "PRRC_kwDOABII58444R8o",
      "diff_hunk": "@@ -581,18 +588,59 @@ class PeerManagerImpl final : public PeerManager\n \n     void ProcessOrphanTx(std::set<uint256>& orphan_work_set) EXCLUSIVE_LOCKS_REQUIRED(cs_main, g_cs_orphans)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n-    /** Process a single headers message from a peer. */\n+    /** Process a single headers message from a peer.\n+     *\n+     * @param[in]   pfrom     CNode of the peer\n+     * @param[in]   peer      The peer sending us the headers\n+     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.\n+     * @param[in]   via_compact_block   Whether this header came in via compact block handling.\n+    */\n     void ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n-                               const std::vector<CBlockHeader>& headers,\n+                               std::vector<CBlockHeader>& headers,\n                                bool via_compact_block)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n     /** Various helpers for headers processing, invoked by ProcessHeadersMessage() */\n+    /** Return true if headers are continuous and have valid proof-of-work */\n+    bool CheckHeadersPoW(const std::vector<CBlockHeader>& headers, const Consensus::Params& consensusParams, Peer& peer);\n+    /** Calculate an anti-DoS work threshold for headers chains */\n+    arith_uint256 GetAntiDoSWorkThreshold();\n     /** Deal with state tracking and headers sync for peers that send the\n      * occasional non-connecting header (this can happen due to BIP 130 headers\n      * announcements for blocks interacting with the 2hr (MAX_FUTURE_BLOCK_TIME) rule). */\n     void HandleFewUnconnectingHeaders(CNode& pfrom, Peer& peer, const std::vector<CBlockHeader>& headers);\n     /** Return true if the headers connect to each other, false otherwise */\n     bool CheckHeadersAreContinuous(const std::vector<CBlockHeader>& headers) const;\n+    /** Try to continue a low-work headers sync that has already begun.\n+     *  @param[in]  peer                            The peer we're syncing with.\n+     *  @param[in]  pfrom                           CNode of the peer\n+     *  @param[in,out] headers                      The headers to be processed.",
      "path": "src/net_processing.cpp",
      "position": 80,
      "original_position": 52,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953820788,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "This was the result of a couple iterations that I went through with @sipa.  I believe @sipa pointed out to me that we could push the headers manipulation down another level into the headers sync object itself, and then we could save an allocation -- but I was going to save further refinements of this code to a future PR at this point.",
      "created_at": "2022-08-24T20:36:49Z",
      "updated_at": "2022-08-24T20:36:49Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954277672",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954277672"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 622,
      "original_line": 622,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954282286",
      "pull_request_review_id": 1084486341,
      "id": 954282286,
      "node_id": "PRRC_kwDOABII58444TEu",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}",
      "path": "src/headerssync.h",
      "position": 103,
      "original_position": 103,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953979307,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Not necessarily true I think, such as if we are destroying the object early (such as due to peer disconnection, or being in a unit test environment, etc).",
      "created_at": "2022-08-24T20:42:52Z",
      "updated_at": "2022-08-24T20:42:52Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954282286",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954282286"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 103,
      "original_line": 103,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954284583",
      "pull_request_review_id": 1084489767,
      "id": 954284583,
      "node_id": "PRRC_kwDOABII58444Ton",
      "diff_hunk": "@@ -0,0 +1,316 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 56,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953981817,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I believe `clear()` does not actually free memory, but the `swap()` here does.  (May not matter much since the caller will just be deleting us anyway, but seems better to free resources as soon as we can.)",
      "created_at": "2022-08-24T20:45:59Z",
      "updated_at": "2022-08-24T20:45:59Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954284583",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954284583"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 56,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954286388",
      "pull_request_review_id": 1084492265,
      "id": 954286388,
      "node_id": "PRRC_kwDOABII58444UE0",
      "diff_hunk": "@@ -0,0 +1,316 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 56,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953981817,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "This seems based on the (somewhat surprising) `std::vector<T>::clear()` behavior which does not release memory, just sets the size of the used area of it to 0. That doesn't apply here though, because `std::deque` is not an `std::vector` and I don't think it exhibits that behavior.\r\n\r\nAlso, even if it was, I think `m_redownloaded_headers = std::deque<CompressedHeader>{};` is cleaner (since C++11 move semantics that's equivalent to that swap statement, but a bit more readable).",
      "created_at": "2022-08-24T20:48:21Z",
      "updated_at": "2022-08-24T20:48:21Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954286388",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954286388"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 56,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954299829",
      "pull_request_review_id": 1084511531,
      "id": 954299829,
      "node_id": "PRRC_kwDOABII58444XW1",
      "diff_hunk": "@@ -0,0 +1,316 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps our peer reorged away from the chain\n+        // they were on. Give up on this sync for now (likely we will start a\n+        // new sync with a new starting point).\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (presync phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and occasionally store\n+    // commitments.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current)\n+{\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (presync phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    if (next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        // Add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // It's possible the chain grew since we started the sync; so\n+            // potentially we could succeed in syncing the peer's chain if we",
      "path": "src/headerssync.cpp",
      "position": 201,
      "original_position": 200,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 954213541,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think even in that scenario, this code ought to succeed, because we establish the minimum work requirement to leave this logic at the start of a sync, and not as we go.  So if a peer had a high-enough-work chain at the start of our sync, then this calculation should correctly capture the number of valid headers they might have to satisfy that work requirement.",
      "created_at": "2022-08-24T21:06:19Z",
      "updated_at": "2022-08-24T21:06:19Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954299829",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954299829"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 201,
      "original_line": 201,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954305370",
      "pull_request_review_id": 1084519113,
      "id": 954305370,
      "node_id": "PRRC_kwDOABII58444Yta",
      "diff_hunk": "@@ -0,0 +1,316 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 56,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953981817,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Seems I'm wrong; `std::deque<T>::clear()` does not guarantee memory is released. ",
      "created_at": "2022-08-24T21:13:52Z",
      "updated_at": "2022-08-24T21:13:52Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954305370",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954305370"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 56,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954375128",
      "pull_request_review_id": 1084612308,
      "id": 954375128,
      "node_id": "PRRC_kwDOABII58444pvY",
      "diff_hunk": "@@ -581,18 +588,59 @@ class PeerManagerImpl final : public PeerManager\n \n     void ProcessOrphanTx(std::set<uint256>& orphan_work_set) EXCLUSIVE_LOCKS_REQUIRED(cs_main, g_cs_orphans)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n-    /** Process a single headers message from a peer. */\n+    /** Process a single headers message from a peer.\n+     *\n+     * @param[in]   pfrom     CNode of the peer\n+     * @param[in]   peer      The peer sending us the headers\n+     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.\n+     * @param[in]   via_compact_block   Whether this header came in via compact block handling.\n+    */\n     void ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n-                               const std::vector<CBlockHeader>& headers,\n+                               std::vector<CBlockHeader>& headers,\n                                bool via_compact_block)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n     /** Various helpers for headers processing, invoked by ProcessHeadersMessage() */\n+    /** Return true if headers are continuous and have valid proof-of-work */\n+    bool CheckHeadersPoW(const std::vector<CBlockHeader>& headers, const Consensus::Params& consensusParams, Peer& peer);\n+    /** Calculate an anti-DoS work threshold for headers chains */\n+    arith_uint256 GetAntiDoSWorkThreshold();\n     /** Deal with state tracking and headers sync for peers that send the\n      * occasional non-connecting header (this can happen due to BIP 130 headers\n      * announcements for blocks interacting with the 2hr (MAX_FUTURE_BLOCK_TIME) rule). */\n     void HandleFewUnconnectingHeaders(CNode& pfrom, Peer& peer, const std::vector<CBlockHeader>& headers);\n     /** Return true if the headers connect to each other, false otherwise */\n     bool CheckHeadersAreContinuous(const std::vector<CBlockHeader>& headers) const;\n+    /** Try to continue a low-work headers sync that has already begun.",
      "path": "src/net_processing.cpp",
      "position": 74,
      "original_position": 49,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953760195,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-24T23:23:39Z",
      "updated_at": "2022-08-24T23:23:39Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954375128",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954375128"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 616,
      "original_line": 616,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954375148",
      "pull_request_review_id": 1084612349,
      "id": 954375148,
      "node_id": "PRRC_kwDOABII58444pvs",
      "diff_hunk": "@@ -581,18 +588,59 @@ class PeerManagerImpl final : public PeerManager\n \n     void ProcessOrphanTx(std::set<uint256>& orphan_work_set) EXCLUSIVE_LOCKS_REQUIRED(cs_main, g_cs_orphans)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n-    /** Process a single headers message from a peer. */\n+    /** Process a single headers message from a peer.\n+     *\n+     * @param[in]   pfrom     CNode of the peer\n+     * @param[in]   peer      The peer sending us the headers\n+     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.\n+     * @param[in]   via_compact_block   Whether this header came in via compact block handling.\n+    */\n     void ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n-                               const std::vector<CBlockHeader>& headers,\n+                               std::vector<CBlockHeader>& headers,\n                                bool via_compact_block)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n     /** Various helpers for headers processing, invoked by ProcessHeadersMessage() */\n+    /** Return true if headers are continuous and have valid proof-of-work */\n+    bool CheckHeadersPoW(const std::vector<CBlockHeader>& headers, const Consensus::Params& consensusParams, Peer& peer);\n+    /** Calculate an anti-DoS work threshold for headers chains */\n+    arith_uint256 GetAntiDoSWorkThreshold();\n     /** Deal with state tracking and headers sync for peers that send the\n      * occasional non-connecting header (this can happen due to BIP 130 headers\n      * announcements for blocks interacting with the 2hr (MAX_FUTURE_BLOCK_TIME) rule). */\n     void HandleFewUnconnectingHeaders(CNode& pfrom, Peer& peer, const std::vector<CBlockHeader>& headers);\n     /** Return true if the headers connect to each other, false otherwise */\n     bool CheckHeadersAreContinuous(const std::vector<CBlockHeader>& headers) const;\n+    /** Try to continue a low-work headers sync that has already begun.\n+     *  @param[in]  peer                            The peer we're syncing with.\n+     *  @param[in]  pfrom                           CNode of the peer\n+     *  @param[in,out] headers                      The headers to be processed.\n+     *  @return     True if the passed in headers were successfully processed; false otherwise.",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 53,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953764078,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Reworked this comment to be clearer.",
      "created_at": "2022-08-24T23:23:43Z",
      "updated_at": "2022-08-24T23:23:43Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954375148",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954375148"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 617,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954375609",
      "pull_request_review_id": 1084612970,
      "id": 954375609,
      "node_id": "PRRC_kwDOABII58444p25",
      "diff_hunk": "@@ -2316,6 +2393,104 @@ bool PeerManagerImpl::CheckHeadersAreContinuous(const std::vector<CBlockHeader>&\n     return true;\n }\n \n+bool PeerManagerImpl::IsContinuationOfLowWorkHeadersSync(Peer& peer, CNode& pfrom, std::vector<CBlockHeader>& headers)\n+{\n+    if (peer.m_headers_sync) {\n+        auto result = peer.m_headers_sync->ProcessNextHeaders(headers, headers.size() == MAX_HEADERS_RESULTS);\n+        if (result.request_more) {\n+            auto locator = peer.m_headers_sync->MakeNextHeadersRequest();\n+            // If we were instructed to ask for a locator, it should not be empty.\n+            Assume(!locator.vHave.empty());\n+            if (!locator.vHave.empty()) {\n+                // It should be impossible for the getheaders request to fail,\n+                // because we should have cleared the last getheaders timestamp\n+                // when processing the headers that triggered this call. But\n+                // it may be possible to bypass this via compactblock\n+                // processing, so check the result before logging just to be\n+                // safe.\n+                bool sent_getheaders = MaybeSendGetHeaders(pfrom, locator, peer);\n+                if (sent_getheaders) {\n+                    LogPrint(BCLog::NET, \"more getheaders (from %s) to peer=%d\\n\",\n+                            locator.vHave.front().ToString(), pfrom.GetId());\n+                }",
      "path": "src/net_processing.cpp",
      "position": 269,
      "original_position": 142,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953811246,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-24T23:24:50Z",
      "updated_at": "2022-08-24T23:24:50Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954375609",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954375609"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2467,
      "original_line": 2467,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954375666",
      "pull_request_review_id": 1084613029,
      "id": 954375666,
      "node_id": "PRRC_kwDOABII58444p3y",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** PRESYNC means the peer has not yet demonstrated their chain has\n+         * sufficient work and we're only building commitments to the chain they\n+         * serve us. */\n+        PRESYNC,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Construct a HeadersSyncState object representing a headers sync via this\n+     *  download-twice mechanism).\n+     *\n+     * id: node id (for logging)\n+     * consensus_params: parameters needed for difficulty adjustment validation\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+            const CBlockIndex* chain_start, const arith_uint256& minimum_required_work);\n+\n+    /** Result data structure for ProcessNextHeaders. */\n+    struct ProcessingResult {\n+        std::vector<CBlockHeader> pow_validated_headers;\n+        bool success{false};\n+        bool request_more{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * received_headers: headers that were received over the network for processing.\n+     *                   Assumes the caller has already verified the headers connect,\n+     *                   and has checked that each header satisfies the\n+     *                   proof-of-work target included in the header (but not\n+     *                   necessarily verified that the proof-of-work target is\n+     *                   correct and passes consensus rules).\n+     * full_headers_message: true if the message was at max capacity,\n+     *                       indicating more headers may be available\n+     * ProcessingResult.pow_validated_headers: will be filled in with any\n+     *                       headers that the caller can fully process and\n+     *                       validate now (because these returned headers are\n+     *                       on a chain with sufficient work)\n+     * ProcessingResult.success: set to false if an error is detected and the sync is\n+     *                       aborted; true otherwise.\n+     * ProcessingResult.request_more: if true, the caller is suggested to call\n+     *                       MakeNextHeadersRequest and send a getheaders message using it.\n+     */\n+    ProcessingResult ProcessNextHeaders(const std::vector<CBlockHeader>&\n+            received_headers, bool full_headers_message);\n+\n+    /** Issue the next GETHEADERS message to our peer.\n+     *\n+     * This will return a locator appropriate for the current sync object, to continue the\n+     * synchronization phase it is in.\n+     */\n+    CBlockLocator MakeNextHeadersRequest() const;",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 166,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953815564,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Agreed, done.",
      "created_at": "2022-08-24T23:24:56Z",
      "updated_at": "2022-08-24T23:24:56Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954375666",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954375666"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 166,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954375719",
      "pull_request_review_id": 1084613102,
      "id": 954375719,
      "node_id": "PRRC_kwDOABII58444p4n",
      "diff_hunk": "@@ -2461,21 +2636,65 @@ void PeerManagerImpl::UpdatePeerStateForReceivedHeaders(CNode& pfrom,\n }\n \n void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n-                                            const std::vector<CBlockHeader>& headers,\n+                                            std::vector<CBlockHeader>& headers,\n                                             bool via_compact_block)\n {\n-    const CNetMsgMaker msgMaker(pfrom.GetCommonVersion());\n     size_t nCount = headers.size();\n \n     if (nCount == 0) {\n         // Nothing interesting. Stop asking this peers for more headers.\n+        // If we were in the middle of headers sync, receiving an empty headers\n+        // message suggests that the peer suddenly has nothing to give us\n+        // (perhaps it reorged to our chain). Clear download state for this peer.\n+        LOCK(peer.m_headers_sync_mutex);\n+        if (peer.m_headers_sync) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }\n+        return;\n+    }\n+\n+    // Before we do any processing, make sure these pass basic sanity checks.\n+    // We'll rely on headers having valid proof-of-work further down, as an\n+    // anti-DoS criteria (note: this check is required before passing any\n+    // headers/ into HeadersSyncState).",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 250,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953833317,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed.",
      "created_at": "2022-08-24T23:25:04Z",
      "updated_at": "2022-08-24T23:25:04Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954375719",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954375719"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 2659,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954375821",
      "pull_request_review_id": 1084613230,
      "id": 954375821,
      "node_id": "PRRC_kwDOABII58444p6N",
      "diff_hunk": "@@ -2461,21 +2636,65 @@ void PeerManagerImpl::UpdatePeerStateForReceivedHeaders(CNode& pfrom,\n }\n \n void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n-                                            const std::vector<CBlockHeader>& headers,\n+                                            std::vector<CBlockHeader>& headers,\n                                             bool via_compact_block)\n {\n-    const CNetMsgMaker msgMaker(pfrom.GetCommonVersion());\n     size_t nCount = headers.size();\n \n     if (nCount == 0) {\n         // Nothing interesting. Stop asking this peers for more headers.\n+        // If we were in the middle of headers sync, receiving an empty headers\n+        // message suggests that the peer suddenly has nothing to give us\n+        // (perhaps it reorged to our chain). Clear download state for this peer.\n+        LOCK(peer.m_headers_sync_mutex);\n+        if (peer.m_headers_sync) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }\n+        return;\n+    }\n+\n+    // Before we do any processing, make sure these pass basic sanity checks.\n+    // We'll rely on headers having valid proof-of-work further down, as an\n+    // anti-DoS criteria (note: this check is required before passing any\n+    // headers/ into HeadersSyncState).\n+    if (!CheckHeadersPoW(headers, m_chainparams.GetConsensus(), peer)) {\n+        Misbehaving(peer, 100, \"invalid proof-of-work\");\n         return;\n     }\n \n     const CBlockIndex *pindexLast = nullptr;\n \n+    // We'll set already_validated_work to true if the headers-sync logic\n+    // returns headers for us to process, to bypass the minimum work check\n+    // (which is done separately inside m_headers_sync)\n+    bool already_validated_work = false;\n+\n+    // If we're in the middle of headers sync, let it do its magic.\n+    bool have_headers_sync = false;\n+    {\n+        LOCK(peer.m_headers_sync_mutex);\n+\n+        already_validated_work = IsContinuationOfLowWorkHeadersSync(peer, pfrom, headers);",
      "path": "src/net_processing.cpp",
      "position": 441,
      "original_position": 268,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953853582,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Updated the comments around this variable.",
      "created_at": "2022-08-24T23:25:18Z",
      "updated_at": "2022-08-24T23:25:18Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954375821",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954375821"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2775,
      "original_line": 2775,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954376297",
      "pull_request_review_id": 1084613830,
      "id": 954376297,
      "node_id": "PRRC_kwDOABII58444qBp",
      "diff_hunk": "@@ -2461,21 +2636,65 @@ void PeerManagerImpl::UpdatePeerStateForReceivedHeaders(CNode& pfrom,\n }\n \n void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n-                                            const std::vector<CBlockHeader>& headers,\n+                                            std::vector<CBlockHeader>& headers,\n                                             bool via_compact_block)\n {\n-    const CNetMsgMaker msgMaker(pfrom.GetCommonVersion());\n     size_t nCount = headers.size();\n \n     if (nCount == 0) {\n         // Nothing interesting. Stop asking this peers for more headers.\n+        // If we were in the middle of headers sync, receiving an empty headers\n+        // message suggests that the peer suddenly has nothing to give us\n+        // (perhaps it reorged to our chain). Clear download state for this peer.\n+        LOCK(peer.m_headers_sync_mutex);\n+        if (peer.m_headers_sync) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }\n+        return;\n+    }\n+\n+    // Before we do any processing, make sure these pass basic sanity checks.\n+    // We'll rely on headers having valid proof-of-work further down, as an\n+    // anti-DoS criteria (note: this check is required before passing any\n+    // headers/ into HeadersSyncState).\n+    if (!CheckHeadersPoW(headers, m_chainparams.GetConsensus(), peer)) {\n+        Misbehaving(peer, 100, \"invalid proof-of-work\");\n         return;\n     }\n \n     const CBlockIndex *pindexLast = nullptr;\n \n+    // We'll set already_validated_work to true if the headers-sync logic\n+    // returns headers for us to process, to bypass the minimum work check",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 259,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953858402,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Reworked this comment.",
      "created_at": "2022-08-24T23:26:28Z",
      "updated_at": "2022-08-24T23:26:28Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954376297",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954376297"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 2668,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954376508",
      "pull_request_review_id": 1084614116,
      "id": 954376508,
      "node_id": "PRRC_kwDOABII58444qE8",
      "diff_hunk": "@@ -0,0 +1,316 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 19,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953964777,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I'm not sure \"bits of security\" is quite the right way to think about this, but I added a comment mentioning this is about 23.9 commitment bits that will fit in the buffer.",
      "created_at": "2022-08-24T23:27:00Z",
      "updated_at": "2022-08-24T23:27:00Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954376508",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954376508"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 19,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954376548",
      "pull_request_review_id": 1084614183,
      "id": 954376548,
      "node_id": "PRRC_kwDOABII58444qFk",
      "diff_hunk": "@@ -3913,7 +4154,8 @@ void PeerManagerImpl::ProcessMessage(CNode& pfrom, const std::string& msg_type,\n             // the peer if the header turns out to be for an invalid block.\n             // Note that if a peer tries to build on an invalid chain, that\n             // will be detected and the peer will be disconnected/discouraged.\n-            return ProcessHeadersMessage(pfrom, *peer, {cmpctblock.header}, /*via_compact_block=*/true);\n+            std::vector<CBlockHeader> dummy{cmpctblock.header};",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 351,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953884672,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed.",
      "created_at": "2022-08-24T23:27:07Z",
      "updated_at": "2022-08-24T23:27:07Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954376548",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954376548"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 4157,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954376673",
      "pull_request_review_id": 1084614317,
      "id": 954376673,
      "node_id": "PRRC_kwDOABII58444qHh",
      "diff_hunk": "@@ -0,0 +1,316 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {",
      "path": "src/headerssync.cpp",
      "position": 90,
      "original_position": 90,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953995060,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-24T23:27:22Z",
      "updated_at": "2022-08-24T23:27:22Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954376673",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954376673"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 90,
      "original_line": 90,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954376714",
      "pull_request_review_id": 1084614379,
      "id": 954376714,
      "node_id": "PRRC_kwDOABII58444qIK",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** PRESYNC means the peer has not yet demonstrated their chain has\n+         * sufficient work and we're only building commitments to the chain they\n+         * serve us. */\n+        PRESYNC,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Construct a HeadersSyncState object representing a headers sync via this\n+     *  download-twice mechanism).\n+     *\n+     * id: node id (for logging)\n+     * consensus_params: parameters needed for difficulty adjustment validation\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+            const CBlockIndex* chain_start, const arith_uint256& minimum_required_work);\n+\n+    /** Result data structure for ProcessNextHeaders. */\n+    struct ProcessingResult {\n+        std::vector<CBlockHeader> pow_validated_headers;\n+        bool success{false};\n+        bool request_more{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * received_headers: headers that were received over the network for processing.\n+     *                   Assumes the caller has already verified the headers connect,\n+     *                   and has checked that each header satisfies the\n+     *                   proof-of-work target included in the header (but not\n+     *                   necessarily verified that the proof-of-work target is\n+     *                   correct and passes consensus rules).\n+     * full_headers_message: true if the message was at max capacity,\n+     *                       indicating more headers may be available\n+     * ProcessingResult.pow_validated_headers: will be filled in with any\n+     *                       headers that the caller can fully process and\n+     *                       validate now (because these returned headers are\n+     *                       on a chain with sufficient work)\n+     * ProcessingResult.success: set to false if an error is detected and the sync is\n+     *                       aborted; true otherwise.\n+     * ProcessingResult.request_more: if true, the caller is suggested to call\n+     *                       MakeNextHeadersRequest and send a getheaders message using it.\n+     */\n+    ProcessingResult ProcessNextHeaders(const std::vector<CBlockHeader>&\n+            received_headers, bool full_headers_message);\n+\n+    /** Issue the next GETHEADERS message to our peer.\n+     *\n+     * This will return a locator appropriate for the current sync object, to continue the\n+     * synchronization phase it is in.\n+     */\n+    CBlockLocator MakeNextHeadersRequest() const;\n+\n+private:\n+    /** Clear out all download state that might be in progress (freeing any used\n+     * memory), and mark this object as no longer usable.\n+     */\n+    void Finalize();\n+\n+    /**\n+     *  Only called in PRESYNC.\n+     *  Validate the work on the headers we received from the network, and\n+     *  store commitments for later. Update overall state with successfully\n+     *  processed headers.\n+     *  On failure, this invokes Finalize() and returns false.\n+     */\n+    bool ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers);\n+\n+    /** In PRESYNC, process and update state for a single header */\n+    bool ValidateAndProcessSingleHeader(const CBlockHeader& previous, const\n+            CBlockHeader& current);\n+\n+    /** In REDOWNLOAD, check a header's commitment (if applicable) and add to\n+     * buffer for later processing */\n+    bool ValidateAndStoreRedownloadedHeader(const CBlockHeader& header);\n+\n+    /** Return a set of headers that satisfy our proof-of-work threshold */\n+    std::vector<CBlockHeader> PopHeadersReadyForAcceptance();\n+\n+private:\n+    /** NodeId of the peer (used for log messages) **/\n+    const NodeId m_id;\n+\n+    /** We use the consensus params in our anti-DoS calculations */\n+    const Consensus::Params& m_consensus_params;\n+\n+    /** Store the last block in our block index that the peer's chain builds from */\n+    const CBlockIndex* m_chain_start{nullptr};\n+\n+    /** Minimum work that we're looking for on this chain. */\n+    const arith_uint256 m_minimum_required_work;\n+\n+    /** Work that we've seen so far on the peer's chain */\n+    arith_uint256 m_current_chain_work;\n+\n+    /** m_hasher is a salted hasher for making our 1-bit commitments to headers we've seen. */\n+    const SaltedTxidHasher m_hasher;\n+\n+    /** A queue of commitment bits, created during the 1st phase, and verified during the 2nd. */\n+    bitdeque<> m_header_commitments;\n+\n+    /** The (secret) offset on the heights for which to create commitments.\n+     *\n+     * m_header_commitments entries are created at any height h for which\n+     * (h % HEADER_COMMITMENT_PERIOD) == m_commit_offset. */\n+    const unsigned m_commit_offset;\n+\n+    /** m_max_commitments is a bound we calculate on how long an honest peer's chain could be,\n+     * given the MTP rule.\n+     *\n+     * Any peer giving us more headers than this will have its sync aborted. This serves as a\n+     * memory bound on m_header_commitments. */\n+    uint64_t m_max_commitments{0};\n+\n+    /** Store the latest header received while in PRESYNC */",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 229,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 954202598,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Updated comment.",
      "created_at": "2022-08-24T23:27:28Z",
      "updated_at": "2022-08-24T23:27:29Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954376714",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954376714"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 229,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954376927",
      "pull_request_review_id": 1084614627,
      "id": 954376927,
      "node_id": "PRRC_kwDOABII58444qLf",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** PRESYNC means the peer has not yet demonstrated their chain has\n+         * sufficient work and we're only building commitments to the chain they\n+         * serve us. */\n+        PRESYNC,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Construct a HeadersSyncState object representing a headers sync via this\n+     *  download-twice mechanism).\n+     *\n+     * id: node id (for logging)\n+     * consensus_params: parameters needed for difficulty adjustment validation\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+            const CBlockIndex* chain_start, const arith_uint256& minimum_required_work);\n+\n+    /** Result data structure for ProcessNextHeaders. */\n+    struct ProcessingResult {\n+        std::vector<CBlockHeader> pow_validated_headers;\n+        bool success{false};\n+        bool request_more{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * received_headers: headers that were received over the network for processing.\n+     *                   Assumes the caller has already verified the headers connect,\n+     *                   and has checked that each header satisfies the\n+     *                   proof-of-work target included in the header (but not\n+     *                   necessarily verified that the proof-of-work target is\n+     *                   correct and passes consensus rules).\n+     * full_headers_message: true if the message was at max capacity,\n+     *                       indicating more headers may be available\n+     * ProcessingResult.pow_validated_headers: will be filled in with any\n+     *                       headers that the caller can fully process and\n+     *                       validate now (because these returned headers are\n+     *                       on a chain with sufficient work)\n+     * ProcessingResult.success: set to false if an error is detected and the sync is\n+     *                       aborted; true otherwise.\n+     * ProcessingResult.request_more: if true, the caller is suggested to call\n+     *                       MakeNextHeadersRequest and send a getheaders message using it.\n+     */\n+    ProcessingResult ProcessNextHeaders(const std::vector<CBlockHeader>&\n+            received_headers, bool full_headers_message);\n+\n+    /** Issue the next GETHEADERS message to our peer.\n+     *\n+     * This will return a locator appropriate for the current sync object, to continue the\n+     * synchronization phase it is in.\n+     */\n+    CBlockLocator MakeNextHeadersRequest() const;\n+\n+private:\n+    /** Clear out all download state that might be in progress (freeing any used\n+     * memory), and mark this object as no longer usable.\n+     */\n+    void Finalize();\n+\n+    /**\n+     *  Only called in PRESYNC.\n+     *  Validate the work on the headers we received from the network, and\n+     *  store commitments for later. Update overall state with successfully\n+     *  processed headers.\n+     *  On failure, this invokes Finalize() and returns false.\n+     */\n+    bool ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers);\n+\n+    /** In PRESYNC, process and update state for a single header */\n+    bool ValidateAndProcessSingleHeader(const CBlockHeader& previous, const",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 184,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 954205962,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Eliminated the extra argument.",
      "created_at": "2022-08-24T23:27:55Z",
      "updated_at": "2022-08-24T23:27:55Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954376927",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954376927"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 184,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954376958",
      "pull_request_review_id": 1084614665,
      "id": 954376958,
      "node_id": "PRRC_kwDOABII58444qL-",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** PRESYNC means the peer has not yet demonstrated their chain has\n+         * sufficient work and we're only building commitments to the chain they\n+         * serve us. */\n+        PRESYNC,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Construct a HeadersSyncState object representing a headers sync via this\n+     *  download-twice mechanism).\n+     *\n+     * id: node id (for logging)\n+     * consensus_params: parameters needed for difficulty adjustment validation\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+            const CBlockIndex* chain_start, const arith_uint256& minimum_required_work);\n+\n+    /** Result data structure for ProcessNextHeaders. */\n+    struct ProcessingResult {\n+        std::vector<CBlockHeader> pow_validated_headers;\n+        bool success{false};\n+        bool request_more{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * received_headers: headers that were received over the network for processing.\n+     *                   Assumes the caller has already verified the headers connect,\n+     *                   and has checked that each header satisfies the\n+     *                   proof-of-work target included in the header (but not\n+     *                   necessarily verified that the proof-of-work target is\n+     *                   correct and passes consensus rules).\n+     * full_headers_message: true if the message was at max capacity,\n+     *                       indicating more headers may be available\n+     * ProcessingResult.pow_validated_headers: will be filled in with any\n+     *                       headers that the caller can fully process and\n+     *                       validate now (because these returned headers are\n+     *                       on a chain with sufficient work)\n+     * ProcessingResult.success: set to false if an error is detected and the sync is\n+     *                       aborted; true otherwise.\n+     * ProcessingResult.request_more: if true, the caller is suggested to call\n+     *                       MakeNextHeadersRequest and send a getheaders message using it.\n+     */\n+    ProcessingResult ProcessNextHeaders(const std::vector<CBlockHeader>&\n+            received_headers, bool full_headers_message);\n+\n+    /** Issue the next GETHEADERS message to our peer.\n+     *\n+     * This will return a locator appropriate for the current sync object, to continue the\n+     * synchronization phase it is in.\n+     */\n+    CBlockLocator MakeNextHeadersRequest() const;\n+\n+private:\n+    /** Clear out all download state that might be in progress (freeing any used\n+     * memory), and mark this object as no longer usable.\n+     */\n+    void Finalize();\n+\n+    /**\n+     *  Only called in PRESYNC.\n+     *  Validate the work on the headers we received from the network, and\n+     *  store commitments for later. Update overall state with successfully\n+     *  processed headers.\n+     *  On failure, this invokes Finalize() and returns false.\n+     */\n+    bool ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers);\n+\n+    /** In PRESYNC, process and update state for a single header */\n+    bool ValidateAndProcessSingleHeader(const CBlockHeader& previous, const\n+            CBlockHeader& current);\n+\n+    /** In REDOWNLOAD, check a header's commitment (if applicable) and add to\n+     * buffer for later processing */\n+    bool ValidateAndStoreRedownloadedHeader(const CBlockHeader& header);\n+\n+    /** Return a set of headers that satisfy our proof-of-work threshold */\n+    std::vector<CBlockHeader> PopHeadersReadyForAcceptance();\n+\n+private:\n+    /** NodeId of the peer (used for log messages) **/\n+    const NodeId m_id;\n+\n+    /** We use the consensus params in our anti-DoS calculations */\n+    const Consensus::Params& m_consensus_params;\n+\n+    /** Store the last block in our block index that the peer's chain builds from */\n+    const CBlockIndex* m_chain_start{nullptr};\n+\n+    /** Minimum work that we're looking for on this chain. */\n+    const arith_uint256 m_minimum_required_work;\n+\n+    /** Work that we've seen so far on the peer's chain */\n+    arith_uint256 m_current_chain_work;\n+\n+    /** m_hasher is a salted hasher for making our 1-bit commitments to headers we've seen. */\n+    const SaltedTxidHasher m_hasher;\n+\n+    /** A queue of commitment bits, created during the 1st phase, and verified during the 2nd. */\n+    bitdeque<> m_header_commitments;\n+\n+    /** The (secret) offset on the heights for which to create commitments.\n+     *\n+     * m_header_commitments entries are created at any height h for which\n+     * (h % HEADER_COMMITMENT_PERIOD) == m_commit_offset. */\n+    const unsigned m_commit_offset;\n+\n+    /** m_max_commitments is a bound we calculate on how long an honest peer's chain could be,\n+     * given the MTP rule.\n+     *\n+     * Any peer giving us more headers than this will have its sync aborted. This serves as a\n+     * memory bound on m_header_commitments. */\n+    uint64_t m_max_commitments{0};\n+\n+    /** Store the latest header received while in PRESYNC */\n+    CBlockHeader m_last_header_received;\n+\n+    /** Height of m_last_header_received */\n+    int64_t m_current_height{0};\n+\n+    /** During phase 2 (REDOWNLOAD), we buffer redownloaded headers in memory\n+     *  until enough commitments have been verified; those are stored in\n+     *  m_redownloaded_headers */\n+    std::deque<CompressedHeader> m_redownloaded_headers;\n+\n+    /** Height of last header in m_redownloaded_headers */\n+    int64_t m_redownload_buffer_last_height{0};\n+\n+    /** Hash of last header in m_redownloaded_headers (we have to cache it",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 243,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 954208859,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Updated comment.",
      "created_at": "2022-08-24T23:27:59Z",
      "updated_at": "2022-08-24T23:27:59Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954376958",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954376958"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 243,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954376995",
      "pull_request_review_id": 1084614720,
      "id": 954376995,
      "node_id": "PRRC_kwDOABII58444qMj",
      "diff_hunk": "@@ -0,0 +1,316 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps our peer reorged away from the chain\n+        // they were on. Give up on this sync for now (likely we will start a\n+        // new sync with a new starting point).\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (presync phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and occasionally store\n+    // commitments.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(m_last_header_received, hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& previous, const CBlockHeader& current)\n+{\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (presync phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    if (next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        // Add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // It's possible the chain grew since we started the sync; so\n+            // potentially we could succeed in syncing the peer's chain if we\n+            // try again later.\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: exceeded max commitments at height=%i (presync phase)\\n\", m_id, next_height);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));\n+    m_last_header_received = current;\n+    m_current_height = next_height;\n+\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreRedownloadedHeader(const CBlockHeader& header)\n+{\n+    Assume(m_download_state == State::REDOWNLOAD);\n+    if (m_download_state != State::REDOWNLOAD) return false;\n+\n+    int64_t next_height = m_redownload_buffer_last_height + 1;\n+\n+    // Ensure that we're working on a header that connects to the chain we're\n+    // downloading.\n+    if (header.hashPrevBlock != m_redownload_buffer_last_hash) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (redownload phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    // Check that the difficulty adjustments are within our tolerance:\n+    uint32_t previous_nBits{0};\n+    if (!m_redownloaded_headers.empty()) {\n+        previous_nBits = m_redownloaded_headers.back().nBits;\n+    } else {\n+        previous_nBits = m_chain_start->nBits;\n+    }\n+\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous_nBits, header.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (redownload phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    // Track work on the redownloaded chain\n+    m_redownload_chain_work += GetBlockProof(CBlockIndex(header));\n+\n+    if (m_redownload_chain_work >= m_minimum_required_work) {\n+        m_process_all_remaining_headers = true;\n+    }\n+\n+    // If we're at a header for which we previously stored a commitment, verify\n+    // it is correct. Failure will result in aborting download.\n+    // Also, don't check commitments once we've gotten to our target blockhash;\n+    // it's possible our peer has extended its chain between our first sync and\n+    // our second, and we don't want to return failure after we've seen our\n+    // target blockhash just because we ran out of commitments.\n+    if (!m_process_all_remaining_headers && next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+         bool commitment = m_hasher(header.GetHash()) & 1;",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 256,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 954217787,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-24T23:28:04Z",
      "updated_at": "2022-08-24T23:28:05Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954376995",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954376995"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 256,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954384896",
      "pull_request_review_id": 1084625067,
      "id": 954384896,
      "node_id": "PRRC_kwDOABII58444sIA",
      "diff_hunk": "@@ -0,0 +1,316 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959};\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    std::deque<CompressedHeader>().swap(m_redownloaded_headers);",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 56,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953981817,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Rewrote this as `m_redownloaded_headers = std::deque<CompressedHeader>{};`",
      "created_at": "2022-08-24T23:45:28Z",
      "updated_at": "2022-08-24T23:45:29Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954384896",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954384896"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 56,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954647587",
      "pull_request_review_id": 1084990058,
      "id": 954647587,
      "node_id": "PRRC_kwDOABII58445sQj",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** PRESYNC means the peer has not yet demonstrated their chain has\n+         * sufficient work and we're only building commitments to the chain they\n+         * serve us. */\n+        PRESYNC,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Construct a HeadersSyncState object representing a headers sync via this\n+     *  download-twice mechanism).\n+     *\n+     * id: node id (for logging)\n+     * consensus_params: parameters needed for difficulty adjustment validation\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+            const CBlockIndex* chain_start, const arith_uint256& minimum_required_work);\n+\n+    /** Result data structure for ProcessNextHeaders. */\n+    struct ProcessingResult {\n+        std::vector<CBlockHeader> pow_validated_headers;\n+        bool success{false};\n+        bool request_more{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * received_headers: headers that were received over the network for processing.\n+     *                   Assumes the caller has already verified the headers connect,",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 142,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953766769,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "> headers within a headers message that don't connect \r\n\r\nI've been assuming that in the source code comments \"connect\" means the first headers fits onto something we have and \"continuous\" means all headers inside the package connect (with no forks?). For each function we should probably be clear which of the two we care about.\r\n\r\n> The logic does (or should!) correctly deal with a case where the first header in a message doesn't connect with the last header in a prior message,\r\n\r\nIt indeed does, afaik.",
      "created_at": "2022-08-25T08:13:29Z",
      "updated_at": "2022-08-25T08:15:47Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954647587",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954647587"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 151,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954651717",
      "pull_request_review_id": 1084995997,
      "id": 954651717,
      "node_id": "PRRC_kwDOABII58445tRF",
      "diff_hunk": "@@ -581,18 +588,59 @@ class PeerManagerImpl final : public PeerManager\n \n     void ProcessOrphanTx(std::set<uint256>& orphan_work_set) EXCLUSIVE_LOCKS_REQUIRED(cs_main, g_cs_orphans)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n-    /** Process a single headers message from a peer. */\n+    /** Process a single headers message from a peer.\n+     *\n+     * @param[in]   pfrom     CNode of the peer\n+     * @param[in]   peer      The peer sending us the headers\n+     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.\n+     * @param[in]   via_compact_block   Whether this header came in via compact block handling.\n+    */\n     void ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n-                               const std::vector<CBlockHeader>& headers,\n+                               std::vector<CBlockHeader>& headers,\n                                bool via_compact_block)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n     /** Various helpers for headers processing, invoked by ProcessHeadersMessage() */\n+    /** Return true if headers are continuous and have valid proof-of-work */\n+    bool CheckHeadersPoW(const std::vector<CBlockHeader>& headers, const Consensus::Params& consensusParams, Peer& peer);\n+    /** Calculate an anti-DoS work threshold for headers chains */\n+    arith_uint256 GetAntiDoSWorkThreshold();\n     /** Deal with state tracking and headers sync for peers that send the\n      * occasional non-connecting header (this can happen due to BIP 130 headers\n      * announcements for blocks interacting with the 2hr (MAX_FUTURE_BLOCK_TIME) rule). */\n     void HandleFewUnconnectingHeaders(CNode& pfrom, Peer& peer, const std::vector<CBlockHeader>& headers);\n     /** Return true if the headers connect to each other, false otherwise */\n     bool CheckHeadersAreContinuous(const std::vector<CBlockHeader>& headers) const;\n+    /** Try to continue a low-work headers sync that has already begun.\n+     *  @param[in]  peer                            The peer we're syncing with.\n+     *  @param[in]  pfrom                           CNode of the peer\n+     *  @param[in,out] headers                      The headers to be processed.",
      "path": "src/net_processing.cpp",
      "position": 80,
      "original_position": 52,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953820788,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "There is indeed a trade-off with review burden at this point. ",
      "created_at": "2022-08-25T08:17:35Z",
      "updated_at": "2022-08-25T08:17:35Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954651717",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954651717"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 622,
      "original_line": 622,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954656790",
      "pull_request_review_id": 1085003360,
      "id": 954656790,
      "node_id": "PRRC_kwDOABII58445ugW",
      "diff_hunk": "@@ -581,18 +588,59 @@ class PeerManagerImpl final : public PeerManager\n \n     void ProcessOrphanTx(std::set<uint256>& orphan_work_set) EXCLUSIVE_LOCKS_REQUIRED(cs_main, g_cs_orphans)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n-    /** Process a single headers message from a peer. */\n+    /** Process a single headers message from a peer.\n+     *\n+     * @param[in]   pfrom     CNode of the peer\n+     * @param[in]   peer      The peer sending us the headers\n+     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.",
      "path": "src/net_processing.cpp",
      "position": 54,
      "original_position": 30,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953741265,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "The bigger question is if this should be a single param or two (and if so, if that's for a followup):\r\nhttps://github.com/bitcoin/bitcoin/pull/25717#discussion_r954277672\r\n\r\nWhether `headers` is treated as an output or just as an input-that-might-get-modified depends on what the callers do with it.",
      "created_at": "2022-08-25T08:22:41Z",
      "updated_at": "2022-08-25T08:22:41Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954656790",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954656790"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 598,
      "original_line": 598,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954662881",
      "pull_request_review_id": 1085012984,
      "id": 954662881,
      "node_id": "PRRC_kwDOABII58445v_h",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}",
      "path": "src/headerssync.h",
      "position": 103,
      "original_position": 103,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953979307,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Wouldn't it be safer to always require a call to `Finalize()` in these cases? Alternatively, to have a `State::NEW` where we know nothing has been allocated yet.",
      "created_at": "2022-08-25T08:28:37Z",
      "updated_at": "2022-08-25T08:28:38Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954662881",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954662881"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 103,
      "original_line": 103,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954702231",
      "pull_request_review_id": 1085071070,
      "id": 954702231,
      "node_id": "PRRC_kwDOABII584455mX",
      "diff_hunk": "@@ -3668,6 +3669,10 @@ bool ChainstateManager::AcceptBlockHeader(const CBlockHeader& block, BlockValida\n             }\n         }\n     }\n+    if (!min_pow_checked) {\n+        LogPrint(BCLog::VALIDATION, \"%s: not adding new block header %s, missing anti-dos proof-of-work validation\\n\", __func__, hash.ToString());",
      "path": "src/validation.cpp",
      "position": 49,
      "original_position": 17,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "01a9c4dc2869708957ce3d9d4a391740e074363c",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "01a9c4dc2869708957ce3d9d4a391740e074363c: would be good to have a test case, if it's actually possible to hit this condition (see suggestion below)",
      "created_at": "2022-08-25T08:58:36Z",
      "updated_at": "2022-08-25T09:16:09Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954702231",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954702231"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 3673,
      "original_line": 3673,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954715115",
      "pull_request_review_id": 1085071070,
      "id": 954715115,
      "node_id": "PRRC_kwDOABII584458vr",
      "diff_hunk": "@@ -4336,8 +4346,14 @@ void PeerManagerImpl::ProcessMessage(CNode& pfrom, const std::string& msg_type,\n             // which peers send us compact blocks, so the race between here and\n             // cs_main in ProcessNewBlock is fine.\n             mapBlockSource.emplace(hash, std::make_pair(pfrom.GetId(), true));\n+\n+            // Check work on this block against our anti-dos thresholds.\n+            const CBlockIndex* prev_block = m_chainman.m_blockman.LookupBlockIndex(pblock->hashPrevBlock);\n+            if (prev_block && prev_block->nChainWork + CalculateHeadersWork({pblock->GetBlockHeader()}) >= GetAntiDoSWorkThreshold()) {\n+                min_pow_checked = true;",
      "path": "src/net_processing.cpp",
      "position": 650,
      "original_position": 104,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "01a9c4dc2869708957ce3d9d4a391740e074363c",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "01a9c4dc2869708957ce3d9d4a391740e074363c: so in the `else` case `min_pow_checked` would remain `false` and thus trigger the `BLOCK_HEADER_LOW_WORK` condition, can we cover that in a test?",
      "created_at": "2022-08-25T09:09:11Z",
      "updated_at": "2022-08-25T09:15:16Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954715115",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954715115"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 4436,
      "original_line": 4436,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954839356",
      "pull_request_review_id": 1085263210,
      "id": 954839356,
      "node_id": "PRRC_kwDOABII58446bE8",
      "diff_hunk": "@@ -121,6 +121,12 @@ class HeadersSyncState {\n     /** Return the height reached during the PRESYNC phase */\n     int64_t GetPresyncHeight() const { return m_current_height; }\n \n+    /** Return the block timestamp of the last block received during the PRESYNC phase. */",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 4,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "bb60d93657df10702a48ea3094d2eee732a80e52",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "bb60d93657df10702a48ea3094d2eee732a80e52: last _header_ received",
      "created_at": "2022-08-25T11:18:38Z",
      "updated_at": "2022-08-25T11:51:53Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954839356",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954839356"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 124,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954849643",
      "pull_request_review_id": 1085263210,
      "id": 954849643,
      "node_id": "PRRC_kwDOABII58446dlr",
      "diff_hunk": "@@ -107,11 +110,35 @@ def test_peerinfo_includes_headers_presync_height(self):\n         # getpeerinfo should show a sync in progress\n         assert_equal(node.getpeerinfo()[0]['presynced_headers'], 2000)\n \n+    def test_large_reorgs_can_succeed(self):\n+        self.log.info(\"Test that a 2000+ block reorg, starting from a point that is more than 2000 blocks before a locator entry, can succeed\")\n+\n+        self.sync_all() # Ensure all nodes are synced.\n+        self.disconnect_all()\n+\n+        # locator(block at height T) will have heights:\n+        # [T, T-1, ..., T-10, T-12, T-16, T-24, T-40, T-72, T-136, T-264,\n+        #  T-520, T-1032, T-2056, T-4104, ...]\n+        # So mine a number of blocks > 4104 to ensure that the first window of\n+        # received headers during a sync are fully between locator entries.\n+        BLOCKS_TO_MINE = 4110\n+\n+        self.generate(self.nodes[0], BLOCKS_TO_MINE, sync_fun=self.no_op)\n+        self.generate(self.nodes[1], BLOCKS_TO_MINE+2, sync_fun=self.no_op)\n+\n+        self.reconnect_all()\n+\n+        self.sync_blocks(timeout=300)",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 47,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "ea7f50a3bce71c7e883245ea82e6547c02317f91",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "ea7f50a3bce71c7e883245ea82e6547c02317f91: maybe comment that this works because `sync_blocks` checks that both nodes agree on the tip.",
      "created_at": "2022-08-25T11:27:53Z",
      "updated_at": "2022-08-25T11:51:53Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954849643",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954849643"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 131,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954863605",
      "pull_request_review_id": 1085263210,
      "id": 954863605,
      "node_id": "PRRC_kwDOABII58446g_1",
      "diff_hunk": "@@ -3727,7 +3727,9 @@ void ChainstateManager::ReportHeadersPresync(const arith_uint256& work, int64_t\n         if (now < m_last_presync_update + std::chrono::milliseconds{250}) return;\n         m_last_presync_update = now;\n     }\n-    if (chainstate.IsInitialBlockDownload()) {\n+    bool initial_download = chainstate.IsInitialBlockDownload();\n+    uiInterface.NotifyHeaderTip(GetSynchronizationState(initial_download), height, timestamp, true);",
      "path": "src/validation.cpp",
      "position": null,
      "original_position": 6,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6a1df15b859bebad69a171e80d3c8eda4e936c46",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "6a1df15b859bebad69a171e80d3c8eda4e936c46 : `/*presync=*/true`",
      "created_at": "2022-08-25T11:45:06Z",
      "updated_at": "2022-08-25T11:51:53Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954863605",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954863605"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 3731,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954866050",
      "pull_request_review_id": 1085263210,
      "id": 954866050,
      "node_id": "PRRC_kwDOABII58446hmC",
      "diff_hunk": "@@ -615,8 +615,8 @@ void BitcoinGUI::setClientModel(ClientModel *_clientModel, interfaces::BlockAndH\n         connect(_clientModel, &ClientModel::numConnectionsChanged, this, &BitcoinGUI::setNumConnections);\n         connect(_clientModel, &ClientModel::networkActiveChanged, this, &BitcoinGUI::setNetworkActive);\n \n-        modalOverlay->setKnownBestHeight(tip_info->header_height, QDateTime::fromSecsSinceEpoch(tip_info->header_time));\n-        setNumBlocks(tip_info->block_height, QDateTime::fromSecsSinceEpoch(tip_info->block_time), tip_info->verification_progress, false, SynchronizationState::INIT_DOWNLOAD);\n+        modalOverlay->setKnownBestHeight(tip_info->header_height, QDateTime::fromSecsSinceEpoch(tip_info->header_time), false);",
      "path": "src/qt/bitcoingui.cpp",
      "position": null,
      "original_position": 6,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "acee94b4fdb1a28db1c217d82ca0611d7ba5a56d",
      "in_reply_to_id": null,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "acee94b4fdb1a28db1c217d82ca0611d7ba5a56d  `/*presync=/*false`",
      "created_at": "2022-08-25T11:47:55Z",
      "updated_at": "2022-08-25T11:51:53Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954866050",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954866050"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 618,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954887389",
      "pull_request_review_id": 1085328948,
      "id": 954887389,
      "node_id": "PRRC_kwDOABII58446mzd",
      "diff_hunk": "@@ -581,18 +588,59 @@ class PeerManagerImpl final : public PeerManager\n \n     void ProcessOrphanTx(std::set<uint256>& orphan_work_set) EXCLUSIVE_LOCKS_REQUIRED(cs_main, g_cs_orphans)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n-    /** Process a single headers message from a peer. */\n+    /** Process a single headers message from a peer.\n+     *\n+     * @param[in]   pfrom     CNode of the peer\n+     * @param[in]   peer      The peer sending us the headers\n+     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.",
      "path": "src/net_processing.cpp",
      "position": 54,
      "original_position": 30,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953741265,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "With the recent changes in the net_processing, I think it's easier to think about it as \"reference to a list of headers to be processed, and which may be modified\". All the caller would do with an output argument is overwrite the passed-in headers variable anyway.",
      "created_at": "2022-08-25T12:11:47Z",
      "updated_at": "2022-08-25T12:11:47Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r954887389",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/954887389"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 598,
      "original_line": 598,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955058609",
      "pull_request_review_id": 1085583423,
      "id": 955058609,
      "node_id": "PRRC_kwDOABII58447Qmx",
      "diff_hunk": "@@ -3668,6 +3669,10 @@ bool ChainstateManager::AcceptBlockHeader(const CBlockHeader& block, BlockValida\n             }\n         }\n     }\n+    if (!min_pow_checked) {\n+        LogPrint(BCLog::VALIDATION, \"%s: not adding new block header %s, missing anti-dos proof-of-work validation\\n\", __func__, hash.ToString());",
      "path": "src/validation.cpp",
      "position": 49,
      "original_position": 17,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "01a9c4dc2869708957ce3d9d4a391740e074363c",
      "in_reply_to_id": 954702231,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "There's a test in `p2p_unrequested_blocks.py` that fails when I comment out this `if()` code -- does that seems like sufficient test coverage?\r\n\r\n```\r\ndiff --git a/src/validation.cpp b/src/validation.cpp\r\nindex 21d801306158..c0df90128d04 100644\r\n--- a/src/validation.cpp\r\n+++ b/src/validation.cpp\r\n@@ -3669,10 +3669,10 @@ bool ChainstateManager::AcceptBlockHeader(const CBlockHeader& block, BlockValida\r\n             }\r\n         }\r\n     }\r\n-    if (!min_pow_checked) {\r\n-        LogPrint(BCLog::VALIDATION, \"%s: not adding new block header %s, missing anti-dos proof-of-work validation\\n\", __func__, hash.ToString());\r\n-        return state.Invalid(BlockValidationResult::BLOCK_HEADER_LOW_WORK, \"too-little-chainwork\");\r\n-    }\r\n+    //if (!min_pow_checked) {\r\n+    //    LogPrint(BCLog::VALIDATION, \"%s: not adding new block header %s, missing anti-dos proof-of-work validation\\n\", __func__, hash.ToString());\r\n+    //    return state.Invalid(BlockValidationResult::BLOCK_HEADER_LOW_WORK, \"too-little-chainwork\");\r\n+   // }\r\n     CBlockIndex* pindex{m_blockman.AddToBlockIndex(block, m_best_header)};\r\n \r\n     if (ppindex)\r\n```\r\n\r\n```\r\n2022-08-25T14:41:14.204000Z TestFramework (INFO): Initializing test directory /tmp/bitcoin_func_test_zotpggyy\r\n2022-08-25T14:41:14.791000Z TestFramework (ERROR): Assertion failed\r\nTraceback (most recent call last):\r\n  File \"/home/sdaftuar/ccl-bitcoin/2022-02-headers-dos-prevention/test/functional/test_framework/test_framework.py\", line 133, in main\r\n    self.run_test()\r\n  File \"/home/sdaftuar/ccl-bitcoin/2022-02-headers-dos-prevention/test/functional/p2p_unrequested_blocks.py\", line 105, in run_test\r\n    assert_equal(self.check_hash_in_chaintips(self.nodes[1], blocks_h2[1].hash), False)\r\n  File \"/home/sdaftuar/ccl-bitcoin/2022-02-headers-dos-prevention/test/functional/test_framework/util.py\", line 56, in assert_equal\r\n    raise AssertionError(\"not(%s)\" % \" == \".join(str(arg) for arg in (thing1, thing2) + args))\r\nAssertionError: not(True == False)\r\n2022-08-25T14:41:14.842000Z TestFramework (INFO): Stopping nodes\r\n2022-08-25T14:41:14.996000Z TestFramework (WARNING): Not cleaning up dir /tmp/bitcoin_func_test_zotpggyy\r\n2022-08-25T14:41:14.996000Z TestFramework (ERROR): Test failed. Test logging available at /tmp/bitcoin_func_test_zotpggyy/test_framework.log\r\n2022-08-25T14:41:14.996000Z TestFramework (ERROR): \r\n2022-08-25T14:41:14.996000Z TestFramework (ERROR): Hint: Call /home/sdaftuar/ccl-bitcoin/2022-02-headers-dos-prevention/test/functional/combine_logs.py '/tmp/bitcoin_func_test_zotpggyy' to consolidate all logs\r\n2022-08-25T14:41:14.996000Z TestFramework (ERROR): \r\n2022-08-25T14:41:14.997000Z TestFramework (ERROR): If this failure happened unexpectedly or intermittently, please file a bug and provide a link or upload of the combined log.\r\n2022-08-25T14:41:14.997000Z TestFramework (ERROR): https://github.com/bitcoin/bitcoin/issues\r\n2022-08-25T14:41:14.997000Z TestFramework (ERROR): \r\n```",
      "created_at": "2022-08-25T14:43:11Z",
      "updated_at": "2022-08-25T14:43:55Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955058609",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955058609"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 3673,
      "original_line": 3673,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955077588",
      "pull_request_review_id": 1085610249,
      "id": 955077588,
      "node_id": "PRRC_kwDOABII58447VPU",
      "diff_hunk": "@@ -3668,6 +3669,10 @@ bool ChainstateManager::AcceptBlockHeader(const CBlockHeader& block, BlockValida\n             }\n         }\n     }\n+    if (!min_pow_checked) {\n+        LogPrint(BCLog::VALIDATION, \"%s: not adding new block header %s, missing anti-dos proof-of-work validation\\n\", __func__, hash.ToString());",
      "path": "src/validation.cpp",
      "position": 49,
      "original_position": 17,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "01a9c4dc2869708957ce3d9d4a391740e074363c",
      "in_reply_to_id": 954702231,
      "user": {
        "login": "Sjors",
        "id": 10217,
        "node_id": "MDQ6VXNlcjEwMjE3",
        "avatar_url": "https://avatars.githubusercontent.com/u/10217?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Sjors",
        "html_url": "https://github.com/Sjors",
        "followers_url": "https://api.github.com/users/Sjors/followers",
        "following_url": "https://api.github.com/users/Sjors/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Sjors/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Sjors/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Sjors/subscriptions",
        "organizations_url": "https://api.github.com/users/Sjors/orgs",
        "repos_url": "https://api.github.com/users/Sjors/repos",
        "events_url": "https://api.github.com/users/Sjors/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Sjors/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Shouldn't that test be able to check for the log message?",
      "created_at": "2022-08-25T14:57:24Z",
      "updated_at": "2022-08-25T14:57:24Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955077588",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955077588"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 3673,
      "original_line": 3673,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955104972",
      "pull_request_review_id": 1085654167,
      "id": 955104972,
      "node_id": "PRRC_kwDOABII58447b7M",
      "diff_hunk": "@@ -3668,6 +3669,10 @@ bool ChainstateManager::AcceptBlockHeader(const CBlockHeader& block, BlockValida\n             }\n         }\n     }\n+    if (!min_pow_checked) {\n+        LogPrint(BCLog::VALIDATION, \"%s: not adding new block header %s, missing anti-dos proof-of-work validation\\n\", __func__, hash.ToString());",
      "path": "src/validation.cpp",
      "position": 49,
      "original_position": 17,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "01a9c4dc2869708957ce3d9d4a391740e074363c",
      "in_reply_to_id": 954702231,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-25T15:20:56Z",
      "updated_at": "2022-08-25T15:20:56Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955104972",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955104972"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 3673,
      "original_line": 3673,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955105167",
      "pull_request_review_id": 1085654570,
      "id": 955105167,
      "node_id": "PRRC_kwDOABII58447b-P",
      "diff_hunk": "@@ -107,11 +110,35 @@ def test_peerinfo_includes_headers_presync_height(self):\n         # getpeerinfo should show a sync in progress\n         assert_equal(node.getpeerinfo()[0]['presynced_headers'], 2000)\n \n+    def test_large_reorgs_can_succeed(self):\n+        self.log.info(\"Test that a 2000+ block reorg, starting from a point that is more than 2000 blocks before a locator entry, can succeed\")\n+\n+        self.sync_all() # Ensure all nodes are synced.\n+        self.disconnect_all()\n+\n+        # locator(block at height T) will have heights:\n+        # [T, T-1, ..., T-10, T-12, T-16, T-24, T-40, T-72, T-136, T-264,\n+        #  T-520, T-1032, T-2056, T-4104, ...]\n+        # So mine a number of blocks > 4104 to ensure that the first window of\n+        # received headers during a sync are fully between locator entries.\n+        BLOCKS_TO_MINE = 4110\n+\n+        self.generate(self.nodes[0], BLOCKS_TO_MINE, sync_fun=self.no_op)\n+        self.generate(self.nodes[1], BLOCKS_TO_MINE+2, sync_fun=self.no_op)\n+\n+        self.reconnect_all()\n+\n+        self.sync_blocks(timeout=300)",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": null,
      "original_position": 47,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "ea7f50a3bce71c7e883245ea82e6547c02317f91",
      "in_reply_to_id": 954849643,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-25T15:21:07Z",
      "updated_at": "2022-08-25T15:21:07Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955105167",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955105167"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 131,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955105566",
      "pull_request_review_id": 1085655398,
      "id": 955105566,
      "node_id": "PRRC_kwDOABII58447cEe",
      "diff_hunk": "@@ -121,6 +121,12 @@ class HeadersSyncState {\n     /** Return the height reached during the PRESYNC phase */\n     int64_t GetPresyncHeight() const { return m_current_height; }\n \n+    /** Return the block timestamp of the last block received during the PRESYNC phase. */",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 4,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "bb60d93657df10702a48ea3094d2eee732a80e52",
      "in_reply_to_id": 954839356,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed.",
      "created_at": "2022-08-25T15:21:29Z",
      "updated_at": "2022-08-25T15:21:30Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955105566",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955105566"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 124,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955105657",
      "pull_request_review_id": 1085655533,
      "id": 955105657,
      "node_id": "PRRC_kwDOABII58447cF5",
      "diff_hunk": "@@ -3727,7 +3727,9 @@ void ChainstateManager::ReportHeadersPresync(const arith_uint256& work, int64_t\n         if (now < m_last_presync_update + std::chrono::milliseconds{250}) return;\n         m_last_presync_update = now;\n     }\n-    if (chainstate.IsInitialBlockDownload()) {\n+    bool initial_download = chainstate.IsInitialBlockDownload();\n+    uiInterface.NotifyHeaderTip(GetSynchronizationState(initial_download), height, timestamp, true);",
      "path": "src/validation.cpp",
      "position": null,
      "original_position": 6,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "6a1df15b859bebad69a171e80d3c8eda4e936c46",
      "in_reply_to_id": 954863605,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-25T15:21:34Z",
      "updated_at": "2022-08-25T15:21:35Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955105657",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955105657"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 3731,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955105830",
      "pull_request_review_id": 1085655828,
      "id": 955105830,
      "node_id": "PRRC_kwDOABII58447cIm",
      "diff_hunk": "@@ -615,8 +615,8 @@ void BitcoinGUI::setClientModel(ClientModel *_clientModel, interfaces::BlockAndH\n         connect(_clientModel, &ClientModel::numConnectionsChanged, this, &BitcoinGUI::setNumConnections);\n         connect(_clientModel, &ClientModel::networkActiveChanged, this, &BitcoinGUI::setNetworkActive);\n \n-        modalOverlay->setKnownBestHeight(tip_info->header_height, QDateTime::fromSecsSinceEpoch(tip_info->header_time));\n-        setNumBlocks(tip_info->block_height, QDateTime::fromSecsSinceEpoch(tip_info->block_time), tip_info->verification_progress, false, SynchronizationState::INIT_DOWNLOAD);\n+        modalOverlay->setKnownBestHeight(tip_info->header_height, QDateTime::fromSecsSinceEpoch(tip_info->header_time), false);",
      "path": "src/qt/bitcoingui.cpp",
      "position": null,
      "original_position": 6,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "acee94b4fdb1a28db1c217d82ca0611d7ba5a56d",
      "in_reply_to_id": 954866050,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-25T15:21:45Z",
      "updated_at": "2022-08-25T15:21:45Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955105830",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955105830"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 618,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955113786",
      "pull_request_review_id": 1085667758,
      "id": 955113786,
      "node_id": "PRRC_kwDOABII58447eE6",
      "diff_hunk": "@@ -581,18 +588,59 @@ class PeerManagerImpl final : public PeerManager\n \n     void ProcessOrphanTx(std::set<uint256>& orphan_work_set) EXCLUSIVE_LOCKS_REQUIRED(cs_main, g_cs_orphans)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n-    /** Process a single headers message from a peer. */\n+    /** Process a single headers message from a peer.\n+     *\n+     * @param[in]   pfrom     CNode of the peer\n+     * @param[in]   peer      The peer sending us the headers\n+     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.",
      "path": "src/net_processing.cpp",
      "position": 54,
      "original_position": 30,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953741265,
      "user": {
        "login": "vasild",
        "id": 266751,
        "node_id": "MDQ6VXNlcjI2Njc1MQ==",
        "avatar_url": "https://avatars.githubusercontent.com/u/266751?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/vasild",
        "html_url": "https://github.com/vasild",
        "followers_url": "https://api.github.com/users/vasild/followers",
        "following_url": "https://api.github.com/users/vasild/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/vasild/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/vasild/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/vasild/subscriptions",
        "organizations_url": "https://api.github.com/users/vasild/orgs",
        "repos_url": "https://api.github.com/users/vasild/repos",
        "events_url": "https://api.github.com/users/vasild/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/vasild/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Why modify it if you don't want the caller to use it afterwards?\r\n\r\n~~Is it only modified by the `headers.swap(result.pow_validated_headers)` in `IsContinuationOfLowWorkHeadersSync()` and all callers ignore the result? If yes, then that `swap()` can be removed and the parameters made `const`?~~\r\n\r\nEdit: the callers of `IsContinuationOfLowWorkHeadersSync()` don't ignore the result, I was confused, sorry. Maybe document what is expected as `in` and what is returned as `out` for the `headers` parameter in both `PeerManagerImpl::ProcessHeadersMessage()` and `IsContinuationOfLowWorkHeadersSync()`.",
      "created_at": "2022-08-25T15:27:49Z",
      "updated_at": "2022-08-25T15:35:49Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955113786",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955113786"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 598,
      "original_line": 598,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955115583",
      "pull_request_review_id": 1085670734,
      "id": 955115583,
      "node_id": "PRRC_kwDOABII58447eg_",
      "diff_hunk": "@@ -581,18 +588,59 @@ class PeerManagerImpl final : public PeerManager\n \n     void ProcessOrphanTx(std::set<uint256>& orphan_work_set) EXCLUSIVE_LOCKS_REQUIRED(cs_main, g_cs_orphans)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n-    /** Process a single headers message from a peer. */\n+    /** Process a single headers message from a peer.\n+     *\n+     * @param[in]   pfrom     CNode of the peer\n+     * @param[in]   peer      The peer sending us the headers\n+     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.",
      "path": "src/net_processing.cpp",
      "position": 54,
      "original_position": 30,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953741265,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "No, `ProcessHeadersMessage` continues operating on the headers variable.",
      "created_at": "2022-08-25T15:29:15Z",
      "updated_at": "2022-08-25T15:29:15Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955115583",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955115583"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 598,
      "original_line": 598,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955149920",
      "pull_request_review_id": 1085720234,
      "id": 955149920,
      "node_id": "PRRC_kwDOABII58447m5g",
      "diff_hunk": "@@ -581,18 +588,59 @@ class PeerManagerImpl final : public PeerManager\n \n     void ProcessOrphanTx(std::set<uint256>& orphan_work_set) EXCLUSIVE_LOCKS_REQUIRED(cs_main, g_cs_orphans)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n-    /** Process a single headers message from a peer. */\n+    /** Process a single headers message from a peer.\n+     *\n+     * @param[in]   pfrom     CNode of the peer\n+     * @param[in]   peer      The peer sending us the headers\n+     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.\n+     * @param[in]   via_compact_block   Whether this header came in via compact block handling.\n+    */\n     void ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n-                               const std::vector<CBlockHeader>& headers,\n+                               std::vector<CBlockHeader>& headers,\n                                bool via_compact_block)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n     /** Various helpers for headers processing, invoked by ProcessHeadersMessage() */\n+    /** Return true if headers are continuous and have valid proof-of-work */\n+    bool CheckHeadersPoW(const std::vector<CBlockHeader>& headers, const Consensus::Params& consensusParams, Peer& peer);\n+    /** Calculate an anti-DoS work threshold for headers chains */\n+    arith_uint256 GetAntiDoSWorkThreshold();\n     /** Deal with state tracking and headers sync for peers that send the\n      * occasional non-connecting header (this can happen due to BIP 130 headers\n      * announcements for blocks interacting with the 2hr (MAX_FUTURE_BLOCK_TIME) rule). */\n     void HandleFewUnconnectingHeaders(CNode& pfrom, Peer& peer, const std::vector<CBlockHeader>& headers);\n     /** Return true if the headers connect to each other, false otherwise */\n     bool CheckHeadersAreContinuous(const std::vector<CBlockHeader>& headers) const;\n+    /** Try to continue a low-work headers sync that has already begun.\n+     *  @param[in]  peer                            The peer we're syncing with.\n+     *  @param[in]  pfrom                           CNode of the peer\n+     *  @param[in,out] headers                      The headers to be processed.",
      "path": "src/net_processing.cpp",
      "position": 80,
      "original_position": 52,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953820788,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "FWIW, here is a commit that pushes things down further, though I'm going to keep that for potential follow-up PR: https://github.com/sipa/bitcoin/commit/95422ec7ecca5acc427d511b8716bebeed7e4b0a",
      "created_at": "2022-08-25T15:56:45Z",
      "updated_at": "2022-08-25T15:56:45Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955149920",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955149920"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 622,
      "original_line": 622,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955274344",
      "pull_request_review_id": 1085892378,
      "id": 955274344,
      "node_id": "PRRC_kwDOABII58448FRo",
      "diff_hunk": "@@ -0,0 +1,267 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** PRESYNC means the peer has not yet demonstrated their chain has\n+         * sufficient work and we're only building commitments to the chain they\n+         * serve us. */\n+        PRESYNC,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Construct a HeadersSyncState object representing a headers sync via this\n+     *  download-twice mechanism).\n+     *\n+     * id: node id (for logging)\n+     * consensus_params: parameters needed for difficulty adjustment validation\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+            const CBlockIndex* chain_start, const arith_uint256& minimum_required_work);\n+\n+    /** Result data structure for ProcessNextHeaders. */\n+    struct ProcessingResult {\n+        std::vector<CBlockHeader> pow_validated_headers;\n+        bool success{false};\n+        bool request_more{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * received_headers: headers that were received over the network for processing.\n+     *                   Assumes the caller has already verified the headers connect,",
      "path": "src/headerssync.h",
      "position": null,
      "original_position": 142,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953766769,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Ah yes, fixed the comment in `headerssync.h` to use the term \"continuous\" rather than \"connect\".",
      "created_at": "2022-08-25T18:06:57Z",
      "updated_at": "2022-08-25T18:06:58Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955274344",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955274344"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 151,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955277200",
      "pull_request_review_id": 1085896440,
      "id": 955277200,
      "node_id": "PRRC_kwDOABII58448F-Q",
      "diff_hunk": "@@ -581,18 +588,59 @@ class PeerManagerImpl final : public PeerManager\n \n     void ProcessOrphanTx(std::set<uint256>& orphan_work_set) EXCLUSIVE_LOCKS_REQUIRED(cs_main, g_cs_orphans)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n-    /** Process a single headers message from a peer. */\n+    /** Process a single headers message from a peer.\n+     *\n+     * @param[in]   pfrom     CNode of the peer\n+     * @param[in]   peer      The peer sending us the headers\n+     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.",
      "path": "src/net_processing.cpp",
      "position": 54,
      "original_position": 30,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953741265,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "@vasild For `IsContinuationOfLowWorkHeadersSync`, the comment currently says:\r\n```\r\n     *  @param[in,out] headers                      The headers to be processed.\r\n     *  @return     True if the passed in headers were successfully processed\r\n     *              as the continuation of a low-work headers sync in progress;\r\n     *              false otherwise.\r\n     *              If false, the passed in headers will be returned back to\r\n     *              the caller.\r\n     *              If true, the returned headers may be empty, indicating\r\n     *              there is no more work for the caller to do; or the headers\r\n     *              may be populated with entries that have passed anti-DoS\r\n     *              checks (and therefore may be validated for block index\r\n     *              acceptance by the caller).\r\n```\r\n\r\nFor `ProcessHeadersMessage`, the comment reads:\r\n\r\n```\r\n     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.\r\n```\r\n\r\nCan you clarify what documentation you think is missing?",
      "created_at": "2022-08-25T18:10:36Z",
      "updated_at": "2022-08-25T18:10:37Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955277200",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955277200"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 598,
      "original_line": 598,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955399730",
      "pull_request_review_id": 1086079463,
      "id": 955399730,
      "node_id": "PRRC_kwDOABII58448j4y",
      "diff_hunk": "@@ -581,18 +588,59 @@ class PeerManagerImpl final : public PeerManager\n \n     void ProcessOrphanTx(std::set<uint256>& orphan_work_set) EXCLUSIVE_LOCKS_REQUIRED(cs_main, g_cs_orphans)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n-    /** Process a single headers message from a peer. */\n+    /** Process a single headers message from a peer.\n+     *\n+     * @param[in]   pfrom     CNode of the peer\n+     * @param[in]   peer      The peer sending us the headers\n+     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.",
      "path": "src/net_processing.cpp",
      "position": 54,
      "original_position": 30,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953741265,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "The `headers` argument to `ProcessHeadersMessage` should probably be of type `std::vector<CBlockHeader>&&`, making the caller indicate that the referenced vector isn't going to be used anymore. Callers can pass something in with `std::move(headers)`. The compact block call site can turn into `return ProcessHeadersMessage(pfrom, *peer, {cmpctblock.header}, /*via_compact_block=*/true);` then even, without temporary `headers` variable.",
      "created_at": "2022-08-25T20:36:27Z",
      "updated_at": "2022-08-25T21:18:38Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955399730",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955399730"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 598,
      "original_line": 598,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955402901",
      "pull_request_review_id": 1086079463,
      "id": 955402901,
      "node_id": "PRRC_kwDOABII58448kqV",
      "diff_hunk": "@@ -2316,6 +2404,107 @@ bool PeerManagerImpl::CheckHeadersAreContinuous(const std::vector<CBlockHeader>&\n     return true;\n }\n \n+bool PeerManagerImpl::IsContinuationOfLowWorkHeadersSync(Peer& peer, CNode& pfrom, std::vector<CBlockHeader>& headers)\n+{\n+    if (peer.m_headers_sync) {\n+        auto result = peer.m_headers_sync->ProcessNextHeaders(headers, headers.size() == MAX_HEADERS_RESULTS);\n+        if (result.request_more) {\n+            auto locator = peer.m_headers_sync->NextHeadersRequestLocator();\n+            // If we were instructed to ask for a locator, it should not be empty.\n+            Assume(!locator.vHave.empty());\n+            if (!locator.vHave.empty()) {\n+                // It should be impossible for the getheaders request to fail,\n+                // because we should have cleared the last getheaders timestamp\n+                // when processing the headers that triggered this call. But\n+                // it may be possible to bypass this via compactblock\n+                // processing, so check the result before logging just to be\n+                // safe.\n+                bool sent_getheaders = MaybeSendGetHeaders(pfrom, locator, peer);\n+                if (sent_getheaders) {\n+                    LogPrint(BCLog::NET, \"more getheaders (from %s) to peer=%d\\n\",\n+                            locator.vHave.front().ToString(), pfrom.GetId());\n+                } else {\n+                    LogPrint(BCLog::NET, \"error sending next getheaders (from %s) to continue sync with peer=%d\\n\",\n+                            locator.vHave.front().ToString(), pfrom.GetId());\n+                }\n+            }\n+        }\n+\n+        if (peer.m_headers_sync->GetState() == HeadersSyncState::State::FINAL) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }\n+\n+        if (result.success) {\n+            // We only overwrite the headers passed in if processing was\n+            // successful.\n+            headers.swap(result.pow_validated_headers);\n+        }\n+\n+        return result.success;\n+    }\n+    // Either we didn't have a sync in progress, or something went wrong\n+    // processing these headers, or we are returning headers to the caller to\n+    // process.\n+    return false;\n+}\n+\n+bool PeerManagerImpl::TryLowWorkHeadersSync(Peer& peer, CNode& pfrom, const CBlockIndex* chain_start_header, std::vector<CBlockHeader>& headers)\n+{\n+    // Calculate the total work on this chain.\n+    arith_uint256 total_work = chain_start_header->nChainWork + CalculateHeadersWork(headers);\n+\n+    // Our dynamic anti-DoS threshold (minimum work required on a headers chain\n+    // before we'll store it)\n+    arith_uint256 minimum_chain_work = GetAntiDoSWorkThreshold();\n+\n+    // Avoid DoS via low-difficulty-headers by only processing if the headers\n+    // are part of a chain with sufficient work.\n+    if (total_work < minimum_chain_work) {\n+        // Only try to sync with this peer if their headers message was full;\n+        // otherwise they don't have more headers after this so no point in\n+        // trying to sync their too-little-work chain.\n+        if (headers.size() == MAX_HEADERS_RESULTS) {\n+            // Note: we could advance to the last header in this set that is\n+            // known to us, rather than starting at the first header (which we\n+            // may already have). But we actually might have all the headers in\n+            // this set already, because headers sync can be imprecise when a\n+            // peer has to serve us a long chain (due to imprecision in the way\n+            // locators are calculated). So philosophically, if we wanted to\n+            // optimize this correctly, we could consider requesting more\n+            // headers until we find the peer's first new header on this chain,\n+            // and start the sync from there. In practice this is unlikely to\n+            // matter much outside of initial sync, which we generally only do\n+            // once, so for now we'll just start the sync with the first header\n+            // in the set and not worry about this issue.\n+            LOCK(peer.m_headers_sync_mutex);\n+            peer.m_headers_sync.reset(new HeadersSyncState(peer.m_id, m_chainparams.GetConsensus(),\n+                chain_start_header, minimum_chain_work));\n+\n+            // Now a HeadersSyncState object for tracking this synchronization is created,\n+            // process the headers using it as normal.\n+            return IsContinuationOfLowWorkHeadersSync(peer, pfrom, headers);\n+        } else {\n+            LogPrint(BCLog::NET, \"Ignoring low-work chain (height=%u) from peer=%d\\n\", chain_start_header->nHeight + headers.size(), pfrom.GetId());\n+            // Since this is a low-work headers chain, no further processing is required.\n+            std::vector<CBlockHeader>().swap(headers);",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 216,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "0f24c9a9248a564d88bd5ecefdb63426f6712558",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Could use `headers = {};` (syntactic sugar for `headers = std::vector<CBlockHeader>{};`).",
      "created_at": "2022-08-25T20:38:39Z",
      "updated_at": "2022-08-25T21:22:48Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955402901",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955402901"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 2489,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955403369",
      "pull_request_review_id": 1086079463,
      "id": 955403369,
      "node_id": "PRRC_kwDOABII58448kxp",
      "diff_hunk": "@@ -0,0 +1,317 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959}; // 13959/584 = ~23.9 commitments\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_redownloaded_headers = std::deque<CompressedHeader>{};",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 56,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "0f24c9a9248a564d88bd5ecefdb63426f6712558",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Could even use `m_redownloaded_headers = {};`.",
      "created_at": "2022-08-25T20:39:19Z",
      "updated_at": "2022-08-25T21:18:38Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955403369",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955403369"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 56,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955404314",
      "pull_request_review_id": 1086079463,
      "id": 955404314,
      "node_id": "PRRC_kwDOABII58448lAa",
      "diff_hunk": "@@ -0,0 +1,317 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959}; // 13959/584 = ~23.9 commitments\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 54,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "0f24c9a9248a564d88bd5ecefdb63426f6712558",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "If you want to guarantee memory is released here, use `m_headers_commitments = {};` here as well.",
      "created_at": "2022-08-25T20:40:41Z",
      "updated_at": "2022-08-25T21:18:38Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955404314",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955404314"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 54,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955408583",
      "pull_request_review_id": 1086079463,
      "id": 955408583,
      "node_id": "PRRC_kwDOABII58448mDH",
      "diff_hunk": "@@ -2316,6 +2442,149 @@ bool PeerManagerImpl::CheckHeadersAreContinuous(const std::vector<CBlockHeader>&\n     return true;\n }\n \n+bool PeerManagerImpl::IsContinuationOfLowWorkHeadersSync(Peer& peer, CNode& pfrom, std::vector<CBlockHeader>& headers)\n+{\n+    if (peer.m_headers_sync) {\n+        auto result = peer.m_headers_sync->ProcessNextHeaders(headers, headers.size() == MAX_HEADERS_RESULTS);\n+        if (result.request_more) {\n+            auto locator = peer.m_headers_sync->NextHeadersRequestLocator();\n+            // If we were instructed to ask for a locator, it should not be empty.\n+            Assume(!locator.vHave.empty());\n+            if (!locator.vHave.empty()) {\n+                // It should be impossible for the getheaders request to fail,\n+                // because we should have cleared the last getheaders timestamp\n+                // when processing the headers that triggered this call. But\n+                // it may be possible to bypass this via compactblock\n+                // processing, so check the result before logging just to be\n+                // safe.\n+                bool sent_getheaders = MaybeSendGetHeaders(pfrom, locator, peer);\n+                if (sent_getheaders) {\n+                    LogPrint(BCLog::NET, \"more getheaders (from %s) to peer=%d\\n\",\n+                            locator.vHave.front().ToString(), pfrom.GetId());\n+                } else {\n+                    LogPrint(BCLog::NET, \"error sending next getheaders (from %s) to continue sync with peer=%d\\n\",\n+                            locator.vHave.front().ToString(), pfrom.GetId());\n+                }\n+            }\n+        }\n+\n+        if (peer.m_headers_sync->GetState() == HeadersSyncState::State::FINAL) {\n+            peer.m_headers_sync.reset(nullptr);\n+\n+            // Delete this peer's entry in m_headers_presync_stats.\n+            // If this is m_headers_presync_bestpeer, it will be replaced later\n+            // by the next peer that triggers the else{} branch below.\n+            LOCK(m_headers_presync_mutex);\n+            m_headers_presync_stats.erase(pfrom.GetId());\n+        } else {\n+            // Build statistics for this peer's sync.\n+            HeadersPresyncStats stats;\n+            stats.first = peer.m_headers_sync->GetPresyncWork();\n+            if (peer.m_headers_sync->GetState() == HeadersSyncState::State::PRESYNC) {\n+                stats.second = {peer.m_headers_sync->GetPresyncHeight(),\n+                                peer.m_headers_sync->GetPresyncTime()};\n+            }\n+\n+            // Update statistics in stats.\n+            LOCK(m_headers_presync_mutex);\n+            m_headers_presync_stats[pfrom.GetId()] = stats;\n+            auto best_it = m_headers_presync_stats.find(m_headers_presync_bestpeer);\n+            bool best_updated = false;\n+            if (best_it == m_headers_presync_stats.end()) {\n+                // If the cached best peer is outdated, iterate over all remaining ones (including\n+                // newly updated one) to find the best one.\n+                NodeId peer_best{-1};\n+                const HeadersPresyncStats* stat_best{nullptr};\n+                for (const auto& [peer, stat] : m_headers_presync_stats) {\n+                    if (!stat_best || stat > *stat_best) {\n+                        peer_best = peer;\n+                        stat_best = &stat;\n+                    }\n+                }\n+                m_headers_presync_bestpeer = peer_best;\n+                best_updated = (peer_best == pfrom.GetId());\n+            } else if (best_it->first == pfrom.GetId() || stats > best_it->second) {\n+                // pfrom was and remains the best peer, or pfrom just became best.\n+                m_headers_presync_bestpeer = pfrom.GetId();\n+                best_updated = true;\n+            }\n+            if (best_updated && stats.second.has_value()) {\n+                // If the best peer updated, and it is in its first phase, signal.\n+                m_headers_presync_should_signal = true;\n+            }\n+        }\n+\n+        if (result.success) {\n+            // We only overwrite the headers passed in if processing was\n+            // successful.\n+            headers.swap(result.pow_validated_headers);\n+        }\n+\n+        return result.success;\n+    }\n+    // Either we didn't have a sync in progress, or something went wrong\n+    // processing these headers, or we are returning headers to the caller to\n+    // process.\n+    return false;\n+}\n+\n+bool PeerManagerImpl::TryLowWorkHeadersSync(Peer& peer, CNode& pfrom, const CBlockIndex* chain_start_header, std::vector<CBlockHeader>& headers)\n+{\n+    // Calculate the total work on this chain.\n+    arith_uint256 total_work = chain_start_header->nChainWork + CalculateHeadersWork(headers);\n+\n+    // Our dynamic anti-DoS threshold (minimum work required on a headers chain\n+    // before we'll store it)\n+    arith_uint256 minimum_chain_work = GetAntiDoSWorkThreshold();\n+\n+    // Avoid DoS via low-difficulty-headers by only processing if the headers\n+    // are part of a chain with sufficient work.\n+    if (total_work < minimum_chain_work) {\n+        // Only try to sync with this peer if their headers message was full;\n+        // otherwise they don't have more headers after this so no point in\n+        // trying to sync their too-little-work chain.\n+        if (headers.size() == MAX_HEADERS_RESULTS) {\n+            // Note: we could advance to the last header in this set that is\n+            // known to us, rather than starting at the first header (which we\n+            // may already have). But we actually might have all the headers in\n+            // this set already, because headers sync can be imprecise when a\n+            // peer has to serve us a long chain (due to imprecision in the way\n+            // locators are calculated). So philosophically, if we wanted to",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 354,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "bae408fc87b3111cf93d26db66f1947430b04f34",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think this comment is outdated now. We're actually not starting low-work headers sync until the peer gives a headers message which contains at least one header not in m_best_header/m_active_chain already.",
      "created_at": "2022-08-25T20:46:46Z",
      "updated_at": "2022-08-25T21:18:38Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955408583",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955408583"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 2552,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955426335",
      "pull_request_review_id": 1086079463,
      "id": 955426335,
      "node_id": "PRRC_kwDOABII58448qYf",
      "diff_hunk": "@@ -991,10 +1004,11 @@ class ChainstateManager\n      *\n      * @param[in]   block The block we want to process.\n      * @param[in]   force_processing Process this block even if unrequested; used for non-network block sources.\n+     * @param[in]   min_pow_checked  True if proof-of-work anti-DoS checks have been done by caller for headers chain",
      "path": "src/validation.h",
      "position": null,
      "original_position": 48,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "bae408fc87b3111cf93d26db66f1947430b04f34",
      "in_reply_to_id": null,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Maybe point out that this only affects situations where a new header is being added along with the block. If we already had the header, this parameter has no effect.",
      "created_at": "2022-08-25T21:09:55Z",
      "updated_at": "2022-08-25T21:18:38Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955426335",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955426335"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 1007,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955511265",
      "pull_request_review_id": 1086229827,
      "id": 955511265,
      "node_id": "PRRC_kwDOABII58448_Hh",
      "diff_hunk": "@@ -581,18 +588,59 @@ class PeerManagerImpl final : public PeerManager\n \n     void ProcessOrphanTx(std::set<uint256>& orphan_work_set) EXCLUSIVE_LOCKS_REQUIRED(cs_main, g_cs_orphans)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n-    /** Process a single headers message from a peer. */\n+    /** Process a single headers message from a peer.\n+     *\n+     * @param[in]   pfrom     CNode of the peer\n+     * @param[in]   peer      The peer sending us the headers\n+     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.",
      "path": "src/net_processing.cpp",
      "position": 54,
      "original_position": 30,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953741265,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-25T23:58:18Z",
      "updated_at": "2022-08-25T23:58:18Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955511265",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955511265"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 598,
      "original_line": 598,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955511503",
      "pull_request_review_id": 1086230144,
      "id": 955511503,
      "node_id": "PRRC_kwDOABII58448_LP",
      "diff_hunk": "@@ -2316,6 +2404,107 @@ bool PeerManagerImpl::CheckHeadersAreContinuous(const std::vector<CBlockHeader>&\n     return true;\n }\n \n+bool PeerManagerImpl::IsContinuationOfLowWorkHeadersSync(Peer& peer, CNode& pfrom, std::vector<CBlockHeader>& headers)\n+{\n+    if (peer.m_headers_sync) {\n+        auto result = peer.m_headers_sync->ProcessNextHeaders(headers, headers.size() == MAX_HEADERS_RESULTS);\n+        if (result.request_more) {\n+            auto locator = peer.m_headers_sync->NextHeadersRequestLocator();\n+            // If we were instructed to ask for a locator, it should not be empty.\n+            Assume(!locator.vHave.empty());\n+            if (!locator.vHave.empty()) {\n+                // It should be impossible for the getheaders request to fail,\n+                // because we should have cleared the last getheaders timestamp\n+                // when processing the headers that triggered this call. But\n+                // it may be possible to bypass this via compactblock\n+                // processing, so check the result before logging just to be\n+                // safe.\n+                bool sent_getheaders = MaybeSendGetHeaders(pfrom, locator, peer);\n+                if (sent_getheaders) {\n+                    LogPrint(BCLog::NET, \"more getheaders (from %s) to peer=%d\\n\",\n+                            locator.vHave.front().ToString(), pfrom.GetId());\n+                } else {\n+                    LogPrint(BCLog::NET, \"error sending next getheaders (from %s) to continue sync with peer=%d\\n\",\n+                            locator.vHave.front().ToString(), pfrom.GetId());\n+                }\n+            }\n+        }\n+\n+        if (peer.m_headers_sync->GetState() == HeadersSyncState::State::FINAL) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }\n+\n+        if (result.success) {\n+            // We only overwrite the headers passed in if processing was\n+            // successful.\n+            headers.swap(result.pow_validated_headers);\n+        }\n+\n+        return result.success;\n+    }\n+    // Either we didn't have a sync in progress, or something went wrong\n+    // processing these headers, or we are returning headers to the caller to\n+    // process.\n+    return false;\n+}\n+\n+bool PeerManagerImpl::TryLowWorkHeadersSync(Peer& peer, CNode& pfrom, const CBlockIndex* chain_start_header, std::vector<CBlockHeader>& headers)\n+{\n+    // Calculate the total work on this chain.\n+    arith_uint256 total_work = chain_start_header->nChainWork + CalculateHeadersWork(headers);\n+\n+    // Our dynamic anti-DoS threshold (minimum work required on a headers chain\n+    // before we'll store it)\n+    arith_uint256 minimum_chain_work = GetAntiDoSWorkThreshold();\n+\n+    // Avoid DoS via low-difficulty-headers by only processing if the headers\n+    // are part of a chain with sufficient work.\n+    if (total_work < minimum_chain_work) {\n+        // Only try to sync with this peer if their headers message was full;\n+        // otherwise they don't have more headers after this so no point in\n+        // trying to sync their too-little-work chain.\n+        if (headers.size() == MAX_HEADERS_RESULTS) {\n+            // Note: we could advance to the last header in this set that is\n+            // known to us, rather than starting at the first header (which we\n+            // may already have). But we actually might have all the headers in\n+            // this set already, because headers sync can be imprecise when a\n+            // peer has to serve us a long chain (due to imprecision in the way\n+            // locators are calculated). So philosophically, if we wanted to\n+            // optimize this correctly, we could consider requesting more\n+            // headers until we find the peer's first new header on this chain,\n+            // and start the sync from there. In practice this is unlikely to\n+            // matter much outside of initial sync, which we generally only do\n+            // once, so for now we'll just start the sync with the first header\n+            // in the set and not worry about this issue.\n+            LOCK(peer.m_headers_sync_mutex);\n+            peer.m_headers_sync.reset(new HeadersSyncState(peer.m_id, m_chainparams.GetConsensus(),\n+                chain_start_header, minimum_chain_work));\n+\n+            // Now a HeadersSyncState object for tracking this synchronization is created,\n+            // process the headers using it as normal.\n+            return IsContinuationOfLowWorkHeadersSync(peer, pfrom, headers);\n+        } else {\n+            LogPrint(BCLog::NET, \"Ignoring low-work chain (height=%u) from peer=%d\\n\", chain_start_header->nHeight + headers.size(), pfrom.GetId());\n+            // Since this is a low-work headers chain, no further processing is required.\n+            std::vector<CBlockHeader>().swap(headers);",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 216,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "0f24c9a9248a564d88bd5ecefdb63426f6712558",
      "in_reply_to_id": 955402901,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-25T23:59:04Z",
      "updated_at": "2022-08-25T23:59:04Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955511503",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955511503"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 2489,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955511542",
      "pull_request_review_id": 1086230180,
      "id": 955511542,
      "node_id": "PRRC_kwDOABII58448_L2",
      "diff_hunk": "@@ -0,0 +1,317 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959}; // 13959/584 = ~23.9 commitments\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();\n+    m_last_header_received.SetNull();\n+    m_redownloaded_headers = std::deque<CompressedHeader>{};",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 56,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "0f24c9a9248a564d88bd5ecefdb63426f6712558",
      "in_reply_to_id": 955403369,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-25T23:59:09Z",
      "updated_at": "2022-08-25T23:59:10Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955511542",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955511542"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 56,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955511570",
      "pull_request_review_id": 1086230215,
      "id": 955511570,
      "node_id": "PRRC_kwDOABII58448_MS",
      "diff_hunk": "@@ -0,0 +1,317 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959}; // 13959/584 = ~23.9 commitments\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments.clear();",
      "path": "src/headerssync.cpp",
      "position": null,
      "original_position": 54,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "0f24c9a9248a564d88bd5ecefdb63426f6712558",
      "in_reply_to_id": 955404314,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-25T23:59:15Z",
      "updated_at": "2022-08-25T23:59:15Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955511570",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955511570"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 54,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955511607",
      "pull_request_review_id": 1086230265,
      "id": 955511607,
      "node_id": "PRRC_kwDOABII58448_M3",
      "diff_hunk": "@@ -991,10 +1004,11 @@ class ChainstateManager\n      *\n      * @param[in]   block The block we want to process.\n      * @param[in]   force_processing Process this block even if unrequested; used for non-network block sources.\n+     * @param[in]   min_pow_checked  True if proof-of-work anti-DoS checks have been done by caller for headers chain",
      "path": "src/validation.h",
      "position": null,
      "original_position": 48,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "bae408fc87b3111cf93d26db66f1947430b04f34",
      "in_reply_to_id": 955426335,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done.",
      "created_at": "2022-08-25T23:59:22Z",
      "updated_at": "2022-08-25T23:59:22Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955511607",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955511607"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 1007,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955514635",
      "pull_request_review_id": 1086233954,
      "id": 955514635,
      "node_id": "PRRC_kwDOABII58448_8L",
      "diff_hunk": "@@ -2316,6 +2442,149 @@ bool PeerManagerImpl::CheckHeadersAreContinuous(const std::vector<CBlockHeader>&\n     return true;\n }\n \n+bool PeerManagerImpl::IsContinuationOfLowWorkHeadersSync(Peer& peer, CNode& pfrom, std::vector<CBlockHeader>& headers)\n+{\n+    if (peer.m_headers_sync) {\n+        auto result = peer.m_headers_sync->ProcessNextHeaders(headers, headers.size() == MAX_HEADERS_RESULTS);\n+        if (result.request_more) {\n+            auto locator = peer.m_headers_sync->NextHeadersRequestLocator();\n+            // If we were instructed to ask for a locator, it should not be empty.\n+            Assume(!locator.vHave.empty());\n+            if (!locator.vHave.empty()) {\n+                // It should be impossible for the getheaders request to fail,\n+                // because we should have cleared the last getheaders timestamp\n+                // when processing the headers that triggered this call. But\n+                // it may be possible to bypass this via compactblock\n+                // processing, so check the result before logging just to be\n+                // safe.\n+                bool sent_getheaders = MaybeSendGetHeaders(pfrom, locator, peer);\n+                if (sent_getheaders) {\n+                    LogPrint(BCLog::NET, \"more getheaders (from %s) to peer=%d\\n\",\n+                            locator.vHave.front().ToString(), pfrom.GetId());\n+                } else {\n+                    LogPrint(BCLog::NET, \"error sending next getheaders (from %s) to continue sync with peer=%d\\n\",\n+                            locator.vHave.front().ToString(), pfrom.GetId());\n+                }\n+            }\n+        }\n+\n+        if (peer.m_headers_sync->GetState() == HeadersSyncState::State::FINAL) {\n+            peer.m_headers_sync.reset(nullptr);\n+\n+            // Delete this peer's entry in m_headers_presync_stats.\n+            // If this is m_headers_presync_bestpeer, it will be replaced later\n+            // by the next peer that triggers the else{} branch below.\n+            LOCK(m_headers_presync_mutex);\n+            m_headers_presync_stats.erase(pfrom.GetId());\n+        } else {\n+            // Build statistics for this peer's sync.\n+            HeadersPresyncStats stats;\n+            stats.first = peer.m_headers_sync->GetPresyncWork();\n+            if (peer.m_headers_sync->GetState() == HeadersSyncState::State::PRESYNC) {\n+                stats.second = {peer.m_headers_sync->GetPresyncHeight(),\n+                                peer.m_headers_sync->GetPresyncTime()};\n+            }\n+\n+            // Update statistics in stats.\n+            LOCK(m_headers_presync_mutex);\n+            m_headers_presync_stats[pfrom.GetId()] = stats;\n+            auto best_it = m_headers_presync_stats.find(m_headers_presync_bestpeer);\n+            bool best_updated = false;\n+            if (best_it == m_headers_presync_stats.end()) {\n+                // If the cached best peer is outdated, iterate over all remaining ones (including\n+                // newly updated one) to find the best one.\n+                NodeId peer_best{-1};\n+                const HeadersPresyncStats* stat_best{nullptr};\n+                for (const auto& [peer, stat] : m_headers_presync_stats) {\n+                    if (!stat_best || stat > *stat_best) {\n+                        peer_best = peer;\n+                        stat_best = &stat;\n+                    }\n+                }\n+                m_headers_presync_bestpeer = peer_best;\n+                best_updated = (peer_best == pfrom.GetId());\n+            } else if (best_it->first == pfrom.GetId() || stats > best_it->second) {\n+                // pfrom was and remains the best peer, or pfrom just became best.\n+                m_headers_presync_bestpeer = pfrom.GetId();\n+                best_updated = true;\n+            }\n+            if (best_updated && stats.second.has_value()) {\n+                // If the best peer updated, and it is in its first phase, signal.\n+                m_headers_presync_should_signal = true;\n+            }\n+        }\n+\n+        if (result.success) {\n+            // We only overwrite the headers passed in if processing was\n+            // successful.\n+            headers.swap(result.pow_validated_headers);\n+        }\n+\n+        return result.success;\n+    }\n+    // Either we didn't have a sync in progress, or something went wrong\n+    // processing these headers, or we are returning headers to the caller to\n+    // process.\n+    return false;\n+}\n+\n+bool PeerManagerImpl::TryLowWorkHeadersSync(Peer& peer, CNode& pfrom, const CBlockIndex* chain_start_header, std::vector<CBlockHeader>& headers)\n+{\n+    // Calculate the total work on this chain.\n+    arith_uint256 total_work = chain_start_header->nChainWork + CalculateHeadersWork(headers);\n+\n+    // Our dynamic anti-DoS threshold (minimum work required on a headers chain\n+    // before we'll store it)\n+    arith_uint256 minimum_chain_work = GetAntiDoSWorkThreshold();\n+\n+    // Avoid DoS via low-difficulty-headers by only processing if the headers\n+    // are part of a chain with sufficient work.\n+    if (total_work < minimum_chain_work) {\n+        // Only try to sync with this peer if their headers message was full;\n+        // otherwise they don't have more headers after this so no point in\n+        // trying to sync their too-little-work chain.\n+        if (headers.size() == MAX_HEADERS_RESULTS) {\n+            // Note: we could advance to the last header in this set that is\n+            // known to us, rather than starting at the first header (which we\n+            // may already have). But we actually might have all the headers in\n+            // this set already, because headers sync can be imprecise when a\n+            // peer has to serve us a long chain (due to imprecision in the way\n+            // locators are calculated). So philosophically, if we wanted to",
      "path": "src/net_processing.cpp",
      "position": null,
      "original_position": 354,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "bae408fc87b3111cf93d26db66f1947430b04f34",
      "in_reply_to_id": 955408583,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Thanks for catching; I updated this comment.",
      "created_at": "2022-08-26T00:07:09Z",
      "updated_at": "2022-08-26T00:07:10Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955514635",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955514635"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 2552,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955657894",
      "pull_request_review_id": 1086408327,
      "id": 955657894,
      "node_id": "PRRC_kwDOABII58449i6m",
      "diff_hunk": "@@ -65,6 +65,7 @@ namespace BCLog {\n #endif\n         UTIL        = (1 << 25),\n         BLOCKSTORE  = (1 << 26),\n+        HEADERSSYNC = (1 << 27),",
      "path": "src/logging.h",
      "position": 4,
      "original_position": 4,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "9a0d196a9cd30db5e99a50b900f99f66a09a72dc",
      "in_reply_to_id": null,
      "user": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Doing a testnet sync, the headerssync logs look like:\r\n\r\n```\r\n2022-08-26T04:50:14Z [headerssync] Initial headers sync started with peer=0: height=0, max_commitments=3748027, min_work=00000000000000000000000000000000000000000000064728c7be6fe4b2f961\r\n2022-08-26T04:52:45Z [headerssync] Initial headers sync started with peer=3: height=0, max_commitments=3748029, min_work=00000000000000000000000000000000000000000000064728c7be6fe4b2f961\r\n2022-08-26T04:56:48Z [headerssync] Initial headers sync transition with peer=0: reached sufficient work at height=2144000, redownloading from height=0\r\n2022-08-26T04:58:35Z [headerssync] Initial headers sync transition with peer=3: reached sufficient work at height=2144000, redownloading from height=0\r\n2022-08-26T05:02:25Z [headerssync] Initial headers sync complete with peer=0: releasing all at height=2144000 (redownload phase)\r\n```\r\n\r\nwhich isn't very spammy, so I think it would make sense to merge this back into `BCLog::NET` in a followup. (The main rationale being that if you're trying to debug weird p2p behaviour, better to see what's going on with header syncing in case that happens to be the cause, without having to realise the possiblity after the fact then start debugging again later when after adding the extra debug category)",
      "created_at": "2022-08-26T05:18:08Z",
      "updated_at": "2022-08-26T15:52:48Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955657894",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955657894"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 68,
      "original_line": 68,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955704311",
      "pull_request_review_id": 1086408327,
      "id": 955704311,
      "node_id": "PRRC_kwDOABII58449uP3",
      "diff_hunk": "@@ -2461,21 +2647,71 @@ void PeerManagerImpl::UpdatePeerStateForReceivedHeaders(CNode& pfrom,\n }\n \n void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n-                                            const std::vector<CBlockHeader>& headers,\n+                                            std::vector<CBlockHeader>&& headers,\n                                             bool via_compact_block)\n {\n-    const CNetMsgMaker msgMaker(pfrom.GetCommonVersion());\n     size_t nCount = headers.size();\n \n     if (nCount == 0) {\n         // Nothing interesting. Stop asking this peers for more headers.\n+        // If we were in the middle of headers sync, receiving an empty headers\n+        // message suggests that the peer suddenly has nothing to give us\n+        // (perhaps it reorged to our chain). Clear download state for this peer.\n+        LOCK(peer.m_headers_sync_mutex);\n+        if (peer.m_headers_sync) {\n+            peer.m_headers_sync.reset(nullptr);\n+        }\n+        return;\n+    }\n+\n+    // Before we do any processing, make sure these pass basic sanity checks.\n+    // We'll rely on headers having valid proof-of-work further down, as an\n+    // anti-DoS criteria (note: this check is required before passing any\n+    // headers into HeadersSyncState).\n+    if (!CheckHeadersPoW(headers, m_chainparams.GetConsensus(), peer)) {\n+        // Note that even if a header is announced via compact block, the\n+        // header itself should be valid, so this type of error can always be\n+        // punished.\n+        Misbehaving(peer, 100, \"invalid proof-of-work\");\n         return;\n     }\n \n     const CBlockIndex *pindexLast = nullptr;\n \n+    // We'll set already_validated_work to true if these headers are\n+    // successfully processed as part of a low-work headers sync in progress\n+    // (either in PRESYNC or REDOWNLOAD phase).\n+    // If true, this will mean that any headers returned to us (ie during\n+    // REDOWNLOAD) can be validated without further anti-DoS checks.\n+    bool already_validated_work = false;\n+\n+    // If we're in the middle of headers sync, let it do its magic.\n+    bool have_headers_sync = false;\n+    {\n+        LOCK(peer.m_headers_sync_mutex);\n+\n+        already_validated_work = IsContinuationOfLowWorkHeadersSync(peer, pfrom, headers);\n+\n+        // The headers we passed in may have been:\n+        // - untouched, perhaps if no headers-sync was in progress, or some\n+        //   failure occurred\n+        // - erased, such as if the headers were successfully processed and no\n+        //   additional headers processing needs to take place (such as if we\n+        //   are still in PRESYNC)\n+        // - replaced with headers that are now ready for validation, such as\n+        //   during the REDOWNLOAD phase of a low-work headers sync.\n+        // So just check whether we still have headers that we need to process,\n+        // or not.\n+        if (headers.empty()) {\n+            return;\n+        }\n+\n+        have_headers_sync = !!peer.m_headers_sync;",
      "path": "src/net_processing.cpp",
      "position": 457,
      "original_position": 300,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "9a0d196a9cd30db5e99a50b900f99f66a09a72dc",
      "in_reply_to_id": null,
      "user": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Not quite sure where to add this note; but having dropped the optimisation, this results in slightly weird behaviour: if you're doing headers sync with two peers, A and B, and A completes redownload and then syncs up to the tip, then A, B and all other nodes will report `synced_headers` as matching the tip, and all nodes will start downloading blocks, but B will continue to download every header until it re-confirms that min work has been reached. Adding `m_redownload_buffer_last_height` to `getpeerinfo` makes this behaviour more easily observable.\r\n\r\nProbably not worth optimising for this particular case rather than doing something more general and with greater impact, though.",
      "created_at": "2022-08-26T06:35:29Z",
      "updated_at": "2022-08-26T15:52:48Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r955704311",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/955704311"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2791,
      "original_line": 2791,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/956119793",
      "pull_request_review_id": 1086408327,
      "id": 956119793,
      "node_id": "PRRC_kwDOABII5844_Trx",
      "diff_hunk": "@@ -2316,6 +2442,146 @@ bool PeerManagerImpl::CheckHeadersAreContinuous(const std::vector<CBlockHeader>&\n     return true;\n }\n \n+bool PeerManagerImpl::IsContinuationOfLowWorkHeadersSync(Peer& peer, CNode& pfrom, std::vector<CBlockHeader>& headers)\n+{\n+    if (peer.m_headers_sync) {\n+        auto result = peer.m_headers_sync->ProcessNextHeaders(headers, headers.size() == MAX_HEADERS_RESULTS);\n+        if (result.request_more) {\n+            auto locator = peer.m_headers_sync->NextHeadersRequestLocator();\n+            // If we were instructed to ask for a locator, it should not be empty.\n+            Assume(!locator.vHave.empty());\n+            if (!locator.vHave.empty()) {\n+                // It should be impossible for the getheaders request to fail,\n+                // because we should have cleared the last getheaders timestamp\n+                // when processing the headers that triggered this call. But\n+                // it may be possible to bypass this via compactblock\n+                // processing, so check the result before logging just to be\n+                // safe.\n+                bool sent_getheaders = MaybeSendGetHeaders(pfrom, locator, peer);\n+                if (sent_getheaders) {\n+                    LogPrint(BCLog::NET, \"more getheaders (from %s) to peer=%d\\n\",\n+                            locator.vHave.front().ToString(), pfrom.GetId());\n+                } else {\n+                    LogPrint(BCLog::NET, \"error sending next getheaders (from %s) to continue sync with peer=%d\\n\",\n+                            locator.vHave.front().ToString(), pfrom.GetId());\n+                }\n+            }\n+        }\n+\n+        if (peer.m_headers_sync->GetState() == HeadersSyncState::State::FINAL) {\n+            peer.m_headers_sync.reset(nullptr);\n+\n+            // Delete this peer's entry in m_headers_presync_stats.\n+            // If this is m_headers_presync_bestpeer, it will be replaced later\n+            // by the next peer that triggers the else{} branch below.\n+            LOCK(m_headers_presync_mutex);\n+            m_headers_presync_stats.erase(pfrom.GetId());\n+        } else {\n+            // Build statistics for this peer's sync.\n+            HeadersPresyncStats stats;\n+            stats.first = peer.m_headers_sync->GetPresyncWork();\n+            if (peer.m_headers_sync->GetState() == HeadersSyncState::State::PRESYNC) {\n+                stats.second = {peer.m_headers_sync->GetPresyncHeight(),\n+                                peer.m_headers_sync->GetPresyncTime()};\n+            }\n+\n+            // Update statistics in stats.\n+            LOCK(m_headers_presync_mutex);\n+            m_headers_presync_stats[pfrom.GetId()] = stats;\n+            auto best_it = m_headers_presync_stats.find(m_headers_presync_bestpeer);\n+            bool best_updated = false;\n+            if (best_it == m_headers_presync_stats.end()) {\n+                // If the cached best peer is outdated, iterate over all remaining ones (including\n+                // newly updated one) to find the best one.\n+                NodeId peer_best{-1};\n+                const HeadersPresyncStats* stat_best{nullptr};\n+                for (const auto& [peer, stat] : m_headers_presync_stats) {\n+                    if (!stat_best || stat > *stat_best) {\n+                        peer_best = peer;\n+                        stat_best = &stat;\n+                    }\n+                }\n+                m_headers_presync_bestpeer = peer_best;\n+                best_updated = (peer_best == pfrom.GetId());\n+            } else if (best_it->first == pfrom.GetId() || stats > best_it->second) {\n+                // pfrom was and remains the best peer, or pfrom just became best.\n+                m_headers_presync_bestpeer = pfrom.GetId();\n+                best_updated = true;\n+            }\n+            if (best_updated && stats.second.has_value()) {\n+                // If the best peer updated, and it is in its first phase, signal.\n+                m_headers_presync_should_signal = true;\n+            }\n+        }\n+\n+        if (result.success) {\n+            // We only overwrite the headers passed in if processing was\n+            // successful.\n+            headers.swap(result.pow_validated_headers);\n+        }\n+\n+        return result.success;\n+    }\n+    // Either we didn't have a sync in progress, or something went wrong\n+    // processing these headers, or we are returning headers to the caller to\n+    // process.\n+    return false;\n+}\n+\n+bool PeerManagerImpl::TryLowWorkHeadersSync(Peer& peer, CNode& pfrom, const CBlockIndex* chain_start_header, std::vector<CBlockHeader>& headers)\n+{\n+    // Calculate the total work on this chain.\n+    arith_uint256 total_work = chain_start_header->nChainWork + CalculateHeadersWork(headers);\n+\n+    // Our dynamic anti-DoS threshold (minimum work required on a headers chain\n+    // before we'll store it)\n+    arith_uint256 minimum_chain_work = GetAntiDoSWorkThreshold();\n+\n+    // Avoid DoS via low-difficulty-headers by only processing if the headers\n+    // are part of a chain with sufficient work.\n+    if (total_work < minimum_chain_work) {",
      "path": "src/net_processing.cpp",
      "position": 344,
      "original_position": 344,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f0dae7c6fa54d4d00489da02eebfa3e4ef40cb63",
      "in_reply_to_id": null,
      "user": {
        "login": "ajtowns",
        "id": 127186,
        "node_id": "MDQ6VXNlcjEyNzE4Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/127186?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ajtowns",
        "html_url": "https://github.com/ajtowns",
        "followers_url": "https://api.github.com/users/ajtowns/followers",
        "following_url": "https://api.github.com/users/ajtowns/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ajtowns/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ajtowns/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ajtowns/subscriptions",
        "organizations_url": "https://api.github.com/users/ajtowns/orgs",
        "repos_url": "https://api.github.com/users/ajtowns/repos",
        "events_url": "https://api.github.com/users/ajtowns/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ajtowns/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Perhaps add an exception here for peers with some `NetPermissionsFlag` set (reuse noban or download?) to allow you to easily avoid downloading headers twice if you have a trusted node to connect to?",
      "created_at": "2022-08-26T14:43:29Z",
      "updated_at": "2022-08-26T15:52:48Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r956119793",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/956119793"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2542,
      "original_line": 2542,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/956834209",
      "pull_request_review_id": 1087980874,
      "id": 956834209,
      "node_id": "PRRC_kwDOABII5845CCGh",
      "diff_hunk": "@@ -0,0 +1,317 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959}; // 13959/584 = ~23.9 commitments\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments = {};\n+    m_last_header_received.SetNull();\n+    m_redownloaded_headers = {};\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                Assume(m_download_state == State::PRESYNC);\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps our peer reorged away from the chain\n+        // they were on. Give up on this sync for now (likely we will start a\n+        // new sync with a new starting point).\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (presync phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and occasionally store\n+    // commitments.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& current)\n+{\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                m_last_header_received.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (presync phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    if (next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        // Add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {",
      "path": "src/headerssync.cpp",
      "position": 198,
      "original_position": 198,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f0dae7c6fa54d4d00489da02eebfa3e4ef40cb63",
      "in_reply_to_id": null,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I wonder if it wouldn't be better to make `m_max_commitments` function of *right now* when we evaluate this check (e.g `GetMaxCommitments(NodeClock::time_point_now`), rather than when we initiate headers sync. One could argue that the most-work consensus-valid chain could be ignored by this logic as the chain might have grown since the start of the sync and is artificially sorted out by DoS mitigations. I concede, I don't think it matters in practice as if we reach the rate of 6 blocks/second something is already wrong, however I think ideally the consensus outcome should be abstracted out from our headers/blocks syncing strategy.",
      "created_at": "2022-08-29T01:50:05Z",
      "updated_at": "2022-08-29T02:38:48Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r956834209",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/956834209"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 198,
      "original_line": 198,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/956839174",
      "pull_request_review_id": 1087980874,
      "id": 956839174,
      "node_id": "PRRC_kwDOABII5845CDUG",
      "diff_hunk": "@@ -2263,6 +2360,35 @@ void PeerManagerImpl::SendBlockTransactions(CNode& pfrom, Peer& peer, const CBlo\n     m_connman.PushMessage(&pfrom, msgMaker.Make(NetMsgType::BLOCKTXN, resp));\n }\n \n+bool PeerManagerImpl::CheckHeadersPoW(const std::vector<CBlockHeader>& headers, const Consensus::Params& consensusParams, Peer& peer)\n+{\n+    // Do these headers have proof-of-work matching what's claimed?\n+    if (!HasValidProofOfWork(headers, consensusParams)) {\n+        Misbehaving(peer, 100, \"header with invalid proof of work\");\n+        return false;\n+    }\n+\n+    // Are these headers connected to each other?\n+    if (!CheckHeadersAreContinuous(headers)) {\n+        Misbehaving(peer, 20, \"non-continuous headers sequence\");\n+        return false;",
      "path": "src/net_processing.cpp",
      "position": 213,
      "original_position": 213,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f0dae7c6fa54d4d00489da02eebfa3e4ef40cb63",
      "in_reply_to_id": null,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I'm not sure why a misbehaving score of 20 is assigned here, considering that if `CheckHeadersPow` returns false in `ProcessHeadersMessage()`, an additional misbehaving score of 100 is assigned there (L2757), as such beyond `DISCOURAGEMENT_THRESHOLD`. I'm don't know if it makes sense.",
      "created_at": "2022-08-29T02:07:48Z",
      "updated_at": "2022-08-29T02:38:48Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r956839174",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/956839174"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2374,
      "original_line": 2374,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/956847683",
      "pull_request_review_id": 1087980874,
      "id": 956847683,
      "node_id": "PRRC_kwDOABII5845CFZD",
      "diff_hunk": "@@ -0,0 +1,317 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959}; // 13959/584 = ~23.9 commitments\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments = {};\n+    m_last_header_received.SetNull();\n+    m_redownloaded_headers = {};\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                Assume(m_download_state == State::PRESYNC);\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps our peer reorged away from the chain\n+        // they were on. Give up on this sync for now (likely we will start a\n+        // new sync with a new starting point).\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (presync phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and occasionally store\n+    // commitments.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& current)\n+{\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                m_last_header_received.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (presync phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    if (next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        // Add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // It's possible the chain grew since we started the sync; so\n+            // potentially we could succeed in syncing the peer's chain if we\n+            // try again later.\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: exceeded max commitments at height=%i (presync phase)\\n\", m_id, next_height);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));\n+    m_last_header_received = current;\n+    m_current_height = next_height;\n+\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreRedownloadedHeader(const CBlockHeader& header)\n+{\n+    Assume(m_download_state == State::REDOWNLOAD);\n+    if (m_download_state != State::REDOWNLOAD) return false;\n+\n+    int64_t next_height = m_redownload_buffer_last_height + 1;\n+\n+    // Ensure that we're working on a header that connects to the chain we're\n+    // downloading.\n+    if (header.hashPrevBlock != m_redownload_buffer_last_hash) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (redownload phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    // Check that the difficulty adjustments are within our tolerance:\n+    uint32_t previous_nBits{0};\n+    if (!m_redownloaded_headers.empty()) {\n+        previous_nBits = m_redownloaded_headers.back().nBits;\n+    } else {\n+        previous_nBits = m_chain_start->nBits;\n+    }\n+\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous_nBits, header.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (redownload phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    // Track work on the redownloaded chain\n+    m_redownload_chain_work += GetBlockProof(CBlockIndex(header));\n+\n+    if (m_redownload_chain_work >= m_minimum_required_work) {\n+        m_process_all_remaining_headers = true;\n+    }\n+\n+    // If we're at a header for which we previously stored a commitment, verify\n+    // it is correct. Failure will result in aborting download.\n+    // Also, don't check commitments once we've gotten to our target blockhash;\n+    // it's possible our peer has extended its chain between our first sync and\n+    // our second, and we don't want to return failure after we've seen our\n+    // target blockhash just because we ran out of commitments.\n+    if (!m_process_all_remaining_headers && next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        if (m_header_commitments.size() == 0) {\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: commitment overrun at height=%i (redownload phase)\\n\", m_id, next_height);\n+            // Somehow our peer managed to feed us a different chain and\n+            // we've run out of commitments.\n+            return false;\n+        }\n+        bool commitment = m_hasher(header.GetHash()) & 1;",
      "path": "src/headerssync.cpp",
      "position": 263,
      "original_position": 263,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f0dae7c6fa54d4d00489da02eebfa3e4ef40cb63",
      "in_reply_to_id": null,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Considering the 1-bit commitment, I wonder how hard it would be for an attacker to feed us a high-work chain of headers during PRESYNC, to substitute a second low-work chain of headers during REDOWNLOAD satisfying the exact same commitment pattern.  ",
      "created_at": "2022-08-29T02:35:22Z",
      "updated_at": "2022-08-29T02:38:48Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r956847683",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/956847683"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 263,
      "original_line": 263,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/957081059",
      "pull_request_review_id": 1088328917,
      "id": 957081059,
      "node_id": "PRRC_kwDOABII5845C-Xj",
      "diff_hunk": "@@ -0,0 +1,317 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959}; // 13959/584 = ~23.9 commitments\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments = {};\n+    m_last_header_received.SetNull();\n+    m_redownloaded_headers = {};\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                Assume(m_download_state == State::PRESYNC);\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps our peer reorged away from the chain\n+        // they were on. Give up on this sync for now (likely we will start a\n+        // new sync with a new starting point).\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (presync phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and occasionally store\n+    // commitments.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& current)\n+{\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                m_last_header_received.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (presync phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    if (next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        // Add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {",
      "path": "src/headerssync.cpp",
      "position": 198,
      "original_position": 198,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f0dae7c6fa54d4d00489da02eebfa3e4ef40cb63",
      "in_reply_to_id": 956834209,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Is there a plausible scenario where this would matter?  We also specify at the start of sync the minimum work required; fixing the maximum chain length just has the effect of requiring that a peer's chain has enough work at the time the sync is initiated to pass anti-DoS checks.\r\n\r\nFailure to sync because of a low-work chain will not cause a peer to be dropped, so if their chain grows later we'll try syncing again.  And in the specific case of a peer announcing many blocks to us (like approaching the 6 blocks/second theoretical limit), then this should not take long at all, because as soon as the sync ends and we get one more block announcement, we're resume headers sync.",
      "created_at": "2022-08-29T09:19:14Z",
      "updated_at": "2022-08-29T09:19:14Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r957081059",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/957081059"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 198,
      "original_line": 198,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/957082886",
      "pull_request_review_id": 1088331581,
      "id": 957082886,
      "node_id": "PRRC_kwDOABII5845C-0G",
      "diff_hunk": "@@ -0,0 +1,317 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959}; // 13959/584 = ~23.9 commitments\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments = {};\n+    m_last_header_received.SetNull();\n+    m_redownloaded_headers = {};\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                Assume(m_download_state == State::PRESYNC);\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps our peer reorged away from the chain\n+        // they were on. Give up on this sync for now (likely we will start a\n+        // new sync with a new starting point).\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (presync phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and occasionally store\n+    // commitments.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& current)\n+{\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                m_last_header_received.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (presync phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    if (next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        // Add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // It's possible the chain grew since we started the sync; so\n+            // potentially we could succeed in syncing the peer's chain if we\n+            // try again later.\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: exceeded max commitments at height=%i (presync phase)\\n\", m_id, next_height);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));\n+    m_last_header_received = current;\n+    m_current_height = next_height;\n+\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreRedownloadedHeader(const CBlockHeader& header)\n+{\n+    Assume(m_download_state == State::REDOWNLOAD);\n+    if (m_download_state != State::REDOWNLOAD) return false;\n+\n+    int64_t next_height = m_redownload_buffer_last_height + 1;\n+\n+    // Ensure that we're working on a header that connects to the chain we're\n+    // downloading.\n+    if (header.hashPrevBlock != m_redownload_buffer_last_hash) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (redownload phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    // Check that the difficulty adjustments are within our tolerance:\n+    uint32_t previous_nBits{0};\n+    if (!m_redownloaded_headers.empty()) {\n+        previous_nBits = m_redownloaded_headers.back().nBits;\n+    } else {\n+        previous_nBits = m_chain_start->nBits;\n+    }\n+\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous_nBits, header.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (redownload phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    // Track work on the redownloaded chain\n+    m_redownload_chain_work += GetBlockProof(CBlockIndex(header));\n+\n+    if (m_redownload_chain_work >= m_minimum_required_work) {\n+        m_process_all_remaining_headers = true;\n+    }\n+\n+    // If we're at a header for which we previously stored a commitment, verify\n+    // it is correct. Failure will result in aborting download.\n+    // Also, don't check commitments once we've gotten to our target blockhash;\n+    // it's possible our peer has extended its chain between our first sync and\n+    // our second, and we don't want to return failure after we've seen our\n+    // target blockhash just because we ran out of commitments.\n+    if (!m_process_all_remaining_headers && next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        if (m_header_commitments.size() == 0) {\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: commitment overrun at height=%i (redownload phase)\\n\", m_id, next_height);\n+            // Somehow our peer managed to feed us a different chain and\n+            // we've run out of commitments.\n+            return false;\n+        }\n+        bool commitment = m_hasher(header.GetHash()) & 1;",
      "path": "src/headerssync.cpp",
      "position": 263,
      "original_position": 263,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f0dae7c6fa54d4d00489da02eebfa3e4ef40cb63",
      "in_reply_to_id": 956847683,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "The commitment parameters (frequency of commitments and size of redownload buffer) are chosen to tune this exact ability of an attacker, based on the simulation in https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1.",
      "created_at": "2022-08-29T09:21:25Z",
      "updated_at": "2022-08-29T09:21:25Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r957082886",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/957082886"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 263,
      "original_line": 263,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/957234609",
      "pull_request_review_id": 1088543779,
      "id": 957234609,
      "node_id": "PRRC_kwDOABII5845Dj2x",
      "diff_hunk": "@@ -2263,6 +2360,35 @@ void PeerManagerImpl::SendBlockTransactions(CNode& pfrom, Peer& peer, const CBlo\n     m_connman.PushMessage(&pfrom, msgMaker.Make(NetMsgType::BLOCKTXN, resp));\n }\n \n+bool PeerManagerImpl::CheckHeadersPoW(const std::vector<CBlockHeader>& headers, const Consensus::Params& consensusParams, Peer& peer)\n+{\n+    // Do these headers have proof-of-work matching what's claimed?\n+    if (!HasValidProofOfWork(headers, consensusParams)) {\n+        Misbehaving(peer, 100, \"header with invalid proof of work\");\n+        return false;\n+    }\n+\n+    // Are these headers connected to each other?\n+    if (!CheckHeadersAreContinuous(headers)) {\n+        Misbehaving(peer, 20, \"non-continuous headers sequence\");\n+        return false;",
      "path": "src/net_processing.cpp",
      "position": 213,
      "original_position": 213,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f0dae7c6fa54d4d00489da02eebfa3e4ef40cb63",
      "in_reply_to_id": 956839174,
      "user": {
        "login": "sdaftuar",
        "id": 7463573,
        "node_id": "MDQ6VXNlcjc0NjM1NzM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7463573?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sdaftuar",
        "html_url": "https://github.com/sdaftuar",
        "followers_url": "https://api.github.com/users/sdaftuar/followers",
        "following_url": "https://api.github.com/users/sdaftuar/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sdaftuar/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sdaftuar/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sdaftuar/subscriptions",
        "organizations_url": "https://api.github.com/users/sdaftuar/orgs",
        "repos_url": "https://api.github.com/users/sdaftuar/repos",
        "events_url": "https://api.github.com/users/sdaftuar/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sdaftuar/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Thanks, I think this may have been an oversight when I added the `Misbehaving()` to the caller.  Looks like the score of 20 is how things worked historically; I guess I'll just leave that unchanged but remove the `Misbehaving()` from the caller.",
      "created_at": "2022-08-29T12:02:05Z",
      "updated_at": "2022-08-29T12:04:40Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r957234609",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/957234609"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2374,
      "original_line": 2374,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/958291531",
      "pull_request_review_id": 1090045422,
      "id": 958291531,
      "node_id": "PRRC_kwDOABII5845Hl5L",
      "diff_hunk": "@@ -581,18 +588,59 @@ class PeerManagerImpl final : public PeerManager\n \n     void ProcessOrphanTx(std::set<uint256>& orphan_work_set) EXCLUSIVE_LOCKS_REQUIRED(cs_main, g_cs_orphans)\n         EXCLUSIVE_LOCKS_REQUIRED(!m_peer_mutex);\n-    /** Process a single headers message from a peer. */\n+    /** Process a single headers message from a peer.\n+     *\n+     * @param[in]   pfrom     CNode of the peer\n+     * @param[in]   peer      The peer sending us the headers\n+     * @param[in]   headers   The headers received. Note that this may be modified within ProcessHeadersMessage.",
      "path": "src/net_processing.cpp",
      "position": 54,
      "original_position": 30,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "1fca7ed49eca6c2cd71d37415fa2d419779cf212",
      "in_reply_to_id": 953741265,
      "user": {
        "login": "vasild",
        "id": 266751,
        "node_id": "MDQ6VXNlcjI2Njc1MQ==",
        "avatar_url": "https://avatars.githubusercontent.com/u/266751?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/vasild",
        "html_url": "https://github.com/vasild",
        "followers_url": "https://api.github.com/users/vasild/followers",
        "following_url": "https://api.github.com/users/vasild/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/vasild/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/vasild/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/vasild/subscriptions",
        "organizations_url": "https://api.github.com/users/vasild/orgs",
        "repos_url": "https://api.github.com/users/vasild/repos",
        "events_url": "https://api.github.com/users/vasild/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/vasild/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "@sdaftuar, I was just briefly looking around and I missed the explanations in `IsContinuationOfLowWorkHeadersSync()` `@return`. The comment in `ProcessHeadersMessage()` is too scarce - \"may be modified\", but in which way?\r\n\r\nAnyway, too minor and I guess the @sipa's suggestion above has improved it enough.",
      "created_at": "2022-08-30T10:16:05Z",
      "updated_at": "2022-08-30T10:16:05Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r958291531",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/958291531"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 598,
      "original_line": 598,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/958519948",
      "pull_request_review_id": 1090377051,
      "id": 958519948,
      "node_id": "PRRC_kwDOABII5845IdqM",
      "diff_hunk": "@@ -3711,6 +3711,29 @@ bool ChainstateManager::ProcessNewBlockHeaders(const std::vector<CBlockHeader>&\n     return true;\n }\n \n+void ChainstateManager::ReportHeadersPresync(const arith_uint256& work, int64_t height, int64_t timestamp)\n+{\n+    AssertLockNotHeld(cs_main);\n+    const auto& chainstate = ActiveChainstate();\n+    {\n+        LOCK(cs_main);\n+        // Don't report headers presync progress if we already have a post-minchainwork header chain.\n+        // This means we lose reporting for potentially legimate, but unlikely, deep reorgs, but",
      "path": "src/validation.cpp",
      "position": 11,
      "original_position": 11,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "355547334f7d08640ee1fa291227356d61145d1a",
      "in_reply_to_id": null,
      "user": {
        "login": "instagibbs",
        "id": 5767891,
        "node_id": "MDQ6VXNlcjU3Njc4OTE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5767891?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/instagibbs",
        "html_url": "https://github.com/instagibbs",
        "followers_url": "https://api.github.com/users/instagibbs/followers",
        "following_url": "https://api.github.com/users/instagibbs/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/instagibbs/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/instagibbs/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/instagibbs/subscriptions",
        "organizations_url": "https://api.github.com/users/instagibbs/orgs",
        "repos_url": "https://api.github.com/users/instagibbs/repos",
        "events_url": "https://api.github.com/users/instagibbs/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/instagibbs/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "legitimate ",
      "created_at": "2022-08-30T13:58:26Z",
      "updated_at": "2022-08-30T13:58:26Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r958519948",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/958519948"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 3721,
      "original_line": 3721,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959022153",
      "pull_request_review_id": 1091090387,
      "id": 959022153,
      "node_id": "PRRC_kwDOABII5845KYRJ",
      "diff_hunk": "@@ -71,6 +71,57 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n     return bnNew.GetCompact();\n }\n \n+// Check that on difficulty adjustments, the new difficulty does not increase\n+// or decrease beyond the permitted limits.\n+bool PermittedDifficultyTransition(const Consensus::Params& params, int64_t height, uint32_t old_nbits, uint32_t new_nbits)\n+{\n+    if (params.fPowAllowMinDifficultyBlocks) return true;\n+\n+    if (height % params.DifficultyAdjustmentInterval() == 0) {\n+        int64_t smallest_timespan = params.nPowTargetTimespan/4;\n+        int64_t largest_timespan = params.nPowTargetTimespan*4;\n+\n+        const arith_uint256 pow_limit = UintToArith256(params.powLimit);\n+        arith_uint256 observed_new_target;\n+        observed_new_target.SetCompact(new_nbits);\n+\n+        // Calculate the largest difficulty value possible:\n+        arith_uint256 largest_difficulty_target;\n+        largest_difficulty_target.SetCompact(old_nbits);\n+        largest_difficulty_target *= largest_timespan;\n+        largest_difficulty_target /= params.nPowTargetTimespan;\n+\n+        if (largest_difficulty_target > pow_limit) {\n+            largest_difficulty_target = pow_limit;\n+        }\n+\n+        // Round and then compare this new calculated value to what is\n+        // observed.\n+        arith_uint256 maximum_new_target;\n+        maximum_new_target.SetCompact(largest_difficulty_target.GetCompact());\n+        if (maximum_new_target < observed_new_target) return false;",
      "path": "src/pow.cpp",
      "position": 32,
      "original_position": 32,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "in_reply_to_id": null,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think comment could be added that last received header has been detected to be consensus-invalid as the new target is above what is allowed by the difficulty adjustment bounds in `CalculateNextWorkRequired()`. Same for the lower bound, if my understanding is correct.\r\n\r\nTest coverage in `pow_tests.cpp` L61 and L78.",
      "created_at": "2022-08-30T23:49:01Z",
      "updated_at": "2022-08-31T03:05:01Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r959022153",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959022153"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 102,
      "original_line": 102,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959025992",
      "pull_request_review_id": 1091090387,
      "id": 959025992,
      "node_id": "PRRC_kwDOABII5845KZNI",
      "diff_hunk": "@@ -20,4 +20,18 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n /** Check whether a block hash satisfies the proof-of-work requirement specified by nBits */\n bool CheckProofOfWork(uint256 hash, unsigned int nBits, const Consensus::Params&);\n \n+/**\n+ * Return false if the proof-of-work requirement specified by new_nbits at a\n+ * given height is not possible, given the proof-of-work on the prior block as\n+ * specified by old_nbits.\n+ *\n+ * This function only checks that the new value is within a factor of 4 of the\n+ * old value for blocks at the difficulty adjustment interval, and otherwise\n+ * requires the values to be the same.",
      "path": "src/pow.h",
      "position": 11,
      "original_position": 11,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "in_reply_to_id": null,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Minor comment: while this function is only used for p2p DoS protection and the difficulty adjustment bounds checked should reject invalid headers, this distinction could be precised.\r\n\r\nBitcoin Core is used as a reference software for many bitcoin libraries across the ecosystem and there is often confusion between what is policy, consensus or optimizations based on historical consensus state like BIP90.",
      "created_at": "2022-08-30T23:58:35Z",
      "updated_at": "2022-08-31T03:05:01Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r959025992",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959025992"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 30,
      "original_line": 30,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959034094",
      "pull_request_review_id": 1091090387,
      "id": 959034094,
      "node_id": "PRRC_kwDOABII5845KbLu",
      "diff_hunk": "@@ -0,0 +1,469 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_UTIL_BITDEQUE_H\n+#define BITCOIN_UTIL_BITDEQUE_H\n+\n+#include <bitset>\n+#include <cstddef>\n+#include <deque>\n+#include <limits>\n+#include <stdexcept>\n+#include <tuple>\n+\n+/** Class that mimics std::deque<bool>, but with std::vector<bool>'s bit packing.\n+ *\n+ * BlobSize selects the (minimum) number of bits that are allocated at once.\n+ * Larger values reduce the asymptotic memory usage overhead, at the cost of",
      "path": "src/util/bitdeque.h",
      "position": 18,
      "original_position": 18,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "84852bb6bb3579e475ce78fe729fd125ddbc715f",
      "in_reply_to_id": null,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think it could be valuable to document against which type of workpayloads this is holding or if it's independent of the container element insertion usage. Also it could be precised if the implementation is from scratch by following the standard library headers or inspired from an existent implementation to ease maintenance if there are relevant changes upstream. We do so in LDK. ",
      "created_at": "2022-08-31T00:19:40Z",
      "updated_at": "2022-08-31T03:05:01Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r959034094",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959034094"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 18,
      "original_line": 18,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959082452",
      "pull_request_review_id": 1091090387,
      "id": 959082452,
      "node_id": "PRRC_kwDOABII5845Km_U",
      "diff_hunk": "@@ -477,4 +477,10 @@ class CChain\n     CBlockIndex* FindEarliestAtLeast(int64_t nTime, int height) const;\n };\n \n+/** Get a locator for a block index entry. */\n+CBlockLocator GetLocator(const CBlockIndex* index);",
      "path": "src/chain.h",
      "position": 16,
      "original_position": 5,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "5ea6f9a34bf3e8751fbca8f6107e99a9f553e31f",
      "in_reply_to_id": 951566903,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think it would still be valuable to improve long-term codebase readabilty to prefix the class method `GetLocator()` to a naming like `GetLocatorAtTip()` to dissociate well from the new method in the same file (note, we have yet another `GetLocator()` in `src/index/base.cpp`). There could be a concern of a validation/chain method misusage inducing some safety bug, especially with the ongoing libbitcoinkernel and assumeutxo works.\r\n\r\nBeyond, it could be valuable to document `CBlockLocator` in `src/primitives/block.h`, while this data struct is used by GETHEADERS and GETBLOCKS, iirc its usage, the way we construct with the steps until genesis or sanity bounds we enforce on our side like `MAX_LOCATOR_SIZE` are loosely documented across the codebase. That would make it hard to reuse it for future p2p extensions or for experimentation with other chain-syncing strategies.\r\n\r\nAll suggestions.   ",
      "created_at": "2022-08-31T02:11:49Z",
      "updated_at": "2022-08-31T03:05:01Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r959082452",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959082452"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 487,
      "original_line": 487,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959117908",
      "pull_request_review_id": 1091090387,
      "id": 959117908,
      "node_id": "PRRC_kwDOABII5845KvpU",
      "diff_hunk": "@@ -2506,11 +2506,12 @@ void PeerManagerImpl::ProcessHeadersMessage(CNode& pfrom, Peer& peer,\n             return;\n         }\n     }\n+    Assume(pindexLast);",
      "path": "src/net_processing.cpp",
      "position": 13,
      "original_position": 13,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "ed470940cddbeb40425960d51cefeec4948febe4",
      "in_reply_to_id": null,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Note, I think it would be valuable to document the interactions between `ProcessNewBlockHeaders()` and `pindexLast`. The method documentation says \"ppindex If set, the pointer will be set to to point to the last new block index object for the given headers\". At that stage L2509 (`net_processing.cpp`), I understand that `pindexLast` should be always set as the unset case happens only if `accepted=false` L3696 (`validation.cpp`), afaict `accepte=false` induced that `BlockValidationState` is invalid from `AcceptBlockHeader` and therefore we should return L2506 (`validation.cpp`). If there is a refactor of the chain of nested calls and their return values, we could have a silent break. \r\n\r\nDo we have cases where a) `accepted=false`, b) `BlockValidationState` is valid and c) `pindexLast` unset ? I don't think so.",
      "created_at": "2022-08-31T02:59:54Z",
      "updated_at": "2022-08-31T03:08:52Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r959117908",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959117908"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 2849,
      "original_line": 2509,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959139110",
      "pull_request_review_id": 1091266420,
      "id": 959139110,
      "node_id": "PRRC_kwDOABII5845K00m",
      "diff_hunk": "@@ -0,0 +1,317 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959}; // 13959/584 = ~23.9 commitments\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments = {};\n+    m_last_header_received.SetNull();\n+    m_redownloaded_headers = {};\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                Assume(m_download_state == State::PRESYNC);\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps our peer reorged away from the chain\n+        // they were on. Give up on this sync for now (likely we will start a\n+        // new sync with a new starting point).\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (presync phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and occasionally store\n+    // commitments.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& current)\n+{\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                m_last_header_received.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (presync phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    if (next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        // Add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {",
      "path": "src/headerssync.cpp",
      "position": 198,
      "original_position": 198,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f0dae7c6fa54d4d00489da02eebfa3e4ef40cb63",
      "in_reply_to_id": 956834209,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "As far as I can think, I don't see a plausible scenario where it matters. In fine there is the space of the historical chain where the 6 blocks/second has not been observed, and as such servicing as a additional chain length buffer to correct discrepancies between `HeadersSyncState` object initialization time point and a *right now*, I think. From my understanding, the maximum chain length is a protection targeting memory DoS of `m_header_commitments`, enforcing a constraint on the number of blocks allowed to satisfy the minimum work required.   ",
      "created_at": "2022-08-31T03:54:57Z",
      "updated_at": "2022-08-31T03:54:57Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r959139110",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959139110"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 198,
      "original_line": 198,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959141828",
      "pull_request_review_id": 1091269847,
      "id": 959141828,
      "node_id": "PRRC_kwDOABII5845K1fE",
      "diff_hunk": "@@ -0,0 +1,317 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959}; // 13959/584 = ~23.9 commitments\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments = {};\n+    m_last_header_received.SetNull();\n+    m_redownloaded_headers = {};\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                Assume(m_download_state == State::PRESYNC);\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps our peer reorged away from the chain\n+        // they were on. Give up on this sync for now (likely we will start a\n+        // new sync with a new starting point).\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (presync phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and occasionally store\n+    // commitments.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& current)\n+{\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                m_last_header_received.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (presync phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    if (next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        // Add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // It's possible the chain grew since we started the sync; so\n+            // potentially we could succeed in syncing the peer's chain if we\n+            // try again later.\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: exceeded max commitments at height=%i (presync phase)\\n\", m_id, next_height);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));\n+    m_last_header_received = current;\n+    m_current_height = next_height;\n+\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreRedownloadedHeader(const CBlockHeader& header)\n+{\n+    Assume(m_download_state == State::REDOWNLOAD);\n+    if (m_download_state != State::REDOWNLOAD) return false;\n+\n+    int64_t next_height = m_redownload_buffer_last_height + 1;\n+\n+    // Ensure that we're working on a header that connects to the chain we're\n+    // downloading.\n+    if (header.hashPrevBlock != m_redownload_buffer_last_hash) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (redownload phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    // Check that the difficulty adjustments are within our tolerance:\n+    uint32_t previous_nBits{0};\n+    if (!m_redownloaded_headers.empty()) {\n+        previous_nBits = m_redownloaded_headers.back().nBits;\n+    } else {\n+        previous_nBits = m_chain_start->nBits;\n+    }\n+\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous_nBits, header.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (redownload phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    // Track work on the redownloaded chain\n+    m_redownload_chain_work += GetBlockProof(CBlockIndex(header));\n+\n+    if (m_redownload_chain_work >= m_minimum_required_work) {\n+        m_process_all_remaining_headers = true;\n+    }\n+\n+    // If we're at a header for which we previously stored a commitment, verify\n+    // it is correct. Failure will result in aborting download.\n+    // Also, don't check commitments once we've gotten to our target blockhash;\n+    // it's possible our peer has extended its chain between our first sync and\n+    // our second, and we don't want to return failure after we've seen our\n+    // target blockhash just because we ran out of commitments.\n+    if (!m_process_all_remaining_headers && next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        if (m_header_commitments.size() == 0) {\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: commitment overrun at height=%i (redownload phase)\\n\", m_id, next_height);\n+            // Somehow our peer managed to feed us a different chain and\n+            // we've run out of commitments.\n+            return false;\n+        }\n+        bool commitment = m_hasher(header.GetHash()) & 1;",
      "path": "src/headerssync.cpp",
      "position": 263,
      "original_position": 263,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f0dae7c6fa54d4d00489da02eebfa3e4ef40cb63",
      "in_reply_to_id": 956847683,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "From browsing the simulation, the *1* bit commitment is assumed as a system parameter at the beginning of demonstration, how which the commitment parameters like frequency of commitments and size of redownload buffer are derived. I wonder if N-bit commitment could be more information efficient, though we're free to explore again in the future if the commitment storage memory usage becomes an issue (probably not).   ",
      "created_at": "2022-08-31T04:02:45Z",
      "updated_at": "2022-08-31T04:02:46Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r959141828",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959141828"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 263,
      "original_line": 263,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959401245",
      "pull_request_review_id": 1091627552,
      "id": 959401245,
      "node_id": "PRRC_kwDOABII5845L00d",
      "diff_hunk": "@@ -0,0 +1,144 @@\n+#!/usr/bin/env python3\n+# Copyright (c) 2019-2021 The Bitcoin Core developers\n+# Distributed under the MIT software license, see the accompanying\n+# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\"\"\"Test that we reject low difficulty headers to prevent our block tree from filling up with useless bloat\"\"\"\n+\n+from test_framework.test_framework import BitcoinTestFramework\n+\n+from test_framework.p2p import (\n+    P2PInterface,\n+)\n+\n+from test_framework.messages import (\n+    msg_headers,\n+)\n+\n+from test_framework.blocktools import (\n+    NORMAL_GBT_REQUEST_PARAMS,\n+    create_block,\n+)\n+\n+from test_framework.util import assert_equal\n+\n+NODE1_BLOCKS_REQUIRED = 15\n+NODE2_BLOCKS_REQUIRED = 2047\n+\n+\n+class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\n+    def set_test_params(self):\n+        self.setup_clean_chain = True\n+        self.num_nodes = 3\n+        # Node0 has no required chainwork; node1 requires 15 blocks on top of the genesis block; node2 requires 2047\n+        self.extra_args = [[\"-minimumchainwork=0x0\", \"-checkblockindex=0\"], [\"-minimumchainwork=0x1f\", \"-checkblockindex=0\"], [\"-minimumchainwork=0x1000\", \"-checkblockindex=0\"]]\n+\n+    def setup_network(self):\n+        self.setup_nodes()\n+        self.reconnect_all()\n+        self.sync_all()\n+\n+    def disconnect_all(self):\n+        self.disconnect_nodes(0, 1)\n+        self.disconnect_nodes(0, 2)\n+\n+    def reconnect_all(self):\n+        self.connect_nodes(0, 1)\n+        self.connect_nodes(0, 2)\n+\n+    def test_chains_sync_when_long_enough(self):\n+        self.log.info(\"Generate blocks on the node with no required chainwork, and verify nodes 1 and 2 have no new headers in their headers tree\")\n+        with self.nodes[1].assert_debug_log(expected_msgs=[\"[net] Ignoring low-work chain (height=14)\"]), self.nodes[2].assert_debug_log(expected_msgs=[\"[net] Ignoring low-work chain (height=14)\"]):\n+            self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED-1, sync_fun=self.no_op)\n+\n+        for node in self.nodes[1:]:\n+            chaintips = node.getchaintips()\n+            assert(len(chaintips) == 1)\n+            assert {\n+                'height': 0,\n+                'hash': '0f9188f13cb7b2c71f2a335e3a4fc328bf5beb436012afca590b1a11466e2206',\n+                'branchlen': 0,\n+                'status': 'active',\n+            } in chaintips\n+\n+        self.log.info(\"Generate more blocks to satisfy node1's minchainwork requirement, and verify node2 still has no new headers in headers tree\")\n+        with self.nodes[2].assert_debug_log(expected_msgs=[\"[net] Ignoring low-work chain (height=15)\"]):\n+            self.generate(self.nodes[0], NODE1_BLOCKS_REQUIRED - self.nodes[0].getblockcount(), sync_fun=self.no_op)\n+        self.sync_blocks(self.nodes[0:2])\n+\n+        assert {\n+            'height': 0,\n+            'hash': '0f9188f13cb7b2c71f2a335e3a4fc328bf5beb436012afca590b1a11466e2206',\n+            'branchlen': 0,\n+            'status': 'active',\n+        } in self.nodes[2].getchaintips()\n+\n+        assert(len(self.nodes[2].getchaintips()) == 1)\n+\n+        self.log.info(\"Generate long chain for node0/node1\")\n+        self.generate(self.nodes[0], NODE2_BLOCKS_REQUIRED-self.nodes[0].getblockcount(), sync_fun=self.no_op)\n+\n+        self.log.info(\"Verify that node2 will sync the chain when it gets long enough\")\n+        self.sync_blocks()\n+\n+    def test_peerinfo_includes_headers_presync_height(self):\n+        self.log.info(\"Test that getpeerinfo() includes headers presync height\")\n+\n+        # Disconnect network, so that we can find our own peer connection more\n+        # easily\n+        self.disconnect_all()\n+\n+        p2p = self.nodes[0].add_p2p_connection(P2PInterface())\n+        node = self.nodes[0]\n+\n+        # Ensure we have a long chain already\n+        current_height = self.nodes[0].getblockcount()\n+        if (current_height < 3000):\n+            self.generate(node, 3000-current_height, sync_fun=self.no_op)\n+\n+        # Send a group of 2000 headers, forking from genesis.\n+        new_blocks = []\n+        hashPrevBlock = int(node.getblockhash(0), 16)\n+        for i in range(2000):\n+            block = create_block(hashprev = hashPrevBlock, tmpl=node.getblocktemplate(NORMAL_GBT_REQUEST_PARAMS))\n+            block.solve()\n+            new_blocks.append(block)\n+            hashPrevBlock = block.sha256\n+\n+        headers_message = msg_headers(headers=new_blocks)",
      "path": "test/functional/p2p_headers_sync_with_minchainwork.py",
      "position": 107,
      "original_position": 107,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "in_reply_to_id": null,
      "user": {
        "login": "jonatack",
        "id": 2415484,
        "node_id": "MDQ6VXNlcjI0MTU0ODQ=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2415484?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jonatack",
        "html_url": "https://github.com/jonatack",
        "followers_url": "https://api.github.com/users/jonatack/followers",
        "following_url": "https://api.github.com/users/jonatack/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jonatack/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jonatack/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jonatack/subscriptions",
        "organizations_url": "https://api.github.com/users/jonatack/orgs",
        "repos_url": "https://api.github.com/users/jonatack/repos",
        "events_url": "https://api.github.com/users/jonatack/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jonatack/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Suggested test assertion\r\n\r\n```diff\r\n--- a/test/functional/p2p_headers_sync_with_minchainwork.py\r\n+++ b/test/functional/p2p_headers_sync_with_minchainwork.py\r\n@@ -104,6 +104,9 @@ class RejectLowDifficultyHeadersTest(BitcoinTestFramework):\r\n             new_blocks.append(block)\r\n             hashPrevBlock = block.sha256\r\n \r\n+        # no low-work sync is in progress\r\n+        assert_equal(node.getpeerinfo()[0]['presynced_headers'], -1)\r\n+\r\n         headers_message = msg_headers(headers=new_blocks)\r\n         p2p.send_and_ping(headers_message)\r\n```\r\n",
      "created_at": "2022-08-31T10:02:53Z",
      "updated_at": "2022-08-31T10:02:53Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r959401245",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959401245"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 107,
      "original_line": 107,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959528106",
      "pull_request_review_id": 1091803375,
      "id": 959528106,
      "node_id": "PRRC_kwDOABII5845MTyq",
      "diff_hunk": "@@ -0,0 +1,317 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959}; // 13959/584 = ~23.9 commitments\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments = {};\n+    m_last_header_received.SetNull();\n+    m_redownloaded_headers = {};\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                Assume(m_download_state == State::PRESYNC);\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps our peer reorged away from the chain\n+        // they were on. Give up on this sync for now (likely we will start a\n+        // new sync with a new starting point).\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (presync phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and occasionally store\n+    // commitments.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& current)\n+{\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                m_last_header_received.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (presync phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    if (next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        // Add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {",
      "path": "src/headerssync.cpp",
      "position": 198,
      "original_position": 198,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f0dae7c6fa54d4d00489da02eebfa3e4ef40cb63",
      "in_reply_to_id": 956834209,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I can't imagine this matters, because it would require the peer to have a chain which is invalid at the time the sync starts, which becomes valid while we're busy learning about it. If that were the case, we'll still synchronize it later.",
      "created_at": "2022-08-31T12:34:11Z",
      "updated_at": "2022-08-31T12:34:11Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r959528106",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959528106"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 198,
      "original_line": 198,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959534360",
      "pull_request_review_id": 1091812227,
      "id": 959534360,
      "node_id": "PRRC_kwDOABII5845MVUY",
      "diff_hunk": "@@ -0,0 +1,317 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959}; // 13959/584 = ~23.9 commitments\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments = {};\n+    m_last_header_received.SetNull();\n+    m_redownloaded_headers = {};\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                Assume(m_download_state == State::PRESYNC);\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps our peer reorged away from the chain\n+        // they were on. Give up on this sync for now (likely we will start a\n+        // new sync with a new starting point).\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (presync phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and occasionally store\n+    // commitments.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& current)\n+{\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                m_last_header_received.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (presync phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    if (next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        // Add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // It's possible the chain grew since we started the sync; so\n+            // potentially we could succeed in syncing the peer's chain if we\n+            // try again later.\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: exceeded max commitments at height=%i (presync phase)\\n\", m_id, next_height);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));\n+    m_last_header_received = current;\n+    m_current_height = next_height;\n+\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreRedownloadedHeader(const CBlockHeader& header)\n+{\n+    Assume(m_download_state == State::REDOWNLOAD);\n+    if (m_download_state != State::REDOWNLOAD) return false;\n+\n+    int64_t next_height = m_redownload_buffer_last_height + 1;\n+\n+    // Ensure that we're working on a header that connects to the chain we're\n+    // downloading.\n+    if (header.hashPrevBlock != m_redownload_buffer_last_hash) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (redownload phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    // Check that the difficulty adjustments are within our tolerance:\n+    uint32_t previous_nBits{0};\n+    if (!m_redownloaded_headers.empty()) {\n+        previous_nBits = m_redownloaded_headers.back().nBits;\n+    } else {\n+        previous_nBits = m_chain_start->nBits;\n+    }\n+\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous_nBits, header.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (redownload phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    // Track work on the redownloaded chain\n+    m_redownload_chain_work += GetBlockProof(CBlockIndex(header));\n+\n+    if (m_redownload_chain_work >= m_minimum_required_work) {\n+        m_process_all_remaining_headers = true;\n+    }\n+\n+    // If we're at a header for which we previously stored a commitment, verify\n+    // it is correct. Failure will result in aborting download.\n+    // Also, don't check commitments once we've gotten to our target blockhash;\n+    // it's possible our peer has extended its chain between our first sync and\n+    // our second, and we don't want to return failure after we've seen our\n+    // target blockhash just because we ran out of commitments.\n+    if (!m_process_all_remaining_headers && next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        if (m_header_commitments.size() == 0) {\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: commitment overrun at height=%i (redownload phase)\\n\", m_id, next_height);\n+            // Somehow our peer managed to feed us a different chain and\n+            // we've run out of commitments.\n+            return false;\n+        }\n+        bool commitment = m_hasher(header.GetHash()) & 1;",
      "path": "src/headerssync.cpp",
      "position": 263,
      "original_position": 263,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "f0dae7c6fa54d4d00489da02eebfa3e4ef40cb63",
      "in_reply_to_id": 956847683,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "We've analyzed half a dozen variations before ending up with this approach, including:\r\n\r\n1. Just (2000/N) bits every 2000 headers\r\n2. 1 bit every N headers, with a fixed offset\r\n3. 1 bit every N headers, with a random offset (approach from this PR)\r\n4. 1 bit every N headers, with an offset chosen such that it minimizes the attacker's power\r\n5. A 1 in N probability for every header to have a 1-bit commitment\r\n6. A 1/N-bit commitment for every header, using an entropy coder.\r\n\r\n(1) and (2) are equivalent, as we're only going to verify commitments after blobs of 2000 anyway. (3) is better, and destroys the equivalence with (1) because the attacker doesn't know when to start. (4) is very slightly worse than (3), but still significantly better than (1) and (2). (5) and (6) are significantly worse.\r\n\r\nIf you have other ideas for commitment structures, I'm happy to try simulating them.",
      "created_at": "2022-08-31T12:40:43Z",
      "updated_at": "2022-08-31T12:40:44Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r959534360",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959534360"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 263,
      "original_line": 263,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959535676",
      "pull_request_review_id": 1091814010,
      "id": 959535676,
      "node_id": "PRRC_kwDOABII5845MVo8",
      "diff_hunk": "@@ -71,6 +71,57 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n     return bnNew.GetCompact();\n }\n \n+// Check that on difficulty adjustments, the new difficulty does not increase\n+// or decrease beyond the permitted limits.\n+bool PermittedDifficultyTransition(const Consensus::Params& params, int64_t height, uint32_t old_nbits, uint32_t new_nbits)\n+{\n+    if (params.fPowAllowMinDifficultyBlocks) return true;\n+\n+    if (height % params.DifficultyAdjustmentInterval() == 0) {\n+        int64_t smallest_timespan = params.nPowTargetTimespan/4;\n+        int64_t largest_timespan = params.nPowTargetTimespan*4;\n+\n+        const arith_uint256 pow_limit = UintToArith256(params.powLimit);\n+        arith_uint256 observed_new_target;\n+        observed_new_target.SetCompact(new_nbits);\n+\n+        // Calculate the largest difficulty value possible:\n+        arith_uint256 largest_difficulty_target;\n+        largest_difficulty_target.SetCompact(old_nbits);\n+        largest_difficulty_target *= largest_timespan;\n+        largest_difficulty_target /= params.nPowTargetTimespan;\n+\n+        if (largest_difficulty_target > pow_limit) {\n+            largest_difficulty_target = pow_limit;\n+        }\n+\n+        // Round and then compare this new calculated value to what is\n+        // observed.\n+        arith_uint256 maximum_new_target;\n+        maximum_new_target.SetCompact(largest_difficulty_target.GetCompact());\n+        if (maximum_new_target < observed_new_target) return false;",
      "path": "src/pow.cpp",
      "position": 32,
      "original_position": 32,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "in_reply_to_id": 959022153,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "headerssync.cpp logs this case with an \"invalid difficulty transition\" message.",
      "created_at": "2022-08-31T12:42:01Z",
      "updated_at": "2022-08-31T12:42:01Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r959535676",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959535676"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 102,
      "original_line": 102,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959536759",
      "pull_request_review_id": 1091815526,
      "id": 959536759,
      "node_id": "PRRC_kwDOABII5845MV53",
      "diff_hunk": "@@ -0,0 +1,469 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_UTIL_BITDEQUE_H\n+#define BITCOIN_UTIL_BITDEQUE_H\n+\n+#include <bitset>\n+#include <cstddef>\n+#include <deque>\n+#include <limits>\n+#include <stdexcept>\n+#include <tuple>\n+\n+/** Class that mimics std::deque<bool>, but with std::vector<bool>'s bit packing.\n+ *\n+ * BlobSize selects the (minimum) number of bits that are allocated at once.\n+ * Larger values reduce the asymptotic memory usage overhead, at the cost of",
      "path": "src/util/bitdeque.h",
      "position": 18,
      "original_position": 18,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "84852bb6bb3579e475ce78fe729fd125ddbc715f",
      "in_reply_to_id": 959034094,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "It's independent of usage patterns, and written from scratch, with an API that's compatible with `std::deque`.",
      "created_at": "2022-08-31T12:43:05Z",
      "updated_at": "2022-08-31T12:43:22Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r959536759",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/959536759"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 18,
      "original_line": 18,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/960144110",
      "pull_request_review_id": 1092668805,
      "id": 960144110,
      "node_id": "PRRC_kwDOABII5845OqLu",
      "diff_hunk": "@@ -0,0 +1,268 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).",
      "path": "src/headerssync.h",
      "position": 67,
      "original_position": 67,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "551a8d957c4c44afbd0d608fcdf7c6a4352babce",
      "in_reply_to_id": null,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "So I was wondering how our overall headers-sync logic would perform against potential _best-chain blinding_  attacks, where an adversary with strong hashrate capabilities feed us with a quasi-unlimited number of minimum-chain-work headers chain to prevent the node from ever discovering the most-work valid chain (or at least significantly delay it), as we would continually iterate the \"fake\" chain space.\r\n\r\nDuring IBD, we sync from a single peer (L5301, in `net_processing.cpp`), and for each new inv block received we'll do initial-sync headers with one more peer (L3621, in `net_processing.cpp). So assuming the first peer picked up is controlled by the adversary to feed us one element of the \"fake\" chain space, each new block give us a chance to escape the space by syncing with a honest peer. That said, the adversary might control a number of inbound/outbound slots granting one more ~10min of blinding (I believe the criteria L3621 on which we pick up the one-more peer is falsifiable as it's just an inv(block)). \r\n\r\nAbove IBD, we'll send a GETHEADERS to all our peers (L5312, in `net_processing.cpp`), as such this blinding attack is reducible  \r\n\r\nI think the `nMinimumChainWork` puts a high bar in term of hashrate capabilities required, moreover our peer selection strategy would also need to be bypassed, or a high number of puppets nodes be deployed by the adversary AND the first preferred peer we're selecting for initial-sync headers to be controlled by the attacker.\r\n\r\nIf this issue is concerning, I think we could request that the \"one-more\" initial-headers-sync peers as added by `05f7f31598` to be `fPreferredDownload` (outbound or blessed peer) as a slight hardening.  ",
      "created_at": "2022-09-01T01:27:57Z",
      "updated_at": "2022-09-01T02:59:12Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r960144110",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/960144110"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 67,
      "original_line": 67,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/960154166",
      "pull_request_review_id": 1092668805,
      "id": 960154166,
      "node_id": "PRRC_kwDOABII5845Oso2",
      "diff_hunk": "@@ -0,0 +1,268 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** PRESYNC means the peer has not yet demonstrated their chain has\n+         * sufficient work and we're only building commitments to the chain they\n+         * serve us. */\n+        PRESYNC,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Construct a HeadersSyncState object representing a headers sync via this\n+     *  download-twice mechanism).\n+     *\n+     * id: node id (for logging)\n+     * consensus_params: parameters needed for difficulty adjustment validation\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+            const CBlockIndex* chain_start, const arith_uint256& minimum_required_work);\n+\n+    /** Result data structure for ProcessNextHeaders. */\n+    struct ProcessingResult {\n+        std::vector<CBlockHeader> pow_validated_headers;\n+        bool success{false};\n+        bool request_more{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * received_headers: headers that were received over the network for processing.\n+     *                   Assumes the caller has already verified the headers\n+     *                   are continuous, and has checked that each header\n+     *                   satisfies the proof-of-work target included in the\n+     *                   header (but not necessarily verified that the\n+     *                   proof-of-work target is correct and passes consensus\n+     *                   rules).\n+     * full_headers_message: true if the message was at max capacity,\n+     *                       indicating more headers may be available\n+     * ProcessingResult.pow_validated_headers: will be filled in with any\n+     *                       headers that the caller can fully process and\n+     *                       validate now (because these returned headers are\n+     *                       on a chain with sufficient work)\n+     * ProcessingResult.success: set to false if an error is detected and the sync is\n+     *                       aborted; true otherwise.\n+     * ProcessingResult.request_more: if true, the caller is suggested to call\n+     *                       NextHeadersRequestLocator and send a getheaders message using it.\n+     */\n+    ProcessingResult ProcessNextHeaders(const std::vector<CBlockHeader>&\n+            received_headers, bool full_headers_message);\n+\n+    /** Issue the next GETHEADERS message to our peer.\n+     *\n+     * This will return a locator appropriate for the current sync object, to continue the\n+     * synchronization phase it is in.\n+     */\n+    CBlockLocator NextHeadersRequestLocator() const;\n+\n+private:\n+    /** Clear out all download state that might be in progress (freeing any used\n+     * memory), and mark this object as no longer usable.\n+     */\n+    void Finalize();\n+\n+    /**\n+     *  Only called in PRESYNC.\n+     *  Validate the work on the headers we received from the network, and\n+     *  store commitments for later. Update overall state with successfully\n+     *  processed headers.\n+     *  On failure, this invokes Finalize() and returns false.\n+     */\n+    bool ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers);\n+\n+    /** In PRESYNC, process and update state for a single header */\n+    bool ValidateAndProcessSingleHeader(const CBlockHeader& current);\n+\n+    /** In REDOWNLOAD, check a header's commitment (if applicable) and add to\n+     * buffer for later processing */\n+    bool ValidateAndStoreRedownloadedHeader(const CBlockHeader& header);\n+\n+    /** Return a set of headers that satisfy our proof-of-work threshold */\n+    std::vector<CBlockHeader> PopHeadersReadyForAcceptance();\n+\n+private:\n+    /** NodeId of the peer (used for log messages) **/\n+    const NodeId m_id;\n+\n+    /** We use the consensus params in our anti-DoS calculations */\n+    const Consensus::Params& m_consensus_params;\n+\n+    /** Store the last block in our block index that the peer's chain builds from */\n+    const CBlockIndex* m_chain_start{nullptr};\n+\n+    /** Minimum work that we're looking for on this chain. */\n+    const arith_uint256 m_minimum_required_work;\n+\n+    /** Work that we've seen so far on the peer's chain */\n+    arith_uint256 m_current_chain_work;\n+\n+    /** m_hasher is a salted hasher for making our 1-bit commitments to headers we've seen. */\n+    const SaltedTxidHasher m_hasher;\n+\n+    /** A queue of commitment bits, created during the 1st phase, and verified during the 2nd. */\n+    bitdeque<> m_header_commitments;\n+\n+    /** The (secret) offset on the heights for which to create commitments.",
      "path": "src/headerssync.h",
      "position": 216,
      "original_position": 216,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "551a8d957c4c44afbd0d608fcdf7c6a4352babce",
      "in_reply_to_id": null,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I wonder what is meant here by \"(secret)\", if it should be interpreted as the offset not being known or observable by our peers. As it's a software-defined value (`HEADERS_COMMITMENT_PERIOD`) in this module, I don't know if there is a secrecy goal achieved.  ",
      "created_at": "2022-09-01T01:56:23Z",
      "updated_at": "2022-09-01T02:59:12Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r960154166",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/960154166"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 225,
      "original_line": 216,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/960173753",
      "pull_request_review_id": 1092668805,
      "id": 960173753,
      "node_id": "PRRC_kwDOABII5845Oxa5",
      "diff_hunk": "@@ -0,0 +1,317 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959}; // 13959/584 = ~23.9 commitments\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments = {};\n+    m_last_header_received.SetNull();\n+    m_redownloaded_headers = {};\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                Assume(m_download_state == State::PRESYNC);\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps our peer reorged away from the chain\n+        // they were on. Give up on this sync for now (likely we will start a\n+        // new sync with a new starting point).\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (presync phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and occasionally store\n+    // commitments.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& current)\n+{\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                m_last_header_received.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (presync phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    if (next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        // Add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // It's possible the chain grew since we started the sync; so\n+            // potentially we could succeed in syncing the peer's chain if we\n+            // try again later.\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: exceeded max commitments at height=%i (presync phase)\\n\", m_id, next_height);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));\n+    m_last_header_received = current;\n+    m_current_height = next_height;\n+\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreRedownloadedHeader(const CBlockHeader& header)\n+{\n+    Assume(m_download_state == State::REDOWNLOAD);\n+    if (m_download_state != State::REDOWNLOAD) return false;\n+\n+    int64_t next_height = m_redownload_buffer_last_height + 1;\n+\n+    // Ensure that we're working on a header that connects to the chain we're\n+    // downloading.\n+    if (header.hashPrevBlock != m_redownload_buffer_last_hash) {",
      "path": "src/headerssync.cpp",
      "position": 224,
      "original_position": 224,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "551a8d957c4c44afbd0d608fcdf7c6a4352babce",
      "in_reply_to_id": null,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Note, one of the requirement of `ValidateAndStoreRedownloadedHeader` is to have to check the continuity of the headers chain \"Assumes the caller has already verified the headers are continuous\". This requirement is done L2372 in `net_processing.cpp` thus I wonder if this check can be effectively hit, and there is not a redundancy.",
      "created_at": "2022-09-01T02:51:21Z",
      "updated_at": "2022-09-01T02:59:12Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r960173753",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/960173753"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 224,
      "original_line": 224,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/960179371",
      "pull_request_review_id": 1092716274,
      "id": 960179371,
      "node_id": "PRRC_kwDOABII5845Oyyr",
      "diff_hunk": "@@ -0,0 +1,268 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#ifndef BITCOIN_HEADERSSYNC_H\n+#define BITCOIN_HEADERSSYNC_H\n+\n+#include <arith_uint256.h>\n+#include <chain.h>\n+#include <consensus/params.h>\n+#include <net.h> // For NodeId\n+#include <primitives/block.h>\n+#include <uint256.h>\n+#include <util/bitdeque.h>\n+#include <util/hasher.h>\n+\n+#include <deque>\n+#include <vector>\n+\n+// A compressed CBlockHeader, which leaves out the prevhash\n+struct CompressedHeader {\n+    // header\n+    int32_t nVersion{0};\n+    uint256 hashMerkleRoot;\n+    uint32_t nTime{0};\n+    uint32_t nBits{0};\n+    uint32_t nNonce{0};\n+\n+    CompressedHeader()\n+    {\n+        hashMerkleRoot.SetNull();\n+    }\n+\n+    CompressedHeader(const CBlockHeader& header)\n+    {\n+        nVersion = header.nVersion;\n+        hashMerkleRoot = header.hashMerkleRoot;\n+        nTime = header.nTime;\n+        nBits = header.nBits;\n+        nNonce = header.nNonce;\n+    }\n+\n+    CBlockHeader GetFullHeader(const uint256& hash_prev_block) {\n+        CBlockHeader ret;\n+        ret.nVersion = nVersion;\n+        ret.hashPrevBlock = hash_prev_block;\n+        ret.hashMerkleRoot = hashMerkleRoot;\n+        ret.nTime = nTime;\n+        ret.nBits = nBits;\n+        ret.nNonce = nNonce;\n+        return ret;\n+    };\n+};\n+\n+/** HeadersSyncState:\n+ *\n+ * We wish to download a peer's headers chain in a DoS-resistant way.\n+ *\n+ * The Bitcoin protocol does not offer an easy way to determine the work on a\n+ * peer's chain. Currently, we can query a peer's headers by using a GETHEADERS\n+ * message, and our peer can return a set of up to 2000 headers that connect to\n+ * something we know. If a peer's chain has more than 2000 blocks, then we need\n+ * a way to verify that the chain actually has enough work on it to be useful to\n+ * us -- by being above our anti-DoS minimum-chain-work threshold -- before we\n+ * commit to storing those headers in memory. Otherwise, it would be cheap for\n+ * an attacker to waste all our memory by serving us low-work headers\n+ * (particularly for a new node coming online for the first time).\n+ *\n+ * To prevent memory-DoS with low-work headers, while still always being\n+ * able to reorg to whatever the most-work chain is, we require that a chain\n+ * meet a work threshold before committing it to memory. We can do this by\n+ * downloading a peer's headers twice, whenever we are not sure that the chain\n+ * has sufficient work:\n+ *\n+ * - In the first download phase, called pre-synchronization, we can calculate\n+ * the work on the chain as we go (just by checking the nBits value on each\n+ * header, and validating the proof-of-work).\n+ *\n+ * - Once we have reached a header where the cumulative chain work is\n+ * sufficient, we switch to downloading the headers a second time, this time\n+ * processing them fully, and possibly storing them in memory.\n+ *\n+ * To prevent an attacker from using (eg) the honest chain to convince us that\n+ * they have a high-work chain, but then feeding us an alternate set of\n+ * low-difficulty headers in the second phase, we store commitments to the\n+ * chain we see in the first download phase that we check in the second phase,\n+ * as follows:\n+ *\n+ * - In phase 1 (presync), store 1 bit (using a salted hash function) for every\n+ * N headers that we see. With a reasonable choice of N, this uses relatively\n+ * little memory even for a very long chain.\n+ *\n+ * - In phase 2 (redownload), keep a lookahead buffer and only accept headers\n+ * from that buffer into the block index (permanent memory usage) once they\n+ * have some target number of verified commitments on top of them. With this\n+ * parametrization, we can achieve a given security target for potential\n+ * permanent memory usage, while choosing N to minimize memory use during the\n+ * sync (temporary, per-peer storage).\n+ */\n+\n+class HeadersSyncState {\n+public:\n+    ~HeadersSyncState() {}\n+\n+    enum class State {\n+        /** PRESYNC means the peer has not yet demonstrated their chain has\n+         * sufficient work and we're only building commitments to the chain they\n+         * serve us. */\n+        PRESYNC,\n+        /** REDOWNLOAD means the peer has given us a high-enough-work chain,\n+         * and now we're redownloading the headers we saw before and trying to\n+         * accept them */\n+        REDOWNLOAD,\n+        /** We're done syncing with this peer and can discard any remaining state */\n+        FINAL\n+    };\n+\n+    /** Return the current state of our download */\n+    State GetState() const { return m_download_state; }\n+\n+    /** Construct a HeadersSyncState object representing a headers sync via this\n+     *  download-twice mechanism).\n+     *\n+     * id: node id (for logging)\n+     * consensus_params: parameters needed for difficulty adjustment validation\n+     * chain_start: best known fork point that the peer's headers branch from\n+     * minimum_required_work: amount of chain work required to accept the chain\n+     */\n+    HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+            const CBlockIndex* chain_start, const arith_uint256& minimum_required_work);\n+\n+    /** Result data structure for ProcessNextHeaders. */\n+    struct ProcessingResult {\n+        std::vector<CBlockHeader> pow_validated_headers;\n+        bool success{false};\n+        bool request_more{false};\n+    };\n+\n+    /** Process a batch of headers, once a sync via this mechanism has started\n+     *\n+     * received_headers: headers that were received over the network for processing.\n+     *                   Assumes the caller has already verified the headers\n+     *                   are continuous, and has checked that each header\n+     *                   satisfies the proof-of-work target included in the\n+     *                   header (but not necessarily verified that the\n+     *                   proof-of-work target is correct and passes consensus\n+     *                   rules).\n+     * full_headers_message: true if the message was at max capacity,\n+     *                       indicating more headers may be available\n+     * ProcessingResult.pow_validated_headers: will be filled in with any\n+     *                       headers that the caller can fully process and\n+     *                       validate now (because these returned headers are\n+     *                       on a chain with sufficient work)\n+     * ProcessingResult.success: set to false if an error is detected and the sync is\n+     *                       aborted; true otherwise.\n+     * ProcessingResult.request_more: if true, the caller is suggested to call\n+     *                       NextHeadersRequestLocator and send a getheaders message using it.\n+     */\n+    ProcessingResult ProcessNextHeaders(const std::vector<CBlockHeader>&\n+            received_headers, bool full_headers_message);\n+\n+    /** Issue the next GETHEADERS message to our peer.\n+     *\n+     * This will return a locator appropriate for the current sync object, to continue the\n+     * synchronization phase it is in.\n+     */\n+    CBlockLocator NextHeadersRequestLocator() const;\n+\n+private:\n+    /** Clear out all download state that might be in progress (freeing any used\n+     * memory), and mark this object as no longer usable.\n+     */\n+    void Finalize();\n+\n+    /**\n+     *  Only called in PRESYNC.\n+     *  Validate the work on the headers we received from the network, and\n+     *  store commitments for later. Update overall state with successfully\n+     *  processed headers.\n+     *  On failure, this invokes Finalize() and returns false.\n+     */\n+    bool ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers);\n+\n+    /** In PRESYNC, process and update state for a single header */\n+    bool ValidateAndProcessSingleHeader(const CBlockHeader& current);\n+\n+    /** In REDOWNLOAD, check a header's commitment (if applicable) and add to\n+     * buffer for later processing */\n+    bool ValidateAndStoreRedownloadedHeader(const CBlockHeader& header);\n+\n+    /** Return a set of headers that satisfy our proof-of-work threshold */\n+    std::vector<CBlockHeader> PopHeadersReadyForAcceptance();\n+\n+private:\n+    /** NodeId of the peer (used for log messages) **/\n+    const NodeId m_id;\n+\n+    /** We use the consensus params in our anti-DoS calculations */\n+    const Consensus::Params& m_consensus_params;\n+\n+    /** Store the last block in our block index that the peer's chain builds from */\n+    const CBlockIndex* m_chain_start{nullptr};\n+\n+    /** Minimum work that we're looking for on this chain. */\n+    const arith_uint256 m_minimum_required_work;\n+\n+    /** Work that we've seen so far on the peer's chain */\n+    arith_uint256 m_current_chain_work;\n+\n+    /** m_hasher is a salted hasher for making our 1-bit commitments to headers we've seen. */\n+    const SaltedTxidHasher m_hasher;\n+\n+    /** A queue of commitment bits, created during the 1st phase, and verified during the 2nd. */\n+    bitdeque<> m_header_commitments;\n+\n+    /** The (secret) offset on the heights for which to create commitments.",
      "path": "src/headerssync.h",
      "position": 216,
      "original_position": 216,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "551a8d957c4c44afbd0d608fcdf7c6a4352babce",
      "in_reply_to_id": 960154166,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "The period is not secret, the offset is secret and non-observable (until it is too late for the attacker).",
      "created_at": "2022-09-01T03:06:29Z",
      "updated_at": "2022-09-01T03:06:30Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r960179371",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/960179371"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 225,
      "original_line": 216,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/960825258",
      "pull_request_review_id": 1093659307,
      "id": 960825258,
      "node_id": "PRRC_kwDOABII5845RQeq",
      "diff_hunk": "@@ -0,0 +1,317 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959}; // 13959/584 = ~23.9 commitments\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments = {};\n+    m_last_header_received.SetNull();\n+    m_redownloaded_headers = {};\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                Assume(m_download_state == State::PRESYNC);\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);",
      "path": "src/headerssync.cpp",
      "position": 142,
      "original_position": 142,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "in_reply_to_id": null,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "This non-zero headers set requirement could be documented in the method definition. Sanitized L2735 in `net_processing.cpp`. ",
      "created_at": "2022-09-01T15:52:03Z",
      "updated_at": "2022-09-01T18:11:48Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r960825258",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/960825258"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 142,
      "original_line": 142,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/960845597",
      "pull_request_review_id": 1093659307,
      "id": 960845597,
      "node_id": "PRRC_kwDOABII5845RVcd",
      "diff_hunk": "@@ -0,0 +1,317 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959}; // 13959/584 = ~23.9 commitments\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments = {};\n+    m_last_header_received.SetNull();\n+    m_redownloaded_headers = {};\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                Assume(m_download_state == State::PRESYNC);\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);\n+    if (headers.size() == 0) return true;\n+\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    if (headers[0].hashPrevBlock != m_last_header_received.GetHash()) {\n+        // Somehow our peer gave us a header that doesn't connect.\n+        // This might be benign -- perhaps our peer reorged away from the chain\n+        // they were on. Give up on this sync for now (likely we will start a\n+        // new sync with a new starting point).\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (presync phase)\\n\", m_id, m_current_height);\n+        return false;\n+    }\n+\n+    // If it does connect, (minimally) validate and occasionally store\n+    // commitments.\n+    for (const auto& hdr : headers) {\n+        if (!ValidateAndProcessSingleHeader(hdr)) {\n+            return false;\n+        }\n+    }\n+\n+    if (m_current_chain_work >= m_minimum_required_work) {\n+        m_redownloaded_headers.clear();\n+        m_redownload_buffer_last_height = m_chain_start->nHeight;\n+        m_redownload_buffer_first_prev_hash = m_chain_start->GetBlockHash();\n+        m_redownload_buffer_last_hash = m_chain_start->GetBlockHash();\n+        m_redownload_chain_work = m_chain_start->nChainWork;\n+        m_download_state = State::REDOWNLOAD;\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync transition with peer=%d: reached sufficient work at height=%i, redownloading from height=%i\\n\", m_id, m_current_height, m_redownload_buffer_last_height);\n+    }\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndProcessSingleHeader(const CBlockHeader& current)\n+{\n+    Assume(m_download_state == State::PRESYNC);\n+    if (m_download_state != State::PRESYNC) return false;\n+\n+    int next_height = m_current_height + 1;\n+\n+    // Verify that the difficulty isn't growing too fast; an adversary with\n+    // limited hashing capability has a greater chance of producing a high\n+    // work chain if they compress the work into as few blocks as possible,\n+    // so don't let anyone give a chain that would violate the difficulty\n+    // adjustment maximum.\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                m_last_header_received.nBits, current.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (presync phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    if (next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        // Add a commitment.\n+        m_header_commitments.push_back(m_hasher(current.GetHash()) & 1);\n+        if (m_header_commitments.size() > m_max_commitments) {\n+            // The peer's chain is too long; give up.\n+            // It's possible the chain grew since we started the sync; so\n+            // potentially we could succeed in syncing the peer's chain if we\n+            // try again later.\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: exceeded max commitments at height=%i (presync phase)\\n\", m_id, next_height);\n+            return false;\n+        }\n+    }\n+\n+    m_current_chain_work += GetBlockProof(CBlockIndex(current));\n+    m_last_header_received = current;\n+    m_current_height = next_height;\n+\n+    return true;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreRedownloadedHeader(const CBlockHeader& header)\n+{\n+    Assume(m_download_state == State::REDOWNLOAD);\n+    if (m_download_state != State::REDOWNLOAD) return false;\n+\n+    int64_t next_height = m_redownload_buffer_last_height + 1;\n+\n+    // Ensure that we're working on a header that connects to the chain we're\n+    // downloading.\n+    if (header.hashPrevBlock != m_redownload_buffer_last_hash) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: non-continuous headers at height=%i (redownload phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    // Check that the difficulty adjustments are within our tolerance:\n+    uint32_t previous_nBits{0};\n+    if (!m_redownloaded_headers.empty()) {\n+        previous_nBits = m_redownloaded_headers.back().nBits;\n+    } else {\n+        previous_nBits = m_chain_start->nBits;\n+    }\n+\n+    if (!PermittedDifficultyTransition(m_consensus_params, next_height,\n+                previous_nBits, header.nBits)) {\n+        LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: invalid difficulty transition at height=%i (redownload phase)\\n\", m_id, next_height);\n+        return false;\n+    }\n+\n+    // Track work on the redownloaded chain\n+    m_redownload_chain_work += GetBlockProof(CBlockIndex(header));\n+\n+    if (m_redownload_chain_work >= m_minimum_required_work) {\n+        m_process_all_remaining_headers = true;\n+    }\n+\n+    // If we're at a header for which we previously stored a commitment, verify\n+    // it is correct. Failure will result in aborting download.\n+    // Also, don't check commitments once we've gotten to our target blockhash;\n+    // it's possible our peer has extended its chain between our first sync and\n+    // our second, and we don't want to return failure after we've seen our\n+    // target blockhash just because we ran out of commitments.\n+    if (!m_process_all_remaining_headers && next_height % HEADER_COMMITMENT_PERIOD == m_commit_offset) {\n+        if (m_header_commitments.size() == 0) {\n+            LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: commitment overrun at height=%i (redownload phase)\\n\", m_id, next_height);\n+            // Somehow our peer managed to feed us a different chain and\n+            // we've run out of commitments.",
      "path": "src/headerssync.cpp",
      "position": 260,
      "original_position": 260,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "in_reply_to_id": null,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Here we could invite the node operator to submit a bug report, as if a peer effectively managed to feed us a different low-work chain and exhaust our commitments it would be a data point than a) this anti-DoS headers sync mitigation can be falsified and b) we have adversaries in the wild with hashing capability actively targeting nodes. ",
      "created_at": "2022-09-01T16:12:00Z",
      "updated_at": "2022-09-01T18:11:48Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r960845597",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/960845597"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 260,
      "original_line": 260,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/960965919",
      "pull_request_review_id": 1093659307,
      "id": 960965919,
      "node_id": "PRRC_kwDOABII5845Ry0f",
      "diff_hunk": "@@ -381,6 +382,15 @@ struct Peer {\n     /** Time of the last getheaders message to this peer */\n     NodeClock::time_point m_last_getheaders_timestamp{};\n \n+    /** Protects m_headers_sync **/\n+    Mutex m_headers_sync_mutex;\n+    /** Headers-sync state for this peer (eg for initial sync, or syncing large\n+     * reorgs) **/",
      "path": "src/net_processing.cpp",
      "position": 15,
      "original_position": 15,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "in_reply_to_id": null,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "IIUC, syncing large reorgs is defined here by if we receive chain headers forking more than the 144 block buffer as documented in `GetAntiDoSWorkThreshold()`. Any fork under that threshold should have its headers accepted without the anti-DoS headers mechanism playing out.   ",
      "created_at": "2022-09-01T18:10:35Z",
      "updated_at": "2022-09-01T18:11:48Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r960965919",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/960965919"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 388,
      "original_line": 388,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/960968749",
      "pull_request_review_id": 1093860016,
      "id": 960968749,
      "node_id": "PRRC_kwDOABII5845Rzgt",
      "diff_hunk": "@@ -0,0 +1,317 @@\n+// Copyright (c) 2022 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n+#include <headerssync.h>\n+#include <logging.h>\n+#include <pow.h>\n+#include <timedata.h>\n+#include <util/check.h>\n+\n+// The two constants below are computed using the simulation script on\n+// https://gist.github.com/sipa/016ae445c132cdf65a2791534dfb7ae1\n+\n+//! Store a commitment to a header every HEADER_COMMITMENT_PERIOD blocks.\n+constexpr size_t HEADER_COMMITMENT_PERIOD{584};\n+\n+//! Only feed headers to validation once this many headers on top have been\n+//! received and validated against commitments.\n+constexpr size_t REDOWNLOAD_BUFFER_SIZE{13959}; // 13959/584 = ~23.9 commitments\n+\n+// Our memory analysis assumes 48 bytes for a CompressedHeader (so we should\n+// re-calculate parameters if we compress further)\n+static_assert(sizeof(CompressedHeader) == 48);\n+\n+HeadersSyncState::HeadersSyncState(NodeId id, const Consensus::Params& consensus_params,\n+        const CBlockIndex* chain_start, const arith_uint256& minimum_required_work) :\n+    m_id(id), m_consensus_params(consensus_params),\n+    m_chain_start(chain_start),\n+    m_minimum_required_work(minimum_required_work),\n+    m_current_chain_work(chain_start->nChainWork),\n+    m_commit_offset(GetRand<unsigned>(HEADER_COMMITMENT_PERIOD)),\n+    m_last_header_received(m_chain_start->GetBlockHeader()),\n+    m_current_height(chain_start->nHeight)\n+{\n+    // Estimate the number of blocks that could possibly exist on the peer's\n+    // chain *right now* using 6 blocks/second (fastest blockrate given the MTP\n+    // rule) times the number of seconds from the last allowed block until\n+    // today. This serves as a memory bound on how many commitments we might\n+    // store from this peer, and we can safely give up syncing if the peer\n+    // exceeds this bound, because it's not possible for a consensus-valid\n+    // chain to be longer than this (at the current time -- in the future we\n+    // could try again, if necessary, to sync a longer chain).\n+    m_max_commitments = 6*(Ticks<std::chrono::seconds>(GetAdjustedTime() - NodeSeconds{std::chrono::seconds{chain_start->GetMedianTimePast()}}) + MAX_FUTURE_BLOCK_TIME) / HEADER_COMMITMENT_PERIOD;\n+\n+    LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync started with peer=%d: height=%i, max_commitments=%i, min_work=%s\\n\", m_id, m_current_height, m_max_commitments, m_minimum_required_work.ToString());\n+}\n+\n+/** Free any memory in use, and mark this object as no longer usable. This is\n+ * required to guarantee that we won't reuse this object with the same\n+ * SaltedTxidHasher for another sync. */\n+void HeadersSyncState::Finalize()\n+{\n+    Assume(m_download_state != State::FINAL);\n+    m_header_commitments = {};\n+    m_last_header_received.SetNull();\n+    m_redownloaded_headers = {};\n+    m_redownload_buffer_last_hash.SetNull();\n+    m_redownload_buffer_first_prev_hash.SetNull();\n+    m_process_all_remaining_headers = false;\n+    m_current_height = 0;\n+\n+    m_download_state = State::FINAL;\n+}\n+\n+/** Process the next batch of headers received from our peer.\n+ *  Validate and store commitments, and compare total chainwork to our target to\n+ *  see if we can switch to REDOWNLOAD mode.  */\n+HeadersSyncState::ProcessingResult HeadersSyncState::ProcessNextHeaders(const\n+        std::vector<CBlockHeader>& received_headers, const bool full_headers_message)\n+{\n+    ProcessingResult ret;\n+\n+    Assume(!received_headers.empty());\n+    if (received_headers.empty()) return ret;\n+\n+    Assume(m_download_state != State::FINAL);\n+    if (m_download_state == State::FINAL) return ret;\n+\n+    if (m_download_state == State::PRESYNC) {\n+        // During PRESYNC, we minimally validate block headers and\n+        // occasionally add commitments to them, until we reach our work\n+        // threshold (at which point m_download_state is updated to REDOWNLOAD).\n+        ret.success = ValidateAndStoreHeadersCommitments(received_headers);\n+        if (ret.success) {\n+            if (full_headers_message || m_download_state == State::REDOWNLOAD) {\n+                // A full headers message means the peer may have more to give us;\n+                // also if we just switched to REDOWNLOAD then we need to re-request\n+                // headers from the beginning.\n+                ret.request_more = true;\n+            } else {\n+                Assume(m_download_state == State::PRESYNC);\n+                // If we're in PRESYNC and we get a non-full headers\n+                // message, then the peer's chain has ended and definitely doesn't\n+                // have enough work, so we can stop our sync.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (presync phase)\\n\", m_id, m_current_height);\n+            }\n+        }\n+    } else if (m_download_state == State::REDOWNLOAD) {\n+        // During REDOWNLOAD, we compare our stored commitments to what we\n+        // receive, and add headers to our redownload buffer. When the buffer\n+        // gets big enough (meaning that we've checked enough commitments),\n+        // we'll return a batch of headers to the caller for processing.\n+        ret.success = true;\n+        for (const auto& hdr : received_headers) {\n+            if (!ValidateAndStoreRedownloadedHeader(hdr)) {\n+                // Something went wrong -- the peer gave us an unexpected chain.\n+                // We could consider looking at the reason for failure and\n+                // punishing the peer, but for now just give up on sync.\n+                ret.success = false;\n+                break;\n+            }\n+        }\n+\n+        if (ret.success) {\n+            // Return any headers that are ready for acceptance.\n+            ret.pow_validated_headers = PopHeadersReadyForAcceptance();\n+\n+            // If we hit our target blockhash, then all remaining headers will be\n+            // returned and we can clear any leftover internal state.\n+            if (m_redownloaded_headers.empty() && m_process_all_remaining_headers) {\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync complete with peer=%d: releasing all at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            } else if (full_headers_message) {\n+                // If the headers message is full, we need to request more.\n+                ret.request_more = true;\n+            } else {\n+                // For some reason our peer gave us a high-work chain, but is now\n+                // declining to serve us that full chain again. Give up.\n+                // Note that there's no more processing to be done with these\n+                // headers, so we can still return success.\n+                LogPrint(BCLog::HEADERSSYNC, \"Initial headers sync aborted with peer=%d: incomplete headers message at height=%i (redownload phase)\\n\", m_id, m_redownload_buffer_last_height);\n+            }\n+        }\n+    }\n+\n+    if (!(ret.success && ret.request_more)) Finalize();\n+    return ret;\n+}\n+\n+bool HeadersSyncState::ValidateAndStoreHeadersCommitments(const std::vector<CBlockHeader>& headers)\n+{\n+    // The caller should not give us an empty set of headers.\n+    Assume(headers.size() > 0);",
      "path": "src/headerssync.cpp",
      "position": 142,
      "original_position": 142,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "in_reply_to_id": 960825258,
      "user": {
        "login": "sipa",
        "id": 548488,
        "node_id": "MDQ6VXNlcjU0ODQ4OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/sipa",
        "html_url": "https://github.com/sipa",
        "followers_url": "https://api.github.com/users/sipa/followers",
        "following_url": "https://api.github.com/users/sipa/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/sipa/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/sipa/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
        "organizations_url": "https://api.github.com/users/sipa/orgs",
        "repos_url": "https://api.github.com/users/sipa/repos",
        "events_url": "https://api.github.com/users/sipa/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/sipa/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "#25968 gets rid of that special case",
      "created_at": "2022-09-01T18:14:15Z",
      "updated_at": "2022-09-01T18:14:15Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r960968749",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/960968749"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 142,
      "original_line": 142,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/961860152",
      "pull_request_review_id": 1095132796,
      "id": 961860152,
      "node_id": "PRRC_kwDOABII5845VNI4",
      "diff_hunk": "@@ -875,7 +875,7 @@ class PeerManagerImpl final : public PeerManager\n         EXCLUSIVE_LOCKS_REQUIRED(!m_most_recent_block_mutex, peer.m_getdata_requests_mutex) LOCKS_EXCLUDED(::cs_main);\n \n     /** Process a new block. Perform any post-processing housekeeping */\n-    void ProcessBlock(CNode& node, const std::shared_ptr<const CBlock>& block, bool force_processing);\n+    void ProcessBlock(CNode& node, const std::shared_ptr<const CBlock>& block, bool force_processing, bool min_pow_checked);",
      "path": "src/net_processing.cpp",
      "position": 5,
      "original_position": 5,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "ed6cddd98e32263fc116a4380af6d66da20da990",
      "in_reply_to_id": null,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Rational under which conditions `min_pow_checked` can be setup to true could be added. E.g, when anti-dos checks have been carried out like in compact block processing or when the block have been submitted by a RPC caller (like in `generateblock()` or `submitheader()`. Or replicate the comments added for `AcceptBlockHeader()`, `ProcessNewBlock()`, `ProcessNewBlockHeaders()`.  ",
      "created_at": "2022-09-02T16:52:27Z",
      "updated_at": "2022-09-02T18:21:12Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r961860152",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/961860152"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 902,
      "original_line": 878,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/961907449",
      "pull_request_review_id": 1095132796,
      "id": 961907449,
      "node_id": "PRRC_kwDOABII5845VYr5",
      "diff_hunk": "@@ -5034,6 +5033,27 @@ void PeerManagerImpl::MaybeSendAddr(CNode& node, Peer& peer, std::chrono::micros\n     }\n }\n \n+void PeerManagerImpl::MaybeSendSendHeaders(CNode& node, Peer& peer)\n+{\n+    // Delay sending SENDHEADERS (BIP 130) until we're done with an\n+    // initial-headers-sync with this peer. Receiving headers announcements for\n+    // new blocks while trying to sync their headers chain is problematic,\n+    // because of the state tracking done.",
      "path": "src/net_processing.cpp",
      "position": 43,
      "original_position": 43,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "83c6a0c5249c4ecbd11f7828c84a50fb473faba3",
      "in_reply_to_id": null,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "IIUC the rational, in the lack of this delay of sending SENDHEADERS, we could have duplicated headers-sync with a peer. One, with HEADERS reception triggering continuous GETHEADERS in `ProcessHeadersMessage()` (L2854). A second, when we receive an INV(block_hash) at INV reception, as the \"1 new headers-sync peer every new block\" logic (L3622).\r\n\r\nNote, delaying sending SENDHEADERS might have interference with old versions, or least BIP130 mentions \"As support for sendheaders is optional, software that implements this may also optionally impose additional constraints, such as only honoring sendheaders messages shortly after a connection is established\".  ",
      "created_at": "2022-09-02T18:08:30Z",
      "updated_at": "2022-09-02T18:21:12Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r961907449",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/961907449"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 5129,
      "original_line": 5041,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/961913713",
      "pull_request_review_id": 1095132796,
      "id": 961913713,
      "node_id": "PRRC_kwDOABII5845VaNx",
      "diff_hunk": "@@ -3711,6 +3711,29 @@ bool ChainstateManager::ProcessNewBlockHeaders(const std::vector<CBlockHeader>&\n     return true;\n }\n \n+void ChainstateManager::ReportHeadersPresync(const arith_uint256& work, int64_t height, int64_t timestamp)\n+{\n+    AssertLockNotHeld(cs_main);\n+    const auto& chainstate = ActiveChainstate();\n+    {\n+        LOCK(cs_main);\n+        // Don't report headers presync progress if we already have a post-minchainwork header chain.\n+        // This means we lose reporting for potentially legimate, but unlikely, deep reorgs, but\n+        // prevent attackers that spam low-work headers from filling our logs.\n+        if (m_best_header->nChainWork >= UintToArith256(GetConsensus().nMinimumChainWork)) return;\n+        // Rate limit headers presync updates to 4 per second, as these are not subject to DoS\n+        // protection.\n+        auto now = std::chrono::steady_clock::now();\n+        if (now < m_last_presync_update + std::chrono::milliseconds{250}) return;\n+        m_last_presync_update = now;\n+    }\n+    if (chainstate.IsInitialBlockDownload()) {\n+        const int64_t blocks_left{(GetTime() - timestamp) / GetConsensus().nPowTargetSpacing};\n+        const double progress{100.0 * height / (height + blocks_left)};\n+        LogPrintf(\"Pre-synchronizing blockheaders, height: %d (~%.2f%%)\\n\", height, progress);",
      "path": "src/validation.cpp",
      "position": 23,
      "original_position": 23,
      "commit_id": "3add23454624c4c79c9eebc060b6fbed4e3131a7",
      "original_commit_id": "355547334f7d08640ee1fa291227356d61145d1a",
      "in_reply_to_id": null,
      "user": {
        "login": "ariard",
        "id": 23310655,
        "node_id": "MDQ6VXNlcjIzMzEwNjU1",
        "avatar_url": "https://avatars.githubusercontent.com/u/23310655?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ariard",
        "html_url": "https://github.com/ariard",
        "followers_url": "https://api.github.com/users/ariard/followers",
        "following_url": "https://api.github.com/users/ariard/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/ariard/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/ariard/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/ariard/subscriptions",
        "organizations_url": "https://api.github.com/users/ariard/orgs",
        "repos_url": "https://api.github.com/users/ariard/repos",
        "events_url": "https://api.github.com/users/ariard/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/ariard/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Even if it's a validation-related log, the peer id could be provided to indicate the headers source in case of anomalies detection.",
      "created_at": "2022-09-02T18:19:19Z",
      "updated_at": "2022-09-02T18:21:12Z",
      "html_url": "https://github.com/bitcoin/bitcoin/pull/25717#discussion_r961913713",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/961913713"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bitcoin/pulls/25717"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 3735,
      "original_line": 3733,
      "side": "RIGHT"
    }
  ]
}